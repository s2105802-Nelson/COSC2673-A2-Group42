{"cells":[{"cell_type":"markdown","metadata":{"id":"QDRW51eYK_pg"},"source":["<hr style=\"color:green\" />\n","<h1 style=\"color:green\">COSC2673 Assignment 2: Image Classification for Cancerous Cells</h1>\n","<h2 style=\"color:green\">File 21: PyTorch First CNN model test on Main data, using Custom Config</h2>\n","<hr style=\"color:green\" />\n","\n","<p>\n","The AlexNet implementation, taken directly across, did not perform well. Drilling into what it was actually doing, it's clear that it's designed for larger sized images, and that many layers with those configurations was likely stripping way too much information\n","</p>\n","<p>\n","Create a custom, simple CNN structure appropriate to the 27x27 pixel image size\n","</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5842,"status":"ok","timestamp":1683517665569,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"LuaHh7dfK_pj"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import cv2\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torch.utils.data\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision.io import read_image\n"]},{"cell_type":"markdown","metadata":{"id":"4ayZvnueK_pk"},"source":["Configure this script as to whether it runs on Google Colab, or locally"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1683517665570,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"vFtUm6uXK_pk"},"outputs":[],"source":["# When on Google Colab, running full training, change both to true. Locally, advised set both to false\n","isGoogleColab = False\n","useFullData = False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23800,"status":"ok","timestamp":1683517689365,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"zzl3NpfVK_pk","outputId":"c89d1d15-03a2-4139-feab-a6a97174e454"},"outputs":[],"source":["# In local, the base directory is the current directory\n","baseDirectory = \"./\"\n","\n","if isGoogleColab:\n","    from google.colab import drive\n","    \n","    # If this is running on Google colab, assume the notebook runs in a \"COSC2673\" folder, which also contains the data files \n","    # in a subfolder called \"image_classification_data\"\n","    drive.mount(\"/content/drive\")\n","    !ls /content/drive/'My Drive'/COSC2673/\n","\n","    # Import the directory so that custom python libraries can be imported\n","    import sys\n","    sys.path.append(\"/content/drive/MyDrive/COSC2673/\")\n","\n","    # Set the base directory to the Google Drive specific folder\n","    baseDirectory = \"/content/drive/MyDrive/COSC2673/\""]},{"cell_type":"markdown","metadata":{"id":"vZCfUn3EK_pl"},"source":["Import the custom python files that contain reusable code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3513,"status":"ok","timestamp":1683517692876,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"beWSdbauK_pl","outputId":"5d6ee838-f71a-462f-9ba9-ae54c5bb8078"},"outputs":[],"source":["import data_basic_utility as dbutil\n","import graphing_utility as graphutil\n","import statistics_utility as statsutil\n","\n","import a2_utility as a2util\n","import pytorch_utility as ptutil\n","from pytorch_utility import CancerBinaryDataset\n","from pytorch_utility import CancerCellTypeDataset\n","\n","\n","# randomSeed = dbutil.get_random_seed()\n","randomSeed = 266305\n","print(\"Random Seed: \" + str(randomSeed))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":631,"status":"ok","timestamp":1683517693503,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"zgIXX9BXK_pl"},"outputs":[],"source":["# this file should have previously been created in the root directory\n","dfImages = pd.read_csv(baseDirectory + \"images_main.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1683517693504,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"6_WFGagHK_pl","outputId":"6392de72-66c1-4153-9abd-1f457d94fb71"},"outputs":[],"source":["# Get The training Split and the Validation Split\n","dfImagesTrain = dfImages[dfImages[\"trainValTest\"] == 0].reset_index()\n","dfImagesVal = dfImages[dfImages[\"trainValTest\"] == 1].reset_index()\n","dfImagesTest = dfImages[dfImages[\"trainValTest\"] == 2].reset_index()\n","\n","print(dfImagesTrain.shape)\n","print(dfImagesVal.shape)\n","print(dfImagesTest.shape)\n","\n","dfImagesTrain.head()"]},{"cell_type":"markdown","metadata":{"id":"TZ1R94YdK_pn"},"source":["Note: The definition of the Custom Datasets for both the isCancerous data and the Cell Type data are defined in the pytorch_utility.py file.\n","\n","Also, rather than loading all the training images and calculating the mean and standard deviation values in here, that was run separately in file 05a.PyTorchGetMeanAndStd.ipynb\n","\n","Here we can just define the values to use, which shouldn't change unless the data is reloaded and a new train/validation/test split is generated"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683517693504,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"whsaUaCCRNpG","outputId":"49c5a8c3-c080-47cf-b9f7-163f5718cde0"},"outputs":[],"source":["train_mean, train_std = ptutil.getTrainMeanAndStdTensors()\n","print(train_mean)\n","print(train_std)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1683517693504,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"I8z3yX8ZK_pn"},"outputs":[],"source":["# Create a tranform operation that also normalizes the images according to the mean and standard deviations of the images\n","transform_normalize = transforms.Compose(\n","    [transforms.ToPILImage(),\n","    transforms.ToTensor(), \n","    transforms.Normalize(train_mean, train_std)])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1683517693505,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"qb_33dFmK_pn"},"outputs":[],"source":["cancerous_training_data = None\n","\n","# Create a custom Dataset for the training and validation data\n","if useFullData:\n","    cancerous_training_data = CancerBinaryDataset(isGoogleColab, dfImagesTrain, baseDirectory, transform=transform_normalize)\n","else:\n","    # For testing in a small dataset\n","    dfImagesTrainTest = dfImagesTrain.iloc[range(1000), :].reset_index()\n","    cancerous_training_data = CancerBinaryDataset(isGoogleColab, dfImagesTrainTest, baseDirectory, transform=transform_normalize, target_transform=None)\n","\n","cancerous_validation_data = CancerBinaryDataset(isGoogleColab, dfImagesVal, baseDirectory, transform=transform_normalize, target_transform=None)\n","cancerous_test_data = CancerBinaryDataset(isGoogleColab, dfImagesTest, baseDirectory, transform=transform_normalize, target_transform=None)\n","\n","# Create data loaders\n","cancerous_train_dataloader = DataLoader(cancerous_training_data, batch_size=32, shuffle=True, num_workers=2)\n","cancerous_val_dataloader = DataLoader(cancerous_validation_data, batch_size=32, shuffle=True, num_workers=2)\n","cancerous_test_dataloader = DataLoader(cancerous_test_data, batch_size=32, shuffle=True, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"F4jD7E2PK_po"},"source":["Now, create a class for the basic, Fully Connected Neural Network. For this basic NN, we will use 3 fully connected layers. The number of features in this will be 27 x 27 x 3, or 2187.\n","\n","Layer 1: Input is the images, which are 27 x 27 pixels, with 3 color values (RGB). Experiment initially with 1458 nodes\n","Layer 2: Input is 1458 from the the previous layer, down to 729\n","Layer 3: Input is 729 from the the previous layer, since this is a binary classification problem, the output will be 2 classes\n","\n","In this, we will use the **ReLU** Activation Function. This is the Rectified Linear Unit function, which allows the function to become non-linear"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1683517693505,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"0two1S9gK_po"},"outputs":[],"source":["# Create a class for the Neural Network\n","class PT_CNN_IsCancerous(nn.Module):\n","\n","    # In the constructor, initialize the layers to use\n","    def __init__(self):\n","        super(PT_CNN_IsCancerous, self).__init__()\n","\n","        # first, define the subsampling methods. Though they are used multiple times, these are the\n","        # operations, so only need to be defined once\n","        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n","\n","        # define the Activation methods to use\n","        self.relu = nn.ReLU(inplace=True)\n","        self.sigmoid = nn.Sigmoid()\n","\n","        # define the convolution layers\n","\n","        # input should be 27x27x3. Apply a 3x3 filter, therefore, output should be 25x25x32 (channels aka feature maps)\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1)\n","        # There will be a Relu\n","        # Then a MaxPool of 2x2, halving the dimensions per feature map\n","        # So input is 12x12x32. Apply a 3x3 filter, also include padding=1, as this is already quite small, and lets consider the edges\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)    \n","        # There will be a Relu        \n","        # So input is 12x12x64. Apply a 3x3 filter, also include padding=1, as this is already quite small, and lets consider the edges\n","        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n","        # Then a MaxPool of 2x2, halving the dimensions per feature map\n","        # Then an Average Pool of keeping the dimensions as 6x6\n","        \n","        # define the fully connected neural layers\n","        self.fc1 = nn.Linear(128 * 6 * 6, 4608)\n","        self.fc2 = nn.Linear(4608, 2306)\n","        self.fc3 = nn.Linear(2306, 2)\n","\n","    # Create the forward function, which is used in training\n","    def forward(self, x):\n","\n","        # print(\"Init Shape: \" + str(x.shape))\n","\n","        # Process the first 2 convolution layers, applying maxpooling\n","        x = self.relu(self.conv1(x))\n","        x = self.maxpool(x)\n","\n","        # Then process the remaining convolution layers without any pooling\n","        x = self.relu(self.conv2(x))        \n","        x = self.relu(self.conv3(x))\n","\n","        # Then apply a max pool and average pool on the result\n","        x = self.maxpool(x)\n","        #x = self.avgpool(x)\n","\n","        # Flatten: This should convert to tensors that are acceptable for the input into the NN 3 layers\n","        x = x.view(x.size(0), 128 * 6 * 6)\n","\n","        # Now process the 3 layers of the Fully Connected NN\n","        x = self.relu(self.fc1(x))\n","        x = self.relu(self.fc2(x))\n","        x = self.sigmoid(self.fc3(x))        \n","        # x = self.relu(self.fc3(x))        \n","        # x = self.fc3(x)        \n","\n","        # return the result\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"pO5dPhy6K_po"},"source":["Now train the Convolutional Neural Network Model.\n","\n","During training, we will use the following:\n","- Softmax Cross Entropy Loss as our Loss function. This is a good Loss function that basically converts scores for each class into probabilities\n","- The Adam Optimizer, which is a version of Gradient Descent\n","- Initially, just 10 epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3387261,"status":"ok","timestamp":1683521080759,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"vQUAMyGuK_po","outputId":"14945ec1-2266-4969-9bea-20e820f67e1c"},"outputs":[],"source":["# set the Learning Rate to use\n","learning_rate = 0.0001\n","epochsToUse = 10\n","\n","net = PT_CNN_IsCancerous()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n","\n","for epoch in range(epochsToUse):\n","    print(\"Starting Epoch \" + str(epoch) + \"...\")\n","    for i, data in enumerate(cancerous_train_dataloader, 0):\n","        # Get the inputs\n","        inputs, labels = data\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Perform Forward and Backward propagation then optimize the weights\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"CEB7hSkAK_po"},"source":["Training Time in Nelson's Local Environment on the full data takes a very long time, stopped after 100 minutes. This will need to be done in Colab.\n","\n","First, Predict on the training data so that we can find the training error."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24841,"status":"ok","timestamp":1683521105598,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"Dutbv_FKRNpH","outputId":"475b88f3-a546-4d3b-d87f-28349519642f"},"outputs":[],"source":["correct, total = 0,  0\n","predictions = []\n","\n","# Set the Neural Network into evaluation (test) mode\n","net.eval()\n","\n","step=0\n","\n","y_train_cancerous = []\n","y_train_pred_cancerous = []\n","\n","# Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n","showStep = False\n","for i, data in enumerate(cancerous_train_dataloader, 0):\n","    inputs, labels = data\n","\n","    outputs = net(inputs)\n","    class_score, predicted = torch.max(outputs.data, 1)\n","    \n","    # Loop through the batch, build the lists of the raw label and prediction values\n","    for j in range(len(labels)):\n","        y_train_cancerous.append(labels[j].item())\n","        y_train_pred_cancerous.append(predicted[j].item())\n","\n","        if showStep:\n","            print(class_score[j])\n","\n","    showStep=False\n","\n","    predictions.append(predicted)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","\n","print(\"Evaluate the Training Predictions and Error: \\n\")\n","\n","print('Confusion matrix: \\n')\n","print(confusion_matrix(y_train_cancerous, y_train_pred_cancerous))\n","\n","print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_train_cancerous, y_train_pred_cancerous)))\n","print(\"- Precision Score: \" + str(precision_score(y_train_cancerous, y_train_pred_cancerous)))\n","print(\"- Recall Score: \" + str(recall_score(y_train_cancerous, y_train_pred_cancerous)))\n","print(\"- F1 Score: \" + str(f1_score(y_train_cancerous, y_train_pred_cancerous)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":507},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1683521105599,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"vLttfi9nRNpH","outputId":"ab5f4429-0734-4706-dffe-c5807438c6e4"},"outputs":[],"source":["a2util.getClassificationROC(\"IsCancerous\", \"Training\", y_train_cancerous, y_train_pred_cancerous)"]},{"cell_type":"markdown","metadata":{"id":"mUdRn526RNpI"},"source":["Now Predict according to the Validation data and evaluate. While looping through here, we will need to get out the Labels from the data loader, because the order of predictions in the batches do not match the order of the original Target values in the dataset (because we turned Shuffle on)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":227280,"status":"ok","timestamp":1683521332873,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"ithF1bLMK_pp","outputId":"a772f1fc-9967-434e-e24a-a466a8e02162"},"outputs":[],"source":["correct, total = 0,  0\n","predictions = []\n","\n","# Set the Neural Network into evaluation (test) mode\n","net.eval()\n","\n","step=0\n","\n","y_val_cancerous = []\n","y_val_pred_cancerous = []\n","\n","# Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n","for i, data in enumerate(cancerous_val_dataloader, 0):\n","    inputs, labels = data\n","\n","    outputs = net(inputs)\n","    class_score, predicted = torch.max(outputs.data, 1)\n","    \n","    # Loop through the batch, build the lists of the raw label and prediction values\n","    for j in range(len(labels)):\n","        y_val_cancerous.append(labels[j].item())\n","        y_val_pred_cancerous.append(predicted[j].item())\n","\n","    predictions.append(predicted)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","\n","\n","print(\"Evaluate the Validation Predictions and Error: \\n\")\n","\n","print('Confusion matrix: \\n')\n","print(confusion_matrix(y_val_cancerous, y_val_pred_cancerous))\n","\n","print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_val_cancerous, y_val_pred_cancerous)))\n","print(\"- Precision Score: \" + str(precision_score(y_val_cancerous, y_val_pred_cancerous)))\n","print(\"- Recall Score: \" + str(recall_score(y_val_cancerous, y_val_pred_cancerous)))\n","print(\"- F1 Score: \" + str(f1_score(y_val_cancerous, y_val_pred_cancerous)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":507},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683521332873,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"6f4hBdFGRNpI","outputId":"df825239-624f-435c-c1a9-c149b8fda067"},"outputs":[],"source":["a2util.getClassificationROC(\"IsCancerous\", \"Validation\", y_val_cancerous, y_val_pred_cancerous, 2)"]},{"cell_type":"markdown","metadata":{},"source":["Now Predict and evaluate on the test Set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["correct, total = 0,  0\n","predictions = []\n","\n","# Set the Neural Network into evaluation (test) mode\n","net.eval()\n","\n","step=0\n","\n","y_test_cancerous = []\n","y_test_pred_cancerous = []\n","\n","# Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n","for i, data in enumerate(cancerous_test_dataloader, 0):\n","    inputs, labels = data\n","\n","    # This should convert the image tensors into vectors\n","    inputs = inputs.view(-1, 27 * 27 * 3)\n","\n","    outputs = net(inputs)\n","    _, predicted = torch.max(outputs.data, 1)\n","    \n","    # Loop through the batch, build the lists of the raw label and prediction values\n","    for j in range(len(labels)):\n","        y_test_cancerous.append(labels[j].item())\n","        y_test_pred_cancerous.append(predicted[j].item())\n","\n","    predictions.append(predicted)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","\n","print(\"Evaluate the Test Predictions and Error: \\n\")\n","\n","print('Confusion matrix: \\n')\n","print(confusion_matrix(y_test_cancerous, y_test_pred_cancerous))\n","\n","print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_test_cancerous, y_test_pred_cancerous)))\n","print(\"- Precision Score: \" + str(precision_score(y_test_cancerous, y_test_pred_cancerous)))\n","print(\"- Recall Score: \" + str(recall_score(y_test_cancerous, y_test_pred_cancerous)))\n","print(\"- F1 Score: \" + str(f1_score(y_test_cancerous, y_test_pred_cancerous)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["a2util.getClassificationROC(\"IsCancerous\", \"Test\", y_test_cancerous, y_test_pred_cancerous, 2)"]},{"cell_type":"markdown","metadata":{"id":"Z4hM1fUrd7jM"},"source":["# Google Colab Results\n","<hr />\n","\n","Binary with Sigmoid as Final Activation Function\n","- **Training**\n","- Accuracy Score: 0.9518948577261708\n","- Precision Score: 0.9426820475847152\n","- Recall Score: 0.9230497705612425\n","- F1 Score: 0.9327626181558766\n","- **Validation**\n","- Accuracy Score: 0.8894277400581959\n","- Precision Score: 0.9178981937602627\n","- Recall Score: 0.8972712680577849\n","- F1 Score: 0.9074675324675324\n","\n","Binary with ReLU as Final Activation Function\n","- **Training**\n","- **Validation**\n","\n","\n","Binary with no Final Activation Function\n","- **Training**\n","- **Validation**\n","\n","\n","**Results from 3 Layer FC-NN with Dropout**\n","- Training F1: 0.923854447439353\n","- Validation F1: 0.897025171624714\n","\n","Binary Sigmoid performs maybe 1 percentage point better than the NN\n"]},{"cell_type":"markdown","metadata":{"id":"DHozRzzdK_pq"},"source":["Now also train a model for CellType Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":407,"status":"ok","timestamp":1683523805358,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"AVvBdxxjK_pq"},"outputs":[],"source":["celltype_training_data = None\n","\n","# Create a custom Dataset for the training and validation data\n","if useFullData:\n","    celltype_training_data = CancerCellTypeDataset(isGoogleColab, dfImagesTrain, baseDirectory, transform=transform_normalize)\n","else:\n","    # For testing in a small dataset\n","    dfImagesTrainTest = dfImagesTrain.iloc[range(1000), :].reset_index()\n","    celltype_training_data = CancerCellTypeDataset(isGoogleColab, dfImagesTrainTest, baseDirectory, transform=transform_normalize, target_transform=None)\n","\n","celltype_validation_data = CancerCellTypeDataset(isGoogleColab, dfImagesVal, baseDirectory, transform=transform_normalize)\n","celltype_test_data = CancerCellTypeDataset(isGoogleColab, dfImagesTest, baseDirectory, transform=transform_normalize)\n","\n","# Create data loaders\n","celltype_train_dataloader = DataLoader(celltype_training_data, batch_size=32, shuffle=True, num_workers=2)\n","celltype_val_dataloader = DataLoader(celltype_validation_data, batch_size=32, shuffle=True, num_workers=2)\n","celltype_test_dataloader = DataLoader(celltype_test_data, batch_size=32, shuffle=True, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"fd9XOjhpK_pq"},"source":["Create a class for the Cell Type Neural Network model. The structure of the class will be fundamentally the same, only the model will need to output 4 classes"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1683523806293,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"eKOxU2-4K_pq"},"outputs":[],"source":["# Create a class for the Neural Network\n","class PT_CNN_CellType(nn.Module):\n","\n","    # In the constructor, initialize the layers to use\n","    def __init__(self):\n","        super(PT_CNN_CellType, self).__init__()\n","\n","\n","        # first, define the subsampling methods. Though they are used multiple times, these are the\n","        # operations, so only need to be defined once\n","        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n","\n","        # define the Activation methods to use\n","        self.relu = nn.ReLU(inplace=True)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","        # define the convolution layers\n","\n","        # input should be 27x27x3. Apply a 3x3 filter, therefore, output should be 25x25x32 (channels aka feature maps)\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1)\n","        # There will be a Relu\n","        # Then a MaxPool of 2x2, halving the dimensions per feature map\n","        # So input is 12x12x32. Apply a 3x3 filter, also include padding=1, as this is already quite small, and lets consider the edges\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)    \n","        # There will be a Relu        \n","        # So input is 12x12x64. Apply a 3x3 filter, also include padding=1, as this is already quite small, and lets consider the edges\n","        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n","        # Then a MaxPool of 2x2, halving the dimensions per feature map\n","        # Then an Average Pool of keeping the dimensions as 6x6\n","        \n","        # define the fully connected neural layers\n","        self.fc1 = nn.Linear(128 * 6 * 6, 4608)\n","        self.fc2 = nn.Linear(4608, 2306)\n","        self.fc3 = nn.Linear(2306, 4)\n","\n","    # Create the forward function, which is used in training\n","    def forward(self, x):\n","\n","        # print(\"Init Shape: \" + str(x.shape))\n","\n","        # Process the first 2 convolution layers, applying maxpooling\n","        x = self.relu(self.conv1(x))\n","        x = self.maxpool(x)\n","\n","        # Then process the remaining convolution layers without any pooling\n","        x = self.relu(self.conv2(x))        \n","        x = self.relu(self.conv3(x))\n","\n","        # Then apply a max pool and average pool on the result\n","        x = self.maxpool(x)\n","        #x = self.avgpool(x)\n","\n","        # Flatten: This should convert to tensors that are acceptable for the input into the NN 3 layers\n","        x = x.view(x.size(0), 128 * 6 * 6)\n","\n","        # Now process the 3 layers of the Fully Connected NN\n","        x = self.relu(self.fc1(x))\n","        x = self.relu(self.fc2(x))        \n","        # x = self.fc3(x)\n","        # x = self.relu(self.fc3(x))\n","        x = F.softmax(self.fc3(x), dim=1)        \n","\n","        # return the result\n","        return x\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KRU0R9EMK_pq"},"source":["Now train the Fully Connected Neural Network Model. Use the same configuration (objective function, optimizer etc) as the Binary Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1768345,"status":"ok","timestamp":1683525574634,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"d3FjNKScK_pq","outputId":"1310fc31-2022-4471-feda-36bdb55a00fa"},"outputs":[],"source":["# set the Learning Rate to use\n","learning_rate = 0.0001\n","epochsToUse = 10\n","\n","net = PT_CNN_CellType()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n","\n","for epoch in range(epochsToUse):\n","    print(\"Starting Epoch \" + str(epoch) + \"...\")\n","    for i, data in enumerate(celltype_train_dataloader, 0):\n","        # Get the inputs\n","        inputs, labels = data\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Perform Forward and Backward propagation then optimize the weights\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"2d8yW4-2RNpJ"},"source":["Predict on the Training Set to get the Training Accuracy and Error"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25337,"status":"ok","timestamp":1683525599958,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"YzW_UTsrRNpK","outputId":"0972385d-31d4-4e28-99b6-a252ed78c1dd"},"outputs":[],"source":["correct, total = 0,  0\n","predictions = []\n","\n","# Set the Neural Network into evaluation (test) mode\n","net.eval()\n","\n","y_train_celltype = []\n","y_train_pred_celltype = []\n","y_train_pred_celltype_scores = []\n","\n","showBatch=True\n","\n","# Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n","for i, data in enumerate(celltype_train_dataloader, 0):\n","    inputs, labels = data\n","\n","    outputs = net(inputs)\n","\n","    # outputs.data contains a tensor of size 4 for each record, with a score for each class. Use max\n","    # to select the class of the highest score for the prediction.\n","    class_score, predicted = torch.max(outputs.data, 1)\n","    \n","    # Loop through the batch, build the lists of the raw label and prediction values\n","    for j in range(len(labels)):\n","        y_train_celltype.append(labels[j].item())\n","        y_train_pred_celltype.append(predicted[j].item())\n","        y_train_pred_celltype_scores.append(outputs.data[j].tolist())\n","\n","    showBatch = False\n","    \n","    predictions.append(predicted)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","\n","print(\"Evaluate the Training Predictions and Error: \\n\")\n","\n","print('Confusion matrix: \\n')\n","print(confusion_matrix(y_train_celltype, y_train_pred_celltype))\n","\n","print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_train_celltype, y_train_pred_celltype)))\n","print(\"- Precision Score: \" + str(precision_score(y_train_celltype, y_train_pred_celltype, average=\"micro\")))\n","print(\"- Recall Score: \" + str(recall_score(y_train_celltype, y_train_pred_celltype, average=\"micro\")))\n","print(\"- F1 Score: \" + str(f1_score(y_train_celltype, y_train_pred_celltype, average=\"micro\")))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1683525599958,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"bFRW0l6CRNpK","outputId":"c062795b-775b-40b0-a7ae-1854bb1e9f56"},"outputs":[],"source":["for i in range(5):\n","    print(y_train_pred_celltype_scores[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1501,"status":"ok","timestamp":1683525601453,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"nV00GdMmRNpK","outputId":"ab9a74d0-e3e8-493c-fb6e-a4f9946f5edd"},"outputs":[],"source":["a2util.getClassificationROC(\"CellType\", \"Training\", y_train_celltype, y_train_pred_celltype, 4, y_train_pred_celltype_scores)"]},{"cell_type":"markdown","metadata":{"id":"NpLNiit9K_pr"},"source":["Predict on the Validation data and evaluate the results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4329,"status":"ok","timestamp":1683525605779,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"VfU8z2ljK_pr","outputId":"64a16234-117a-4efd-ff0f-42ed743ea630"},"outputs":[],"source":["correct, total = 0,  0\n","predictions = []\n","\n","# Set the Neural Network into evaluation (test) mode\n","net.eval()\n","\n","step=0\n","\n","y_val_celltype = []\n","y_val_pred_celltype = []\n","y_val_pred_celltype_scores = []\n","\n","# Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n","for i, data in enumerate(celltype_val_dataloader, 0):\n","    inputs, labels = data\n","\n","    outputs = net(inputs)\n","    class_score, predicted = torch.max(outputs.data, 1)\n","    \n","    # Loop through the batch, build the lists of the raw label and prediction values\n","    for j in range(len(labels)):\n","        y_val_celltype.append(labels[j].item())\n","        y_val_pred_celltype.append(predicted[j].item())\n","        y_val_pred_celltype_scores.append(outputs.data[j].tolist())\n","\n","    predictions.append(predicted)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","\n","accuracy = (correct/total) * 100\n","print(\"Evaluate the Validation Predictions and Error: \\n\")\n","\n","print('Confusion matrix: \\n')\n","print(confusion_matrix(y_val_celltype, y_val_pred_celltype))\n","\n","print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_val_celltype, y_val_pred_celltype)))\n","print(\"- Precision Score: \" + str(precision_score(y_val_celltype, y_val_pred_celltype, average=\"micro\")))\n","print(\"- Recall Score: \" + str(recall_score(y_val_celltype, y_val_pred_celltype, average=\"micro\")))\n","print(\"- F1 Score: \" + str(f1_score(y_val_celltype, y_val_pred_celltype, average=\"micro\")))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":822,"status":"ok","timestamp":1683525606588,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"L3PMC4flRNpK","outputId":"a26e9df1-0a61-4c71-b3f8-14da45a97ef0"},"outputs":[],"source":["a2util.getClassificationROC(\"CellType\", \"Validation\", y_val_celltype, y_val_pred_celltype, 4, y_val_pred_celltype_scores)"]},{"cell_type":"markdown","metadata":{},"source":["Now Predict and Evaluate on the Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["correct, total = 0,  0\n","predictions = []\n","\n","# Set the Neural Network into evaluation (test) mode\n","net.eval()\n","\n","y_test_celltype = []\n","y_test_pred_celltype = []\n","y_test_pred_celltype_scores = []\n","\n","showBatch=True\n","\n","# Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n","for i, data in enumerate(celltype_test_dataloader, 0):\n","    inputs, labels = data\n","\n","    # This should convert the image tensors into vectors\n","    inputs = inputs.view(-1, 27 * 27 * 3)\n","\n","    outputs = net(inputs)\n","\n","    # outputs.data contains a tensor of size 4 for each record, with a score for each class. Use max\n","    # to select the class of the highest score for the prediction.\n","    class_score, predicted = torch.max(outputs.data, 1)\n","    \n","    # Loop through the batch, build the lists of the raw label and prediction values\n","    for j in range(len(labels)):\n","        y_test_celltype.append(labels[j].item())\n","        y_test_pred_celltype.append(predicted[j].item())\n","        y_test_pred_celltype_scores.append(outputs.data[j].tolist())\n","\n","    showBatch = False\n","    \n","    predictions.append(predicted)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","\n","print(\"Evaluate the Test Predictions and Error: \\n\")\n","\n","print('Confusion matrix: \\n')\n","print(confusion_matrix(y_test_celltype, y_test_pred_celltype))\n","\n","print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_test_celltype, y_test_pred_celltype)))\n","print(\"- Precision Score: \" + str(precision_score(y_test_celltype, y_test_pred_celltype, average=\"micro\")))\n","print(\"- Recall Score: \" + str(recall_score(y_test_celltype, y_test_pred_celltype, average=\"micro\")))\n","print(\"- F1 Score: \" + str(f1_score(y_test_celltype, y_test_pred_celltype, average=\"micro\")))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["a2util.getClassificationROC(\"CellType\", \"Test\", y_test_celltype, y_test_pred_celltype, 4, y_test_pred_celltype_scores)"]},{"cell_type":"markdown","metadata":{"id":"PpgZFd04eX8q"},"source":["# Google Colab Results\n","<hr />\n","\n","Multiclass with Softmax as Final Activation Function\n","- **Training**\n","- Accuracy Score: 0.842414189102973\n","- Precision Score: 0.842414189102973\n","- Recall Score: 0.842414189102973\n","- F1 Score: 0.842414189102973\n","- **Validation**\n","- Accuracy Score: 0.7827352085354026\n","- Precision Score: 0.7827352085354026\n","- Recall Score: 0.7827352085354026\n","- F1 Score: 0.7827352085354026\n","\n","\n","Multiclass with ReLU as Final Activation Function\n","- **Training**\n","- **Validation**\n","\n","\n","Multiclass with no Final Activation Function\n","- **Training**\n","- Accuracy Score: 0.9679724384330739\n","- Precision Score: 0.9679724384330739\n","- Recall Score: 0.9679724384330739\n","- F1 Score: 0.9679724384330739\n","- **Validation**\n","- Accuracy Score: 0.7575169738118331\n","- Precision Score: 0.7575169738118331\n","- Recall Score: 0.7575169738118331\n","- F1 Score: 0.7575169738118331\n","\n","**Results from 3 Layer FC-NN with Dropout**\n","- Training F1: 0.8442005869592957\n","- Validation F1: 0.7778855480116392"]},{"cell_type":"markdown","metadata":{"id":"imXr4wLKRNpL"},"source":["<h1>Analysis of Performance and Accuracy</h1>\n","\n","<strong>Binary Classification - IsCancerous</strong>\n","<p>\n","In this model we see that the model has a very low training error and a high accuracy value, in this experiment achieving a 97.3% accuracy. This indicates that the model has <strong>low bias</strong>. However, when predicting on the validation dataset, the validation error rises. It can be seen that the Accuracy for the validation predictions is 84.4%, and the are under the ROC Curve is significantly less, with a value of 0.824. This is an indication of <strong>high variance</strong>, and in combination with low bias, indicates a possible problem of <strong>overfitting</strong>\n","</p>\n","\n","<strong>Multi-class Classification - Cell Type</strong>\n","<p>\n","Similarly, the Cell Type model has a very low training error and a high accuracy value of 99.5%, indicating that the model has <strong>low bias</strong>. However, the Cell Type model has a considerably worse accuracy of only 76.5% when predicting on the validation dataset. A ROC curve is generated for each class, and it can be seen that the model performs better at predicting some classes compared to others. The average the under the ROC Curve score, with a value of 0.838. This indication of <strong>high variance</strong>, and in combination with low bias, indicates a possible problem of <strong>overfitting</strong>.\n","</p>"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"dcbc78149e46ccbab92a3f68a48c52feb0796c7e10dad8e3f1a2a5a780973376"}}},"nbformat":4,"nbformat_minor":0}
