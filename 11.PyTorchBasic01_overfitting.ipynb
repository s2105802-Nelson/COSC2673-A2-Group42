{"cells":[{"cell_type":"markdown","metadata":{"id":"QDRW51eYK_pg"},"source":["<hr style=\"color:green\" />\n","<h1 style=\"color:green\">COSC2673 Assignment 2: Image Classification for Cancerous Cells</h1>\n","<h2 style=\"color:green\">File 06: Basic PyTorch Fully Connected Neural Network model test on Main data</h2>\n","<hr style=\"color:green\" />\n","\n","<p>\n","In this file, attempt to apply techniques to deal with overfitting on the Basic 01 (3 layer FC NN) model. Experiment with L2 Regularization, Dropout maybe early stopping\n","</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11874,"status":"ok","timestamp":1683444154739,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"LuaHh7dfK_pj"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import cv2\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torch.utils.data\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision.io import read_image\n"]},{"cell_type":"markdown","metadata":{"id":"4ayZvnueK_pk"},"source":["Configure this script as to whether it runs on Google Colab, or locally"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1683444154742,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"vFtUm6uXK_pk"},"outputs":[],"source":["# When on Google Colab, running full training, change both to true. Locally, advised set both to false\n","isGoogleColab = False\n","useFullData = True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23343,"status":"ok","timestamp":1683444178066,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"zzl3NpfVK_pk","outputId":"882d3eb6-777a-4ffa-cc39-d6d8ed8cbcaa"},"outputs":[],"source":["# In local, the base directory is the current directory\n","baseDirectory = \"./\"\n","\n","if isGoogleColab:\n","    from google.colab import drive\n","    \n","    # If this is running on Google colab, assume the notebook runs in a \"COSC2673\" folder, which also contains the data files \n","    # in a subfolder called \"image_classification_data\"\n","    drive.mount(\"/content/drive\")\n","    !ls /content/drive/'My Drive'/COSC2673/\n","\n","    # Import the directory so that custom python libraries can be imported\n","    import sys\n","    sys.path.append(\"/content/drive/MyDrive/COSC2673/\")\n","\n","    # Set the base directory to the Google Drive specific folder\n","    baseDirectory = \"/content/drive/MyDrive/COSC2673/\""]},{"cell_type":"markdown","metadata":{"id":"vZCfUn3EK_pl"},"source":["Import the custom python files that contain reusable code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1658,"status":"ok","timestamp":1683444179719,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"beWSdbauK_pl","outputId":"4ed3e398-e0d9-4653-9860-9f1e5b10f79f"},"outputs":[],"source":["import data_basic_utility as dbutil\n","import graphing_utility as graphutil\n","import statistics_utility as statsutil\n","\n","import a2_utility as a2util\n","import pytorch_utility as ptutil\n","from pytorch_utility import CancerBinaryDataset\n","from pytorch_utility import CancerCellTypeDataset\n","\n","\n","# randomSeed = dbutil.get_random_seed()\n","randomSeed = 266305\n","print(\"Random Seed: \" + str(randomSeed))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":348,"status":"ok","timestamp":1683444180064,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"zgIXX9BXK_pl"},"outputs":[],"source":["# this file should have previously been created in the root directory\n","dfImages = pd.read_csv(baseDirectory + \"images_main.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1683444180066,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"6_WFGagHK_pl","outputId":"baf376de-4fac-46c8-cd5d-b69a47e887f2"},"outputs":[],"source":["# Get The training Split and the Validation Split\n","dfImagesTrain = dfImages[dfImages[\"trainValTest\"] == 0].reset_index()\n","dfImagesVal = dfImages[dfImages[\"trainValTest\"] == 1].reset_index()\n","dfImagesTest = dfImages[dfImages[\"trainValTest\"] == 2].reset_index()\n","\n","print(dfImagesTrain.shape)\n","print(dfImagesVal.shape)\n","print(dfImagesTest.shape)\n","\n","dfImagesTrain.head()"]},{"cell_type":"markdown","metadata":{"id":"TZ1R94YdK_pn"},"source":["Note: The definition of the Custom Datasets for both the isCancerous data and the Cell Type data are defined in the pytorch_utility.py file.\n","\n","Also, rather than loading all the training images and calculating the mean and standard deviation values in here, that was run separately in file 05a.PyTorchGetMeanAndStd.ipynb\n","\n","Here we can just define the values to use, which shouldn't change unless the data is reloaded and a new train/validation/test split is generated"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":483,"status":"ok","timestamp":1683444180528,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"qaY5AA-tsWWw","outputId":"bbf818fa-9256-4907-c894-cf3527001ead"},"outputs":[],"source":["train_mean, train_std = ptutil.getTrainMeanAndStdTensors()\n","print(train_mean)\n","print(train_std)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1683444180529,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"I8z3yX8ZK_pn"},"outputs":[],"source":["# Create a tranform operation that also normalizes the images according to the mean and standard deviations of the images\n","transform_normalize = transforms.Compose(\n","    [transforms.ToPILImage(),\n","    transforms.ToTensor(), \n","    transforms.Normalize(train_mean, train_std)])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1683444180530,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"qb_33dFmK_pn"},"outputs":[],"source":["cancerous_training_data = None\n","\n","# Create a custom Dataset for the training and validation data\n","if useFullData:\n","    cancerous_training_data = CancerBinaryDataset(isGoogleColab, dfImagesTrain, baseDirectory, transform=transform_normalize)\n","else:\n","    # For testing in a small dataset\n","    dfImagesTrainTest = dfImagesTrain.iloc[range(1000), :].reset_index()\n","    cancerous_training_data = CancerBinaryDataset(isGoogleColab, dfImagesTrainTest, baseDirectory, transform=transform_normalize, target_transform=None)\n","\n","cancerous_validation_data = CancerBinaryDataset(isGoogleColab, dfImagesVal, baseDirectory, transform=transform_normalize, target_transform=None)\n","cancerous_test_data = CancerBinaryDataset(isGoogleColab, dfImagesTest, baseDirectory, transform=transform_normalize, target_transform=None)\n","\n","# Create data loaders\n","cancerous_train_dataloader = DataLoader(cancerous_training_data, batch_size=32, shuffle=True, num_workers=2)\n","cancerous_val_dataloader = DataLoader(cancerous_validation_data, batch_size=32, shuffle=True, num_workers=2)\n","cancerous_test_dataloader = DataLoader(cancerous_test_data, batch_size=32, shuffle=True, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"F4jD7E2PK_po"},"source":["# L2 Regularization and Dropout\n","\n","Apply L2 Regularization and trin a model. Then Predict on both Training and Validation data to get the training error and validation error, and see if there is any affect to reduce variance and overfitting\n","\n","Also, define a class here that implements dropout, so various experiments can be run "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1683444180530,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"0two1S9gK_po"},"outputs":[],"source":["# Create a class for the Neural Network\n","class PT_NN_IsCancerous(nn.Module):\n","\n","    # In the constructor, initialize the layers to use\n","    def __init__(self):\n","        super(PT_NN_IsCancerous, self).__init__()\n","        self.fc1 = nn.Linear(27 * 27 * 3, 1458)\n","        self.fc2 = nn.Linear(1458, 729)\n","        self.fc3 = nn.Linear(729, 2)\n","        \n","        # Final output Activation Function\n","        self.sigmoid = nn.Sigmoid()\n","\n","    # Create the forward function, which is used in training\n","    def forward(self, x):\n","        # process through each layer\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.sigmoid(self.fc3(x))\n","\n","        # return the result\n","        return x\n","\n","# Create a class for the Neural Network with dropouts\n","class PT_NN_IsCancerous_Dropout(nn.Module):\n","\n","    # In the constructor, initialize the layers to use\n","    def __init__(self, dropout_prob):\n","        super(PT_NN_IsCancerous_Dropout, self).__init__()\n","        self.dropProb = dropout_prob\n","\n","        # 2 Dropout operations on the first two layers, don't do any dropouts on the output layer\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(p=self.dropProb),\n","            nn.Linear(27 * 27 * 3, 1458),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=self.dropProb),\n","            nn.Linear(1458, 729),\n","            nn.ReLU(inplace=True),                        \n","            nn.Linear(729, 2),    \n","            nn.Sigmoid()       \n","        )\n","\n","    # Create the forward function, which is used in training\n","    def forward(self, x):\n","        # Run the Sequential Classifier Layer\n","        return self.classifier(x)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pO5dPhy6K_po"},"source":["Now train the Fully Connected Neural Network Model.\n","\n","During training, we will use the following:\n","- Softmax Cross Entropy Loss as our Loss function. This is a good Loss function that basically converts scores for each class into probabilities\n","- The Adam Optimizer, which is a version of Gradient Descent\n","- Initially, just 10 epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":996850,"status":"ok","timestamp":1683445177370,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"vQUAMyGuK_po","outputId":"845d8bea-a63b-4bc0-ddf9-2e90a81fa4a5"},"outputs":[],"source":["\n","dropoutVals = [0.2, 0.25, 0.3, 0.4, 0.5]\n","\n","for dropout_rate in dropoutVals:\n","\n","    # set the Learning Rate to use\n","    learning_rate = 0.0001\n","    epochsToUse = 20\n","\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # Use this for dropouts\n","    net = PT_NN_IsCancerous_Dropout(dropout_rate)\n","    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n","\n","\n","    for epoch in range(epochsToUse):\n","        print(\"Starting Epoch \" + str(epoch) + \"...\")\n","        for i, data in enumerate(cancerous_train_dataloader, 0):\n","            # Get the inputs\n","            inputs, labels = data\n","\n","            # This should convert the image tensors into vectors\n","            inputs = inputs.view(-1, 27 * 27 * 3)\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # Perform Forward and Backward propagation then optimize the weights\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()    "]},{"cell_type":"markdown","metadata":{"id":"CEB7hSkAK_po"},"source":["Training Time in Nelson's Local Environment on the full data takes a very long time, stopped after 100 minutes. This will need to be done in Colab.\n","\n","First, Predict on the training data so that we can find the training error."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13634,"status":"ok","timestamp":1683445190985,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"f1E180BesWWy","outputId":"3532f657-0a61-49bb-f331-ff19767b4574"},"outputs":[],"source":["correct, total = 0,  0\n","predictions = []\n","\n","# Set the Neural Network into evaluation (test) mode\n","net.eval()\n","\n","step=0\n","\n","y_train_cancerous = []\n","y_train_pred_cancerous = []\n","\n","# Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n","for i, data in enumerate(cancerous_train_dataloader, 0):\n","    inputs, labels = data\n","\n","    # This should convert the image tensors into vectors\n","    inputs = inputs.view(-1, 27 * 27 * 3)\n","\n","    outputs = net(inputs)\n","    _, predicted = torch.max(outputs.data, 1)\n","    \n","    # Loop through the batch, build the lists of the raw label and prediction values\n","    for j in range(len(labels)):\n","        y_train_cancerous.append(labels[j].item())\n","        y_train_pred_cancerous.append(predicted[j].item())\n","\n","    predictions.append(predicted)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","\n","print(\"Evaluate the Training Predictions and Error: \\n\")\n","\n","print('Confusion matrix: \\n')\n","print(confusion_matrix(y_train_cancerous, y_train_pred_cancerous))\n","\n","print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_train_cancerous, y_train_pred_cancerous)))\n","print(\"- Precision Score: \" + str(precision_score(y_train_cancerous, y_train_pred_cancerous)))\n","print(\"- Recall Score: \" + str(recall_score(y_train_cancerous, y_train_pred_cancerous)))\n","print(\"- F1 Score: \" + str(f1_score(y_train_cancerous, y_train_pred_cancerous)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":507},"executionInfo":{"elapsed":55,"status":"ok","timestamp":1683445190986,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"4M0Jb0fosWWy","outputId":"29739f2a-7211-4a87-80d8-6fd2db10fe77"},"outputs":[],"source":["a2util.getClassificationROC(\"IsCancerous\", \"Training\", y_train_cancerous, y_train_pred_cancerous)"]},{"cell_type":"markdown","metadata":{"id":"fbvP6a19sWWy"},"source":["Now Predict according to the Validation data and evaluate. While looping through here, we will need to get out the Labels from the data loader, because the order of predictions in the batches do not match the order of the original Target values in the dataset (because we turned Shuffle on)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96948,"status":"ok","timestamp":1683445287916,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"ithF1bLMK_pp","outputId":"73085599-7a6f-46ff-e21a-e57115d45993"},"outputs":[],"source":["correct, total = 0,  0\n","predictions = []\n","\n","# Set the Neural Network into evaluation (test) mode\n","net.eval()\n","\n","step=0\n","\n","y_val_cancerous = []\n","y_val_pred_cancerous = []\n","\n","# Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n","for i, data in enumerate(cancerous_val_dataloader, 0):\n","    inputs, labels = data\n","\n","\n","    # This should convert the image tensors into vectors\n","    inputs = inputs.view(-1, 27 * 27 * 3)\n","\n","    outputs = net(inputs)\n","    class_score, predicted = torch.max(outputs.data, 1)\n","    \n","    # Loop through the batch, build the lists of the raw label and prediction values\n","    for j in range(len(labels)):\n","        y_val_cancerous.append(labels[j].item())\n","        y_val_pred_cancerous.append(predicted[j].item())\n","\n","    predictions.append(predicted)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","\n","\n","print(\"Evaluate the Validation Predictions and Error: \\n\")\n","\n","print('Confusion matrix: \\n')\n","print(confusion_matrix(y_val_cancerous, y_val_pred_cancerous))\n","\n","print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_val_cancerous, y_val_pred_cancerous)))\n","print(\"- Precision Score: \" + str(precision_score(y_val_cancerous, y_val_pred_cancerous)))\n","print(\"- Recall Score: \" + str(recall_score(y_val_cancerous, y_val_pred_cancerous)))\n","print(\"- F1 Score: \" + str(f1_score(y_val_cancerous, y_val_pred_cancerous)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":507},"executionInfo":{"elapsed":1016,"status":"ok","timestamp":1683445288926,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"ivOulSepsWWz","outputId":"b83dad85-18b9-402b-a0c6-600ce9f35428"},"outputs":[],"source":["a2util.getClassificationROC(\"IsCancerous\", \"Validation\", y_val_cancerous, y_val_pred_cancerous, 2)"]},{"cell_type":"markdown","metadata":{},"source":["Now Predict and evaluate on the test Set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["correct, total = 0,  0\n","predictions = []\n","\n","# Set the Neural Network into evaluation (test) mode\n","net.eval()\n","\n","step=0\n","\n","y_test_cancerous = []\n","y_test_pred_cancerous = []\n","\n","# Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n","for i, data in enumerate(cancerous_test_dataloader, 0):\n","    inputs, labels = data\n","\n","    # This should convert the image tensors into vectors\n","    inputs = inputs.view(-1, 27 * 27 * 3)\n","\n","    outputs = net(inputs)\n","    _, predicted = torch.max(outputs.data, 1)\n","    \n","    # Loop through the batch, build the lists of the raw label and prediction values\n","    for j in range(len(labels)):\n","        y_test_cancerous.append(labels[j].item())\n","        y_test_pred_cancerous.append(predicted[j].item())\n","\n","    predictions.append(predicted)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","\n","print(\"Evaluate the Test Predictions and Error: \\n\")\n","\n","print('Confusion matrix: \\n')\n","print(confusion_matrix(y_test_cancerous, y_test_pred_cancerous))\n","\n","print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_test_cancerous, y_test_pred_cancerous)))\n","print(\"- Precision Score: \" + str(precision_score(y_test_cancerous, y_test_pred_cancerous)))\n","print(\"- Recall Score: \" + str(recall_score(y_test_cancerous, y_test_pred_cancerous)))\n","print(\"- F1 Score: \" + str(f1_score(y_test_cancerous, y_test_pred_cancerous)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["a2util.getClassificationROC(\"IsCancerous\", \"Test\", y_test_cancerous, y_test_pred_cancerous, 2)"]},{"cell_type":"markdown","metadata":{"id":"XzcKQ3-_sWWz"},"source":["### Results\n","\n","No L2 Reg or Dropout (from file 06) - On Colab, Full data\n","- Training Accuracy: 0.969\n","- Validation: 0.844\n","\n","L2 with weight_decay=0.0001:\n","- **Training**\n","- Accuracy Score: 0.9960444047467143\n","- Precision Score: 0.998576512455516\n","- Recall Score: 0.9904694669961172\n","- F1 Score: 0.9945064681906787\n","- **Validation**\n","- Accuracy Score: 0.8516003879728419\n","- Precision Score: 0.8929765886287625\n","- Recall Score: 0.8571428571428571\n","- F1 Score: 0.8746928746928747\n","\n","L2 with weight_decay=0.001:\n","- Training Accuracy: 1\n","- Validation: 0.56\n","- Very bad on Local with test data, no need to test on colab\n","\n","Dropout with rate 0.5\n","- **Training**\n","- Accuracy Score: 0.9423248692101569\n","- Precision Score: 0.8836609732516919\n","- Recall Score: 0.9678785739498764\n","- F1 Score: 0.923854447439353\n","- **Validation**\n","- Accuracy Score: 0.8690591658583899\n","- Precision Score: 0.8546511627906976\n","- Recall Score: 0.9438202247191011\n","- F1 Score: 0.897025171624714\n","\n","\n","Dropout with rate 0.25\n","- Training Accuracy: 1\n","- Validation: 0.82\n","- not as good as 0.5 in test, no need to test on colab"]},{"cell_type":"markdown","metadata":{"id":"DHozRzzdK_pq"},"source":["Now also train a model for CellType Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1683445288927,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"AVvBdxxjK_pq","outputId":"ff248384-34a5-4459-ca31-88db23a555c0"},"outputs":[],"source":["celltype_training_data = None\n","\n","# Create a custom Dataset for the training and validation data\n","if useFullData:\n","    celltype_training_data = CancerCellTypeDataset(isGoogleColab, dfImagesTrain, baseDirectory, transform=transform_normalize)\n","else:\n","    # For testing in a small dataset\n","    dfImagesTrainTest = dfImagesTrain.iloc[range(1000), :].reset_index()\n","    celltype_training_data = CancerCellTypeDataset(isGoogleColab, dfImagesTrainTest, baseDirectory, transform=transform_normalize, target_transform=None)\n","\n","celltype_validation_data = CancerCellTypeDataset(isGoogleColab, dfImagesVal, baseDirectory, transform=transform_normalize)\n","celltype_test_data = CancerCellTypeDataset(isGoogleColab, dfImagesTest, baseDirectory, transform=transform_normalize)\n","\n","# Create data loaders\n","celltype_train_dataloader = DataLoader(celltype_training_data, batch_size=32, shuffle=True, num_workers=2)\n","celltype_val_dataloader = DataLoader(celltype_validation_data, batch_size=32, shuffle=True, num_workers=2)\n","celltype_test_dataloader = DataLoader(celltype_test_data, batch_size=32, shuffle=True, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"fd9XOjhpK_pq"},"source":["Create a class for the Cell Type Neural Network model. The structure of the class will be fundamentally the same, only the model will need to output 4 classes"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":29,"status":"ok","timestamp":1683445288928,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"eKOxU2-4K_pq"},"outputs":[],"source":["# Create a class for the Neural Network\n","class PT_NN_CellType(nn.Module):\n","\n","    # In the constructor, initialize the layers to use\n","    def __init__(self):\n","        super(PT_NN_CellType, self).__init__()\n","        self.fc1 = nn.Linear(27 * 27 * 3, 1458)\n","        self.fc2 = nn.Linear(1458, 729)\n","        self.fc3 = nn.Linear(729, 4)\n","        \n","        # Final output Activation Function\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    # Create the forward function, which is used in training\n","    def forward(self, x):\n","        # process through each layer\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.softmax(self.fc3(x))\n","\n","        # return the result\n","        return x\n","\n","\n","# Create a class for the Neural Network with dropouts\n","class PT_NN_CellType_Dropout(nn.Module):\n","\n","    # In the constructor, initialize the layers to use\n","    def __init__(self, dropout_prob):\n","        super(PT_NN_CellType_Dropout, self).__init__()\n","        self.dropProb = dropout_prob\n","\n","        # 2 Dropout operations on the first two layers, don't do any dropouts on the output layer\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(p=self.dropProb),\n","            nn.Linear(27 * 27 * 3, 1458),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=self.dropProb),\n","            nn.Linear(1458, 729),\n","            nn.ReLU(inplace=True),                        \n","            nn.Linear(729, 4),  \n","            nn.Softmax(dim=1)         \n","        )\n","\n","    # Create the forward function, which is used in training\n","    def forward(self, x):\n","        # Run the Sequential Classifier Layer\n","        return self.classifier(x)\n"]},{"cell_type":"markdown","metadata":{"id":"KRU0R9EMK_pq"},"source":["Now train the Fully Connected Neural Network Model. Use the same configuration (objective function, optimizer etc) as the Binary Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":186805,"status":"ok","timestamp":1683445475705,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"d3FjNKScK_pq","outputId":"e4ac7339-d0c0-474a-917b-3745e3bb34b5"},"outputs":[],"source":["# set the Learning Rate to use\n","learning_rate = 0.0001\n","epochsToUse = 50\n","dropout_rate = 0.5\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# Use this for L2 Regularization\n","# net = PT_NN_CellType()\n","# optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.0001)\n","\n","# Use this for dropouts\n","net = PT_NN_CellType_Dropout(dropout_rate)\n","optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n","\n","for epoch in range(epochsToUse):\n","    print(\"Starting Epoch \" + str(epoch) + \"...\")\n","    for i, data in enumerate(celltype_train_dataloader, 0):\n","        # Get the inputs\n","        inputs, labels = data\n","\n","        # This should convert the image tensors into vectors\n","        inputs = inputs.view(-1, 27 * 27 * 3)\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Perform Forward and Backward propagation then optimize the weights\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"XPplgEbCsWW0"},"source":["Predict on the Training Set to get the Training Accuracy and Error"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11424,"status":"ok","timestamp":1683445487101,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"dWdn3fsTsWW0","outputId":"7d7e61d1-8cd0-4795-953e-6654ab3e9f72"},"outputs":[],"source":["correct, total = 0,  0\n","predictions = []\n","\n","# Set the Neural Network into evaluation (test) mode\n","net.eval()\n","\n","y_train_celltype = []\n","y_train_pred_celltype = []\n","y_train_pred_celltype_scores = []\n","\n","showBatch=True\n","\n","# Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n","for i, data in enumerate(celltype_train_dataloader, 0):\n","    inputs, labels = data\n","\n","    # This should convert the image tensors into vectors\n","    inputs = inputs.view(-1, 27 * 27 * 3)\n","\n","    outputs = net(inputs)\n","\n","    # outputs.data contains a tensor of size 4 for each record, with a score for each class. Use max\n","    # to select the class of the highest score for the prediction.\n","    class_score, predicted = torch.max(outputs.data, 1)\n","    \n","    # Loop through the batch, build the lists of the raw label and prediction values\n","    for j in range(len(labels)):\n","        y_train_celltype.append(labels[j].item())\n","        y_train_pred_celltype.append(predicted[j].item())\n","        y_train_pred_celltype_scores.append(outputs.data[j].tolist())\n","\n","    showBatch = False\n","    \n","    predictions.append(predicted)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","\n","print(\"Evaluate the Training Predictions and Error: \\n\")\n","\n","print('Confusion matrix: \\n')\n","print(confusion_matrix(y_train_celltype, y_train_pred_celltype))\n","\n","print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_train_celltype, y_train_pred_celltype)))\n","print(\"- Precision Score: \" + str(precision_score(y_train_celltype, y_train_pred_celltype, average=\"micro\")))\n","print(\"- Recall Score: \" + str(recall_score(y_train_celltype, y_train_pred_celltype, average=\"micro\")))\n","print(\"- F1 Score: \" + str(f1_score(y_train_celltype, y_train_pred_celltype, average=\"micro\")))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1683445487104,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"u27Mb-rTsWW0","outputId":"480fa4ad-f858-42b6-d769-fb1bdb5e2b82"},"outputs":[],"source":["for i in range(5):\n","    print(y_train_pred_celltype_scores[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1934,"status":"ok","timestamp":1683445489015,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"eiHy8WeqsWW0","outputId":"da439ef9-b096-4a9c-a690-2c7e21d5ae59"},"outputs":[],"source":["a2util.getClassificationROC(\"CellType\", \"Training\", y_train_celltype, y_train_pred_celltype, 4, y_train_pred_celltype_scores)"]},{"cell_type":"markdown","metadata":{"id":"NpLNiit9K_pr"},"source":["Predict on the Validation data and evaluate the results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6111,"status":"ok","timestamp":1683445495115,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"VfU8z2ljK_pr","outputId":"30df849a-6d69-4f28-a1c6-3d2b0499772e"},"outputs":[],"source":["correct, total = 0,  0\n","predictions = []\n","\n","# Set the Neural Network into evaluation (test) mode\n","net.eval()\n","\n","step=0\n","\n","y_val_celltype = []\n","y_val_pred_celltype = []\n","y_val_pred_celltype_scores = []\n","\n","# Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n","for i, data in enumerate(celltype_val_dataloader, 0):\n","    inputs, labels = data\n","\n","    # This should convert the image tensors into vectors\n","    inputs = inputs.view(-1, 27 * 27 * 3)\n","\n","    outputs = net(inputs)\n","    class_score, predicted = torch.max(outputs.data, 1)\n","    \n","    # Loop through the batch, build the lists of the raw label and prediction values\n","    for j in range(len(labels)):\n","        y_val_celltype.append(labels[j].item())\n","        y_val_pred_celltype.append(predicted[j].item())\n","        y_val_pred_celltype_scores.append(outputs.data[j].tolist())\n","\n","    predictions.append(predicted)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","\n","accuracy = (correct/total) * 100\n","print(\"Evaluate the Validation Predictions and Error: \\n\")\n","\n","print('Confusion matrix: \\n')\n","print(confusion_matrix(y_val_celltype, y_val_pred_celltype))\n","\n","print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_val_celltype, y_val_pred_celltype)))\n","print(\"- Precision Score: \" + str(precision_score(y_val_celltype, y_val_pred_celltype, average=\"micro\")))\n","print(\"- Recall Score: \" + str(recall_score(y_val_celltype, y_val_pred_celltype, average=\"micro\")))\n","print(\"- F1 Score: \" + str(f1_score(y_val_celltype, y_val_pred_celltype, average=\"micro\")))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1683445495116,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"PxUAI5fksWW1","outputId":"a428e816-67a0-4acb-8718-c3a95efa37c8"},"outputs":[],"source":["a2util.getClassificationROC(\"CellType\", \"Validation\", y_val_celltype, y_val_pred_celltype, 4, y_val_pred_celltype_scores)"]},{"cell_type":"markdown","metadata":{},"source":["Now Predict and Evaluate on the Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["correct, total = 0,  0\n","predictions = []\n","\n","# Set the Neural Network into evaluation (test) mode\n","net.eval()\n","\n","y_test_celltype = []\n","y_test_pred_celltype = []\n","y_test_pred_celltype_scores = []\n","\n","showBatch=True\n","\n","# Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n","for i, data in enumerate(celltype_test_dataloader, 0):\n","    inputs, labels = data\n","\n","    # This should convert the image tensors into vectors\n","    inputs = inputs.view(-1, 27 * 27 * 3)\n","\n","    outputs = net(inputs)\n","\n","    # outputs.data contains a tensor of size 4 for each record, with a score for each class. Use max\n","    # to select the class of the highest score for the prediction.\n","    class_score, predicted = torch.max(outputs.data, 1)\n","    \n","    # Loop through the batch, build the lists of the raw label and prediction values\n","    for j in range(len(labels)):\n","        y_test_celltype.append(labels[j].item())\n","        y_test_pred_celltype.append(predicted[j].item())\n","        y_test_pred_celltype_scores.append(outputs.data[j].tolist())\n","\n","    showBatch = False\n","    \n","    predictions.append(predicted)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","\n","print(\"Evaluate the Test Predictions and Error: \\n\")\n","\n","print('Confusion matrix: \\n')\n","print(confusion_matrix(y_test_celltype, y_test_pred_celltype))\n","\n","print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_test_celltype, y_test_pred_celltype)))\n","print(\"- Precision Score: \" + str(precision_score(y_test_celltype, y_test_pred_celltype, average=\"micro\")))\n","print(\"- Recall Score: \" + str(recall_score(y_test_celltype, y_test_pred_celltype, average=\"micro\")))\n","print(\"- F1 Score: \" + str(f1_score(y_test_celltype, y_test_pred_celltype, average=\"micro\")))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["a2util.getClassificationROC(\"CellType\", \"Test\", y_test_celltype, y_test_pred_celltype, 4, y_test_pred_celltype_scores)"]},{"cell_type":"markdown","metadata":{"id":"a1xI7RrpsWW1"},"source":["### Results\n","\n","No L2 Reg or Dropout (from file 06) - On Colab, Full data\n","- Training Accuracy: 0.995\n","- Validation: 0.763\n","\n","L2 with weight_decay=0.0001:\n","- **Training**\n","- Accuracy Score: 0.988898813321424\n","- Precision Score: 0.988898813321424\n","- Recall Score: 0.988898813321424\n","- F1 Score: 0.988898813321424\n","- **Validation**\n","- Accuracy Score: 0.7371483996120272\n","- Precision Score: 0.7371483996120272\n","- Recall Score: 0.7371483996120272\n","- F1 Score: 0.7371483996120272\n","\n","L2 with weight_decay=0.001:\n","- Training Accuracy: 1\n","- Validation: 0.56\n","- Very bad\n","\n","Dropout with rate 0.5\n","- **Training**\n","- Accuracy Score: 0.8442005869592957\n","- Precision Score: 0.8442005869592957\n","- Recall Score: 0.8442005869592957\n","- F1 Score: 0.8442005869592957\n","- **Validation**\n","- Accuracy Score: 0.7778855480116392\n","- Precision Score: 0.7778855480116392\n","- Recall Score: 0.7778855480116392\n","- F1 Score: 0.7778855480116392\n","\n","\n","Dropout with rate 0.25\n","- Training Accuracy: 1\n","- Validation: 0.82"]},{"cell_type":"markdown","metadata":{"id":"Ys43DeSksWW1"},"source":["<h1>Analysis of Performance and Accuracy</h1>\n","\n","<strong>Binary Classification - IsCancerous</strong>\n","<p>\n","In this model we see that the model has a very low training error and a high accuracy value, in this experiment achieving a 100% accuracy. This indicates that the model has <strong>low bias</strong>. However, when predicting on the validation dataset, the validation error rises. It can be seen that the Accuracy for the validation predictions is 84%, and the are under the ROC Curve is significantly less, with a value of 0.82. This is an indication of <strong>high variance</strong>, and in combination with low bias, indicates a possible problem of <strong>overfitting</strong>\n","</p>\n","\n","<strong>Multi-class Classification - Cell Type</strong>\n","<p>\n","Similarly, the Cell Type model has a very low training error and a high accuracy value of 99.17%, indicating that the model has <strong>low bias</strong>. However, the Cell Type model has a considerably worse accuracy of only 72.94% when predicting on the validation dataset. A ROC curve is generated for each class, and it can be seen that the model performs better at predicting some classes compared to others. The average the under the ROC Curve score, with a value of 0.84. This indication of <strong>high variance</strong>, and in combination with low bias, indicates a possible problem of <strong>overfitting</strong>.\n","</p>\n","\n","\n","<strong>Next Steps</strong>\n","<p>\n","Before fully understanding this model configuration and exploring possible different models, first attempt to improve the performance of this model and fix any issues it might have\n","</p>\n","<p>\n","Common methods to deal with overfitting is to apply some form of regularization, such as Early Stopping or Dropout\n","</p>\n","<p>\n","Additionally, we can attempt to improve the accuracy in general with some methods such as adding additional layers and/or neurons per layer. However, this isn't too useful at this stage since the training accuracy is very high, indicating the complexity of the model at this stage is sufficient to capture enough complexity for good prediction.\n","</p>\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"dcbc78149e46ccbab92a3f68a48c52feb0796c7e10dad8e3f1a2a5a780973376"}}},"nbformat":4,"nbformat_minor":0}
