{"cells":[{"cell_type":"markdown","metadata":{"id":"QDRW51eYK_pg"},"source":["<hr style=\"color:green\" />\n","<h1 style=\"color:green\">COSC2673 Assignment 2: Image Classification for Cancerous Cells</h1>\n","<h2 style=\"color:green\">File 30: Modelling on the Full data For the Canceorous Binary Classification</h2>\n","<hr style=\"color:green\" />\n","\n","<p>\n","After Reviewing files 22, 24, 25 and 26, we have experimented with improving the performance of the CNN with more complexity in the Convolutions and the Classifier layers. Taking into account performance increases but also processing time and complexity, the following model will be used here:\n","</p>\n","<ul>\n","<li>CNN with more complex convolutions (from file 24)</li>\n","</ul>\n","<p>\n","Now, in an attempt to improve the performance of both the Binary and the Multiclass models, this process will make use of the full data, ie. both the Main Labels file and the Extras Label file. Because handling these two will be different, processing will be split into separate notebooks.\n","</p>\n","\n","<ul>\n","<li>Binary IsCancerous (here): All data is labeled, so just add the new data to the dataset</li>\n","<li>Multiclass Cell Type: The data in the Extras file are unlabeled, so therefore, we will need to apply a Semi Supervised Learning approach</li>\n","</ul>\n","<p>\n","In this file, also load the Extra Label data, train the Binary Classification model and evaluate\n","</p>"]},{"cell_type":"code","execution_count":97,"metadata":{"executionInfo":{"elapsed":8285,"status":"ok","timestamp":1683527199852,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"LuaHh7dfK_pj"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import cv2\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torch.utils.data\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision.io import read_image\n"]},{"cell_type":"markdown","metadata":{"id":"4ayZvnueK_pk"},"source":["Configure this script as to whether it runs on Google Colab, or locally"]},{"cell_type":"code","execution_count":98,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1683527199853,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"vFtUm6uXK_pk"},"outputs":[],"source":["# When on Google Colab, running full training, change both to true. Locally, advised set both to false\n","isGoogleColab = False\n","useFullData = False"]},{"cell_type":"code","execution_count":99,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31235,"status":"ok","timestamp":1683527231083,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"zzl3NpfVK_pk","outputId":"164b6588-4bfc-4f20-ba1f-26e3563def4e"},"outputs":[],"source":["# In local, the base directory is the current directory\n","baseDirectory = \"./\"\n","\n","if isGoogleColab:\n","    from google.colab import drive\n","    \n","    # If this is running on Google colab, assume the notebook runs in a \"COSC2673\" folder, which also contains the data files \n","    # in a subfolder called \"image_classification_data\"\n","    drive.mount(\"/content/drive\")\n","    !ls /content/drive/'My Drive'/COSC2673/\n","\n","    # Import the directory so that custom python libraries can be imported\n","    import sys\n","    sys.path.append(\"/content/drive/MyDrive/COSC2673/\")\n","\n","    # Set the base directory to the Google Drive specific folder\n","    baseDirectory = \"/content/drive/MyDrive/COSC2673/\""]},{"cell_type":"markdown","metadata":{"id":"vZCfUn3EK_pl"},"source":["Import the custom python files that contain reusable code"]},{"cell_type":"code","execution_count":100,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6079,"status":"ok","timestamp":1683527237159,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"beWSdbauK_pl","outputId":"6c7403c5-4961-4c43-eb4e-a3be29d7dc1d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Random Seed: 266305\n"]}],"source":["import data_basic_utility as dbutil\n","import graphing_utility as graphutil\n","import statistics_utility as statsutil\n","\n","import a2_utility as a2util\n","import pytorch_utility as ptutil\n","from pytorch_utility import CancerBinaryDataset\n","from pytorch_utility import CancerCellTypeDataset\n","\n","\n","# randomSeed = dbutil.get_random_seed()\n","randomSeed = 266305\n","print(\"Random Seed: \" + str(randomSeed))"]},{"cell_type":"code","execution_count":101,"metadata":{"executionInfo":{"elapsed":767,"status":"ok","timestamp":1683527237916,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"zgIXX9BXK_pl"},"outputs":[],"source":["# this file should have previously been created in the root directory\n","dfImages = pd.read_csv(baseDirectory + \"images_main.csv\")\n","dfImagesExtra = pd.read_csv(baseDirectory + \"images_extra.csv\")"]},{"cell_type":"code","execution_count":102,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1683527237917,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"6_WFGagHK_pl","outputId":"b3660e2b-a19a-49c7-b513-eee0feb2ac3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["(16603, 5)\n","(1823, 5)\n","(1854, 5)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>ImageName</th>\n","      <th>isCancerous</th>\n","      <th>cellType</th>\n","      <th>trainValTest</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>./Image_classification_data/patch_images\\1.png</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>./Image_classification_data/patch_images\\10.png</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>./Image_classification_data/patch_images\\1000.png</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>./Image_classification_data/patch_images\\10000...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>./Image_classification_data/patch_images\\10001...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   index                                          ImageName  isCancerous  \\\n","0      0     ./Image_classification_data/patch_images\\1.png            0   \n","1      1    ./Image_classification_data/patch_images\\10.png            0   \n","2      3  ./Image_classification_data/patch_images\\1000.png            1   \n","3      4  ./Image_classification_data/patch_images\\10000...            0   \n","4      5  ./Image_classification_data/patch_images\\10001...            0   \n","\n","   cellType  trainValTest  \n","0         0             0  \n","1         0             0  \n","2         2             0  \n","3         1             0  \n","4         1             0  "]},"execution_count":102,"metadata":{},"output_type":"execute_result"}],"source":["# Get The training, validation and test Splits of the Main Data\n","dfImagesTrain = dfImages[dfImages[\"trainValTest\"] == 0]\n","dfImagesVal = dfImages[dfImages[\"trainValTest\"] == 1]\n","dfImagesTest = dfImages[dfImages[\"trainValTest\"] == 2]\n","\n","# Get the Extra Data training, validation and test Splits\n","dfExtraTrain = dfImagesExtra[dfImagesExtra[\"trainValTest\"] == 0]\n","dfExtraVal = dfImagesExtra[dfImagesExtra[\"trainValTest\"] == 1]\n","dfExtraTest = dfImagesExtra[dfImagesExtra[\"trainValTest\"] == 2]\n","\n","# Append the extra data to the data to use from the main data\n","dfImagesTrain = dfImagesTrain.append(dfExtraTrain).reset_index()\n","dfImagesVal = dfImagesVal.append(dfExtraVal).reset_index()\n","dfImagesTest = dfImagesTest.append(dfExtraTest).reset_index()\n","\n","print(dfImagesTrain.shape)\n","print(dfImagesVal.shape)\n","print(dfImagesTest.shape)\n","\n","dfImagesTrain.head()"]},{"cell_type":"markdown","metadata":{"id":"TZ1R94YdK_pn"},"source":["Note: The definition of the Custom Datasets for both the isCancerous data and the Cell Type data are defined in the pytorch_utility.py file.\n","\n","Also, rather than loading all the training images and calculating the mean and standard deviation values in here, that was run separately in file 05a.PyTorchGetMeanAndStd.ipynb\n","\n","Here we can just define the values to use, which shouldn't change unless the data is reloaded and a new train/validation/test split is generated"]},{"cell_type":"code","execution_count":103,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1683527237917,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"qaY5AA-tsWWw","outputId":"53ce6f3a-aeec-4d4d-a697-e083f0b01746"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0.8035, 0.5909, 0.7640])\n","tensor([0.1246, 0.1947, 0.1714])\n"]}],"source":["train_mean, train_std = ptutil.getTrainMeanAndStdTensors()\n","print(train_mean)\n","print(train_std)"]},{"cell_type":"code","execution_count":104,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683527237918,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"I8z3yX8ZK_pn"},"outputs":[],"source":["# Create a tranform operation that also normalizes the images according to the mean and standard deviations of the images\n","transform_normalize = transforms.Compose(\n","    [transforms.ToPILImage(),\n","    transforms.ToTensor(), \n","    transforms.Normalize(train_mean, train_std)])\n"]},{"cell_type":"code","execution_count":105,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683527237918,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"qb_33dFmK_pn"},"outputs":[],"source":["cancerous_training_data = None\n","\n","# Create a custom Dataset for the training and validation data\n","if useFullData:\n","    cancerous_training_data = CancerBinaryDataset(isGoogleColab, dfImagesTrain, baseDirectory, transform=transform_normalize)\n","else:\n","    # For testing in a small dataset\n","    dfImagesTrainTest = dfImagesTrain.iloc[range(1000), :].reset_index()\n","    cancerous_training_data = CancerBinaryDataset(isGoogleColab, dfImagesTrainTest, baseDirectory, transform=transform_normalize, target_transform=None)\n","\n","cancerous_validation_data = CancerBinaryDataset(isGoogleColab, dfImagesVal, baseDirectory, transform=transform_normalize, target_transform=None)\n","cancerous_test_data = CancerBinaryDataset(isGoogleColab, dfImagesTest, baseDirectory, transform=transform_normalize, target_transform=None)\n","\n","# Create data loaders\n","cancerous_train_dataloader = DataLoader(cancerous_training_data, batch_size=32, shuffle=True, num_workers=2)\n","cancerous_val_dataloader = DataLoader(cancerous_validation_data, batch_size=32, shuffle=True, num_workers=2)\n","cancerous_test_dataloader = DataLoader(cancerous_test_data, batch_size=32, shuffle=True, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"F4jD7E2PK_po"},"source":["# Early Stopping\n","\n","Using the basic model (class), refactor the code such that the test and validation predictions are made at the end of each epoch and the loss difference between training error and validation error are calculated. Use the F1 Score as the metric\n","\n","Also implement a \"Patience\" level, where the patience is the number of consecutive epochs that can occur with no improvement in loss before the process is stopped early."]},{"cell_type":"code","execution_count":106,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1683527237919,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"0two1S9gK_po"},"outputs":[],"source":["# Create a class for the Neural Network\n","class PT_CNN_IsCancerous(nn.Module):\n","\n","    # In the constructor, initialize the layers to use\n","    def __init__(self, dropout_prob):\n","        super(PT_CNN_IsCancerous, self).__init__()\n","        self.dropProb = dropout_prob   \n","\n","        # 2 Dropout operations on the first two layers, don't do any dropouts on the output layer\n","        self.classifier = nn.Sequential(\n","            # Convolution Layers\n","            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5, stride=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),   \n","\n","            nn.Flatten(),\n","\n","            # Classifier Layers\n","            nn.Dropout(p=self.dropProb),\n","            nn.Linear(256 * 2 * 2, 1024),\n","            nn.ReLU(inplace=True),   \n","            \n","            nn.Dropout(p=self.dropProb),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(inplace=True),    \n","\n","            nn.Linear(512, 2),           \n","            nn.Sigmoid()\n","        )\n","\n","    # Create the forward function, which is used in training\n","    def forward(self, x):\n","        # Run the Sequential Classifier Layer\n","        return self.classifier(x)        \n"]},{"cell_type":"markdown","metadata":{"id":"pO5dPhy6K_po"},"source":["Now train the Fully Connected Neural Network Model.\n","\n","During training, we will use the following:\n","- Softmax Cross Entropy Loss as our Loss function. This is a good Loss function that basically converts scores for each class into probabilities\n","- The Adam Optimizer, which is a version of Gradient Descent\n","- Initially, just 10 epochs"]},{"cell_type":"markdown","metadata":{"id":"xc7rjcsiCKbe"},"source":["Create a function to predict"]},{"cell_type":"code","execution_count":107,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683527237919,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"7SAdIfM-CKbe"},"outputs":[],"source":["def predictCancerousOnDataSetF1(net, setName, dataloader, printResult=True, printFullResults=False):\n","    correct, total = 0,  0\n","    predictions = []\n","\n","    y_cancerous = []\n","    y_pred_cancerous = []\n","\n","    # Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n","    for i, data in enumerate(dataloader, 0):\n","        inputs, labels = data\n","\n","        outputs = net(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        \n","        # Loop through the batch, build the lists of the raw label and prediction values\n","        for j in range(len(labels)):\n","            y_cancerous.append(labels[j].item())\n","            y_pred_cancerous.append(predicted[j].item())\n","\n","        predictions.append(predicted)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    f1Score = f1_score(y_cancerous, y_pred_cancerous)\n","\n","    if printFullResults:\n","        print(setName + \":\")        \n","        print('Confusion matrix: \\n')\n","        print(confusion_matrix(y_cancerous, y_pred_cancerous))\n","        print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_cancerous, y_pred_cancerous)))\n","        print(\"- Precision Score: \" + str(precision_score(y_cancerous, y_pred_cancerous)))\n","        print(\"- Recall Score: \" + str(recall_score(y_cancerous, y_pred_cancerous)))\n","        print(\"- F1 Score: \" + str(f1Score))\n","    elif printResult:\n","        print(\"- \" + setName + \" F1: \" + str(f1Score))        \n","\n","    return f1Score, y_cancerous, y_pred_cancerous"]},{"cell_type":"code","execution_count":108,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2953420,"status":"ok","timestamp":1683530191330,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"vQUAMyGuK_po","outputId":"1249f0c1-16d1-4685-e541-dd82aca7d58e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting Epoch 0...\n","- IsBetter: 2.114980619238871e-05 : 99999\n","Starting Epoch 1...\n","- IsGoodStep\n","Starting Epoch 2...\n","- IsGoodStep\n","Starting Epoch 3...\n","- IsGoodStep\n","Starting Epoch 4...\n","- IsNotBetter: 0.05594851998633665 : 2.114980619238871e-05\n","Starting Epoch 5...\n","- IsNotBetter: 0.06394172793402575 : 2.114980619238871e-05\n","Starting Epoch 6...\n","- IsNotBetter: 0.010867659488725478 : 2.114980619238871e-05\n","Early Stopping occurred at Epoch 6\n","Training:\n","Confusion matrix: \n","\n","[[385  41]\n"," [127 447]]\n","\n","- Accuracy Score: 0.832\n","- Precision Score: 0.9159836065573771\n","- Recall Score: 0.7787456445993032\n","- F1 Score: 0.8418079096045198\n","Test:\n","Confusion matrix: \n","\n","[[579 271]\n"," [ 90 914]]\n","\n","- Accuracy Score: 0.8052858683926645\n","- Precision Score: 0.7713080168776372\n","- Recall Score: 0.9103585657370518\n","- F1 Score: 0.8350845134764734\n","------\n","- Completed with Dropout Prob:  0.25\n","- Test F1 was 0.8350845134764734\n","- Training F1 was 0.8418079096045198\n","------\n","Starting Epoch 0...\n","- IsBetter: 0.0075346124974591255 : 99999\n","Starting Epoch 1...\n","- IsGoodStep\n","Starting Epoch 2...\n","- IsGoodStep\n","Starting Epoch 3...\n","- IsNotBetter: 0.0495816322037681 : 0.0075346124974591255\n","Starting Epoch 4...\n","- IsNotBetter: 0.0742849883042247 : 0.0075346124974591255\n","Starting Epoch 5...\n","- IsNotBetter: 0.09167373516423583 : 0.0075346124974591255\n","Early Stopping occurred at Epoch 5\n","Training:\n","Confusion matrix: \n","\n","[[333  93]\n"," [ 30 544]]\n","\n","- Accuracy Score: 0.877\n","- Precision Score: 0.8540031397174255\n","- Recall Score: 0.9477351916376306\n","- F1 Score: 0.8984310487200661\n","Test:\n","Confusion matrix: \n","\n","[[372 478]\n"," [ 20 984]]\n","\n","- Accuracy Score: 0.7313915857605178\n","- Precision Score: 0.6730506155950753\n","- Recall Score: 0.9800796812749004\n","- F1 Score: 0.7980535279805353\n","------\n","- Completed with Dropout Prob:  0.275\n","- Test F1 was 0.7980535279805353\n","- Training F1 was 0.8984310487200661\n","------\n","Starting Epoch 0...\n","- IsBetter: 0.012604602092289419 : 99999\n","Starting Epoch 1...\n","- IsGoodStep\n","Starting Epoch 2...\n","- IsNotBetter: 0.03127098244116866 : 0.006624205970218511\n","Starting Epoch 3...\n","- IsNotBetter: 0.08633713449456348 : 0.006624205970218511\n","Starting Epoch 4...\n","- IsNotBetter: 0.04002728788519294 : 0.006624205970218511\n","Early Stopping occurred at Epoch 4\n","Training:\n","Confusion matrix: \n","\n","[[374  52]\n"," [ 93 481]]\n","\n","- Accuracy Score: 0.855\n","- Precision Score: 0.9024390243902439\n","- Recall Score: 0.837979094076655\n","- F1 Score: 0.8690153568202349\n","Test:\n","Confusion matrix: \n","\n","[[518 332]\n"," [ 62 942]]\n","\n","- Accuracy Score: 0.7874865156418555\n","- Precision Score: 0.7394034536891679\n","- Recall Score: 0.9382470119521913\n","- F1 Score: 0.8270412642669008\n","------\n","- Completed with Dropout Prob:  0.3\n","- Test F1 was 0.8270412642669008\n","- Training F1 was 0.8690153568202349\n","------\n","Starting Epoch 0...\n","- IsBetter: 0.011194741493473992 : 99999\n","Starting Epoch 1...\n","- IsNotBetter: 0.052760104151057585 : 0.011194741493473992\n","Starting Epoch 2...\n","- IsNotBetter: 0.08444111538385568 : 0.011194741493473992\n","Starting Epoch 3...\n","- IsGoodStep\n","Starting Epoch 4...\n","- IsNotBetter: 0.05488110520202938 : 0.011194741493473992\n","Starting Epoch 5...\n","- IsNotBetter: 0.08792352055950392 : 0.011194741493473992\n","Starting Epoch 6...\n","- IsNotBetter: 0.09264968912358396 : 0.011194741493473992\n","Early Stopping occurred at Epoch 6\n","Training:\n","Confusion matrix: \n","\n","[[352  74]\n"," [ 42 532]]\n","\n","- Accuracy Score: 0.884\n","- Precision Score: 0.8778877887788779\n","- Recall Score: 0.926829268292683\n","- F1 Score: 0.9016949152542373\n","Test:\n","Confusion matrix: \n","\n","[[399 451]\n"," [ 24 980]]\n","\n","- Accuracy Score: 0.743797195253506\n","- Precision Score: 0.6848357791754018\n","- Recall Score: 0.9760956175298805\n","- F1 Score: 0.8049281314168377\n","------\n","- Completed with Dropout Prob:  0.325\n","- Test F1 was 0.8049281314168377\n","- Training F1 was 0.9016949152542373\n","------\n","Starting Epoch 0...\n","- IsBetter: 0.011649082237317532 : 99999\n","Starting Epoch 1...\n","- IsGoodStep\n","Starting Epoch 2...\n","- IsNotBetter: 0.07629729469529234 : 0.011649082237317532\n","Starting Epoch 3...\n","- IsGoodStep\n","Starting Epoch 4...\n","- IsNotBetter: 0.05086380047564476 : 0.00010010833151596898\n","Starting Epoch 5...\n","- IsNotBetter: 0.04996427681381577 : 0.00010010833151596898\n","Starting Epoch 6...\n","- IsNotBetter: 0.09365750403166873 : 0.00010010833151596898\n","Early Stopping occurred at Epoch 6\n","Training:\n","Confusion matrix: \n","\n","[[348  78]\n"," [ 37 537]]\n","\n","- Accuracy Score: 0.885\n","- Precision Score: 0.8731707317073171\n","- Recall Score: 0.9355400696864111\n","- F1 Score: 0.9032800672834315\n","Test:\n","Confusion matrix: \n","\n","[[396 454]\n"," [ 23 981]]\n","\n","- Accuracy Score: 0.7427184466019418\n","- Precision Score: 0.6836236933797909\n","- Recall Score: 0.9770916334661355\n","- F1 Score: 0.8044280442804427\n","------\n","- Completed with Dropout Prob:  0.35\n","- Test F1 was 0.8044280442804427\n","- Training F1 was 0.9032800672834315\n","------\n"]}],"source":["bestTestF1 = -1\n","bestTrainingF1 = -1\n","bestDropoutProb = -1\n","\n","# Don't include 0.5, we've used it before and have results\n","# dropoutVals = [0.1, 0.2, 0.3, 0.4, 0.6]\n","dropoutVals = [0.25, 0.275, 0.3, 0.325, 0.35]\n","\n","# Tune by dropout rate, loop through them and train each\n","for dp in dropoutVals:\n","    # set the Learning Rate to use\n","    learning_rate = 0.0001\n","    maxEpochs = 30\n","    patience = 3\n","    disableEarlyStopping = False\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # Use this for L2 Regularization\n","    # net = PT_NN_IsCancerous()\n","    # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.0001)\n","\n","    # Use this for dropouts\n","    net = PT_CNN_IsCancerous(dropout_prob=dp)\n","    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n","\n","    bestErrorDiff = 99999\n","    concurrentNonImproves = 0\n","    currentEpoch = 0\n","\n","    bestValF1 = -1\n","    lstEpochs = []\n","    lstTrainF1s = []\n","    lstValF1s = []\n","    for epoch in range(maxEpochs):\n","        print(\"Starting Epoch \" + str(epoch) + \"...\")\n","        currentEpoch = epoch\n","\n","        # Set the Neural Network into training mode\n","        net.train()\n","\n","        # Train through this epoch\n","        for i, data in enumerate(cancerous_train_dataloader, 0):\n","            # Get the inputs\n","            inputs, labels = data\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # Perform Forward and Backward propagation then optimize the weights\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","        \n","        # Set the Neural Network into evaluation (test) mode, so we can evaluate both training and validation error\n","        net.eval()        \n","        trainingF1, y_train_cancerous, y_train_pred_cancerous = predictCancerousOnDataSetF1(net, \"Training\", cancerous_train_dataloader, False, False)\n","        validationF1, y_val_cancerous, y_val_pred_cancerous = predictCancerousOnDataSetF1(net, \"Validation\", cancerous_val_dataloader, False, False)\n","\n","        errorDiff = trainingF1 - validationF1\n","        # print(\"- F1 Difference: \" + str(errorDiff))\n","\n","        lstEpochs.append(epoch)\n","        lstTrainF1s.append(trainingF1)\n","        lstValF1s.append(validationF1)\n","\n","        if epoch > 0 and (validationF1 - bestValF1 > 0.01):        \n","            # There is at least percentage point improvement in the validation F1, count this as a \n","            # good iteration, regardless of the error difference\n","            print(\"- IsGoodStep\")\n","            concurrentNonImproves = 0\n","            if errorDiff > 0 and errorDiff < bestErrorDiff:  \n","                bestErrorDiff = errorDiff        \n","        elif errorDiff < bestErrorDiff:        \n","            # This epoch is an improvement on the last, so we will continue. the concurrent non improve counts reset for the patience\n","            print(\"- IsBetter: \" + str(errorDiff) + \" : \" + str(bestErrorDiff))        \n","            concurrentNonImproves = 0\n","            if errorDiff > 0:\n","                bestErrorDiff = errorDiff\n","        else:\n","            # This epoch has the same or worse performance than the last. Check if we have reached the patience, if so, then stop early\n","            print(\"- IsNotBetter: \" + str(errorDiff) + \" : \" + str(bestErrorDiff))        \n","            concurrentNonImproves += 1\n","            if disableEarlyStopping == False:\n","                if concurrentNonImproves >= patience:\n","                    print(\"Early Stopping occurred at Epoch \" + str(epoch))\n","                    break\n","\n","        # update the val F1 score from the previous epoch if it's the best\n","        if validationF1 > bestValF1:\n","            bestValF1 = validationF1\n","\n","    # Use the final NN model to predict on the training data and test data\n","    trainingF1, y_train_cancerous, y_train_pred_cancerous = predictCancerousOnDataSetF1(net, \"Training\", cancerous_train_dataloader, True, True) \n","    testF1, y_test_cancerous, y_test_pred_cancerous = predictCancerousOnDataSetF1(net, \"Test\", cancerous_test_dataloader, True, True)       \n","\n","    print(\"------\")\n","    print(\"- Completed with Dropout Prob:  \" + str(dp))\n","    print(\"- Test F1 was \" + str(testF1))\n","    print(\"- Training F1 was \" + str(trainingF1))\n","    print(\"------\")\n","\n","    if testF1 > bestTestF1:\n","        bestTrainingF1 = trainingF1\n","        bestTestF1 = testF1\n","        bestDropoutProb = dp\n","    "]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The best found Dropout Probability is 0.25\n","Test F1 was 0.8350845134764734\n","Training F1 was 0.8418079096045198\n"]}],"source":["print(\"The best found Dropout Probability is \" + str(bestDropoutProb))\n","\n","print(\"Test F1 was \" + str(bestTestF1))\n","print(\"Training F1 was \" + str(bestTrainingF1))"]},{"cell_type":"markdown","metadata":{"id":"LezmNEykCKbg"},"source":["Look at the training and validation loss plot to track how the model improved during epochs"]},{"cell_type":"code","execution_count":110,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":490},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1683530191331,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"OznU1dWACKbg","outputId":"3430c067-47f7-4cd6-c8b6-109adb1208c7"},"outputs":[],"source":["# dfLoss = pd.DataFrame({ 'epoch': lstEpochs, 'train': lstTrainF1s, 'validation': lstValF1s })\n","# graphutil.graphBasicTwoSeries(dfLoss, \"epoch\", \"train\", \"validation\", \"IsCancerous Training and Validation F1\", \n","#         \"Epoch\", \"Training F1\", \"Validation F1\")"]},{"cell_type":"markdown","metadata":{"id":"CEB7hSkAK_po"},"source":["Training Time in Nelson's Local Environment on the full data takes a very long time, stopped after 100 minutes. This will need to be done in Colab.\n","\n","First, Predict on the training data so that we can find the training error."]},{"cell_type":"code","execution_count":111,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14613,"status":"ok","timestamp":1683530205937,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"f1E180BesWWy","outputId":"f6e5c3e9-f40a-4fa8-c4d9-29fcbdeb1169"},"outputs":[],"source":["# # Use the final NN model to predict on the training data\n","# trainingF1, y_train_cancerous, y_train_pred_cancerous = predictCancerousOnDataSetF1(net, \"Training\", cancerous_train_dataloader, True, True)"]},{"cell_type":"code","execution_count":112,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":508},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1683530205938,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"4M0Jb0fosWWy","outputId":"957ef8c3-e39e-4688-9b56-65bf55aa13ba"},"outputs":[],"source":["# a2util.getClassificationROC(\"IsCancerous\", \"Training\", y_train_cancerous, y_train_pred_cancerous)"]},{"cell_type":"markdown","metadata":{"id":"fbvP6a19sWWy"},"source":["Now Predict according to the Validation data and evaluate. While looping through here, we will need to get out the Labels from the data loader, because the order of predictions in the batches do not match the order of the original Target values in the dataset (because we turned Shuffle on)"]},{"cell_type":"code","execution_count":113,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1567,"status":"ok","timestamp":1683530207501,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"ithF1bLMK_pp","outputId":"e8404f51-0136-4da5-8919-385846d654ed"},"outputs":[],"source":["# testF1, y_test_cancerous, y_test_pred_cancerous = predictCancerousOnDataSetF1(net, \"Test\", cancerous_test_dataloader, True, True)"]},{"cell_type":"code","execution_count":114,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":508},"executionInfo":{"elapsed":957,"status":"ok","timestamp":1683530208455,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"ivOulSepsWWz","outputId":"af3f23f2-9aab-48d3-b01b-e98f5b613625"},"outputs":[],"source":["# a2util.getClassificationROC(\"IsCancerous\", \"Test\", y_test_cancerous, y_test_pred_cancerous, 2)"]},{"cell_type":"markdown","metadata":{},"source":["# Results\n","\n","Append Results for both Binary IsCancerous and Cell Type here"]},{"cell_type":"markdown","metadata":{},"source":["### IsCancerous Results\n","\n","Dropout = 0.5:\n","Without Early Stopping\n","- **Training**\n","- Accuracy Score: 0.889\n","- Precision Score: 0.8930390492359932\n","- Recall Score: 0.9163763066202091\n","- F1 Score: 0.9045571797076526\n","- **Test**\n","- Accuracy Score: 0.7653721682847896\n","- Precision Score: 0.709653647752395\n","- Recall Score: 0.9591633466135459\n","- F1 Score: 0.815756035578145\n","\n","With Early Stopping\n","- **Training**\n","- Accuracy Score: 0.875\n","- Precision Score: 0.8917975567190227\n","- Recall Score: 0.8902439024390244\n","- F1 Score: 0.891020052310375\n","- **Test**\n","- Accuracy Score: 0.7411003236245954\n","- Precision Score: 0.6834733893557423\n","- Recall Score: 0.9721115537848606\n","- F1 Score: 0.8026315789473685\n","\n","\n","After Tuning First Round, with [0.1, 0.2, 0.3, 0.4, 0.6]\n","- The best found Dropout Probability is 0.3\n","- Test F1 was 0.8337777777777777\n","- Training F1 was 0.8847184986595175\n","\n","\n","After Tuning Second Round, with [0.25, 0.275, 0.3, 0.325, 0.35]\n","- The best found Dropout Probability is 0.25\n","- Test F1 was 0.8350845134764734\n","- Training F1 was 0.8418079096045198"]},{"cell_type":"markdown","metadata":{"id":"Ys43DeSksWW1"},"source":["<h1>Analysis of Performance and Accuracy</h1>\n","\n","<h3>Is Cancerous</h3>\n","<p>\n","The best performing model after initial CNN experiments, according to F1 Score, is the CNN with 3 Layer Classifier, with no Early stopping. the F1 Score is <strong>0.907</strong>, which is only a 0.01 improvement on the Basic NN 2 Layer model.\n","</p>\n","<p>\n","However, we can see that the difference between the Training F1 and the Test F1 in this CNN is a lot smaller, with the Training F1 being 0.93. This is a situation where we have higher bias but less variance, with higher bias compared to other experiments. Therefore, it may be possible to model with more accuracy to reduce the bias, however, we must be careful not to overfit.\n","</p>\n","\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"dcbc78149e46ccbab92a3f68a48c52feb0796c7e10dad8e3f1a2a5a780973376"}}},"nbformat":4,"nbformat_minor":0}
