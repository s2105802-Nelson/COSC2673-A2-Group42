{"cells":[{"cell_type":"markdown","metadata":{"id":"QDRW51eYK_pg"},"source":["<hr style=\"color:green\" />\n","<h1 style=\"color:green\">COSC2673 Assignment 2: Image Classification for Cancerous Cells</h1>\n","<h2 style=\"color:green\">File 20: PyTorch First CNN model test on Main data, using AlexNet Config</h2>\n","<hr style=\"color:green\" />\n","\n","<p>\n","In this file, Train a basic Convolutional Neural Network (CNN) with Pytorch, using the AlexNet configuration. The AlexNet configuration contains 5 convolutional layers, 3 max pooling and an average pooling layer, and in the end it has 3 fully-connected layers, the last of which classifies images in 1000 different classes.\n","</p>\n","<p>\n","The original CNN implementation kept having bugs. Made some changes to layers and pooling to make it run, not sure if they are right.\n","</p>\n","<p>\n","This CNN implementation runs, but doesn't seem to be working properly, gives a very low precision. About 0.39 for isCancerous, where the FC NN is giving at least 0.8. <strong> This notebook is bugfixing</strong>\n","</p>"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":3348,"status":"ok","timestamp":1683435741349,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"LuaHh7dfK_pj"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import cv2\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torch.utils.data\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision.io import read_image\n"]},{"cell_type":"markdown","metadata":{"id":"4ayZvnueK_pk"},"source":["Configure this script as to whether it runs on Google Colab, or locally"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1683435741350,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"vFtUm6uXK_pk"},"outputs":[],"source":["# When on Google Colab, running full training, change both to true. Locally, advised set both to false\n","isGoogleColab = False\n","useFullData = False"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2223,"status":"ok","timestamp":1683435743567,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"zzl3NpfVK_pk","outputId":"8fe74720-08bc-4b2a-f5ed-33ec7c328ae8"},"outputs":[],"source":["# In local, the base directory is the current directory\n","baseDirectory = \"./\"\n","\n","if isGoogleColab:\n","    from google.colab import drive\n","    \n","    # If this is running on Google colab, assume the notebook runs in a \"COSC2673\" folder, which also contains the data files \n","    # in a subfolder called \"image_classification_data\"\n","    drive.mount(\"/content/drive\")\n","    !ls /content/drive/'My Drive'/COSC2673/\n","\n","    # Import the directory so that custom python libraries can be imported\n","    import sys\n","    sys.path.append(\"/content/drive/MyDrive/COSC2673/\")\n","\n","    # Set the base directory to the Google Drive specific folder\n","    baseDirectory = \"/content/drive/MyDrive/COSC2673/\""]},{"cell_type":"markdown","metadata":{"id":"vZCfUn3EK_pl"},"source":["Import the custom python files that contain reusable code"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1173,"status":"ok","timestamp":1683435744738,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"beWSdbauK_pl","outputId":"98890812-074d-480c-fc73-afb5e97ffc66"},"outputs":[{"name":"stdout","output_type":"stream","text":["Random Seed: 266305\n"]}],"source":["import data_basic_utility as dbutil\n","import graphing_utility as graphutil\n","import statistics_utility as statsutil\n","\n","import a2_utility as a2util\n","import pytorch_utility as ptutil\n","from pytorch_utility import CancerBinaryDataset\n","from pytorch_utility import CancerCellTypeDataset\n","\n","\n","# randomSeed = dbutil.get_random_seed()\n","randomSeed = 266305\n","print(\"Random Seed: \" + str(randomSeed))"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1683435744739,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"zgIXX9BXK_pl"},"outputs":[],"source":["# this file should have previously been created in the root directory\n","dfImages = pd.read_csv(baseDirectory + \"images_main.csv\")"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1683435744739,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"6_WFGagHK_pl","outputId":"43777e00-4fa5-4204-97df-a877ce69909b"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>ImageName</th>\n","      <th>isCancerous</th>\n","      <th>cellType</th>\n","      <th>trainValTest</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>./Image_classification_data/patch_images\\1.png</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>./Image_classification_data/patch_images\\10.png</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>./Image_classification_data/patch_images\\1000.png</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>./Image_classification_data/patch_images\\10000...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>./Image_classification_data/patch_images\\10001...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   index                                          ImageName  isCancerous  \\\n","0      0     ./Image_classification_data/patch_images\\1.png            0   \n","1      1    ./Image_classification_data/patch_images\\10.png            0   \n","2      3  ./Image_classification_data/patch_images\\1000.png            1   \n","3      4  ./Image_classification_data/patch_images\\10000...            0   \n","4      5  ./Image_classification_data/patch_images\\10001...            0   \n","\n","   cellType  trainValTest  \n","0         0             0  \n","1         0             0  \n","2         2             0  \n","3         1             0  \n","4         1             0  "]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["# Get The training Split and the Validation Split\n","dfImagesTrain = dfImages[dfImages[\"trainValTest\"] == 0].reset_index()\n","dfImagesVal = dfImages[dfImages[\"trainValTest\"] == 1].reset_index()\n","\n","dfImagesTrain.head()"]},{"cell_type":"markdown","metadata":{"id":"TZ1R94YdK_pn"},"source":["Note: The definition of the Custom Datasets for both the isCancerous data and the Cell Type data are defined in the pytorch_utility.py file.\n","\n","Also, rather than loading all the training images and calculating the mean and standard deviation values in here, that was run separately in file 05a.PyTorchGetMeanAndStd.ipynb\n","\n","Here we can just define the values to use, which shouldn't change unless the data is reloaded and a new train/validation/test split is generated"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1683435744740,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"whsaUaCCRNpG","outputId":"7ddb5bc9-379f-4429-8ee4-46d4f0b2014e"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0.8035, 0.5909, 0.7640])\n","tensor([0.1246, 0.1947, 0.1714])\n"]}],"source":["train_mean, train_std = ptutil.getTrainMeanAndStdTensors()\n","print(train_mean)\n","print(train_std)"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683435744740,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"I8z3yX8ZK_pn"},"outputs":[],"source":["# Create a tranform operation that also normalizes the images according to the mean and standard deviations of the images\n","transform_normalize = transforms.Compose(\n","    [transforms.ToPILImage(),\n","    transforms.ToTensor(), \n","    transforms.Normalize(train_mean, train_std)])\n"]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683435744741,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"qb_33dFmK_pn"},"outputs":[],"source":["cancerous_training_data = None\n","\n","# Create a custom Dataset for the training and validation data\n","if useFullData:\n","    cancerous_training_data = CancerBinaryDataset(isGoogleColab, dfImagesTrain, baseDirectory, transform=transform_normalize)\n","else:\n","    # For testing in a small dataset\n","    dfImagesTrainTest = dfImagesTrain.iloc[range(500), :].reset_index()\n","    cancerous_training_data = CancerBinaryDataset(isGoogleColab, dfImagesTrainTest, baseDirectory, transform=transform_normalize, target_transform=None)\n","\n","cancerous_validation_data = CancerBinaryDataset(isGoogleColab, dfImagesVal, baseDirectory, transform=transform_normalize, target_transform=None)\n","\n","# Create data loaders\n","cancerous_train_dataloader = DataLoader(cancerous_training_data, batch_size=32, shuffle=True, num_workers=2)\n","cancerous_val_dataloader = DataLoader(cancerous_validation_data, batch_size=32, shuffle=True, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"F4jD7E2PK_po"},"source":["Now, create a class for the basic, Fully Connected Neural Network. For this basic NN, we will use 3 fully connected layers. The number of features in this will be 27 x 27 x 3, or 2187.\n","\n","Layer 1: Input is the images, which are 27 x 27 pixels, with 3 color values (RGB). Experiment initially with 1458 nodes\n","Layer 2: Input is 1458 from the the previous layer, down to 729\n","Layer 3: Input is 729 from the the previous layer, since this is a binary classification problem, the output will be 2 classes\n","\n","In this, we will use the **ReLU** Activation Function. This is the Rectified Linear Unit function, which allows the function to become non-linear"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683435744741,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"0two1S9gK_po"},"outputs":[],"source":["# Create a class for the Neural Network\n","class PT_CNN_AlexNet_IsCancerous(nn.Module):\n","\n","    # In the constructor, initialize the layers to use\n","    def __init__(self):\n","        super(PT_CNN_AlexNet_IsCancerous, self).__init__()\n","\n","        # first, define the subsampling methods. Though they are used multiple times, these are the\n","        # operations, so only need to be defined once\n","        # Note: added a second max pooling operation with padding 1 to fix some output sizing issue\n","        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n","        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n","\n","        # define the ReLU Activation method to use\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        # define the convolution layers\n","\n","        # input should be 27x27x3. Apply a 3x3 filter, therefore, output should be 25x25x32 (channels aka feature maps)\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1)\n","        # There will be a Relu\n","        # Then a MaxPool of 2x2, halving the dimensions per feature map\n","        # So input is 12x12x32. Apply a 3x3 filter, output is 10x10x64\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)    \n","        # There will be a Relu\n","        # Then a MaxPool of 2x2, halving the dimensions per feature map   \n","        # So input is now  5x5x64. --- Note, these are getting too small, probably losing a lot of details\n","        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n","        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n","        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n","        \n","        # define the fully connected neural layers\n","        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n","        self.fc2 = nn.Linear(4096, 4096)\n","        self.fc3 = nn.Linear(4096, 2)\n","\n","    # Create the forward function, which is used in training\n","    def forward(self, x):\n","\n","        print(\"Init Shape: \" + str(x.shape))\n","\n","        # Process the first 2 convolution layers, applying maxpooling\n","        x = self.relu(self.conv1(x))\n","        print(\"   Step 1: \" + str(x.shape))\n","        x = self.maxpool(x)\n","        print(\"   Step 2: \" + str(x.shape))\n","        x = self.relu(self.conv2(x))\n","        print(\"   Step 3: \" + str(x.shape))\n","        x = self.maxpool(x)\n","        print(\"   Step 4: \" + str(x.shape))\n","        \n","        # Then process the remaining convolution layers without any pooling\n","        x = self.relu(self.conv3(x))\n","        x = self.relu(self.conv4(x))\n","        x = self.relu(self.conv5(x))\n","\n","        # Then apply a max pool and average pool on the result\n","        x = self.maxpool2(x)\n","        x = self.avgpool(x)\n","\n","        # This should convert to tensors that are acceptable for the input into the NN 3 layers\n","        x = x.view(x.size(0), 256 * 6 * 6)\n","\n","        # Now process the 3 layers of the Fully Connected NN\n","        x = self.relu(self.fc1(x))\n","        x = self.relu(self.fc2(x))\n","        x = self.relu(self.fc3(x))        \n","\n","        # return the result\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"pO5dPhy6K_po"},"source":["Now train the Fully Connected Neural Network Model.\n","\n","During training, we will use the following:\n","- Softmax Cross Entropy Loss as our Loss function. This is a good Loss function that basically converts scores for each class into probabilities\n","- The Adam Optimizer, which is a version of Gradient Descent\n","- Initially, just 10 epochs"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":199404,"status":"ok","timestamp":1683435944136,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"vQUAMyGuK_po","outputId":"e9f12b8d-9026-4cbf-e4f9-207996beeacf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting Epoch 0...\n","Starting Epoch 1...\n","Starting Epoch 2...\n","Starting Epoch 3...\n","Starting Epoch 4...\n","Starting Epoch 5...\n","Starting Epoch 6...\n","Starting Epoch 7...\n"]}],"source":["# \n","\n","# set the Learning Rate to use\n","learning_rate = 0.0001\n","epochsToUse = 10\n","\n","net = PT_CNN_AlexNet_IsCancerous()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n","\n","for epoch in range(epochsToUse):\n","    print(\"Starting Epoch \" + str(epoch) + \"...\")\n","    for i, data in enumerate(cancerous_train_dataloader, 0):\n","        # Get the inputs\n","        inputs, labels = data\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Perform Forward and Backward propagation then optimize the weights\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"CEB7hSkAK_po"},"source":["Training Time in Nelson's Local Environment on the full data takes a very long time, stopped after 100 minutes. This will need to be done in Colab.\n","\n","First, Predict on the training data so that we can find the training error."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11255,"status":"ok","timestamp":1683435955377,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"Dutbv_FKRNpH","outputId":"255a192f-c2ac-4420-fef1-c5e5327c1ec3"},"outputs":[],"source":["#\n","\n","correct, total = 0,  0\n","predictions = []\n","\n","# Set the Neural Network into evaluation (test) mode\n","net.eval()\n","\n","step=0\n","\n","y_train_cancerous = []\n","y_train_pred_cancerous = []\n","\n","# Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n","for i, data in enumerate(cancerous_train_dataloader, 0):\n","    inputs, labels = data\n","\n","    outputs = net(inputs)\n","    _, predicted = torch.max(outputs.data, 1)\n","    \n","    # Loop through the batch, build the lists of the raw label and prediction values\n","    for j in range(len(labels)):\n","        y_train_cancerous.append(labels[j].item())\n","        y_train_pred_cancerous.append(predicted[j].item())\n","\n","    predictions.append(predicted)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","\n","print(\"Evaluate the Training Predictions and Error: \\n\")\n","\n","print('Confusion matrix: \\n')\n","print(confusion_matrix(y_train_cancerous, y_train_pred_cancerous))\n","\n","print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_train_cancerous, y_train_pred_cancerous)))\n","print(\"- Precision Score: \" + str(precision_score(y_train_cancerous, y_train_pred_cancerous)))\n","print(\"- Recall Score: \" + str(recall_score(y_train_cancerous, y_train_pred_cancerous)))\n","print(\"- F1 Score: \" + str(f1_score(y_train_cancerous, y_train_pred_cancerous)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":507},"executionInfo":{"elapsed":736,"status":"ok","timestamp":1683435956110,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"vLttfi9nRNpH","outputId":"c55d1ebd-ddbf-4454-e528-d20770062625"},"outputs":[],"source":["a2util.getClassificationROC(\"IsCancerous\", \"Training\", y_train_cancerous, y_train_pred_cancerous)"]},{"cell_type":"markdown","metadata":{"id":"mUdRn526RNpI"},"source":["Now Predict according to the Validation data and evaluate. While looping through here, we will need to get out the Labels from the data loader, because the order of predictions in the batches do not match the order of the original Target values in the dataset (because we turned Shuffle on)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":159364,"status":"ok","timestamp":1683436115471,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"ithF1bLMK_pp","outputId":"b79a640c-c31e-4611-8e4f-7eb6d360ed80"},"outputs":[],"source":["correct, total = 0,  0\n","predictions = []\n","\n","# Set the Neural Network into evaluation (test) mode\n","net.eval()\n","\n","step=0\n","\n","y_val_cancerous = []\n","y_val_pred_cancerous = []\n","\n","# Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n","for i, data in enumerate(cancerous_val_dataloader, 0):\n","    inputs, labels = data\n","\n","    outputs = net(inputs)\n","    class_score, predicted = torch.max(outputs.data, 1)\n","    \n","    # Loop through the batch, build the lists of the raw label and prediction values\n","    for j in range(len(labels)):\n","        y_val_cancerous.append(labels[j].item())\n","        y_val_pred_cancerous.append(predicted[j].item())\n","\n","    predictions.append(predicted)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","\n","\n","print(\"Evaluate the Validation Predictions and Error: \\n\")\n","\n","print('Confusion matrix: \\n')\n","print(confusion_matrix(y_val_cancerous, y_val_pred_cancerous))\n","\n","print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_val_cancerous, y_val_pred_cancerous)))\n","print(\"- Precision Score: \" + str(precision_score(y_val_cancerous, y_val_pred_cancerous)))\n","print(\"- Recall Score: \" + str(recall_score(y_val_cancerous, y_val_pred_cancerous)))\n","print(\"- F1 Score: \" + str(f1_score(y_val_cancerous, y_val_pred_cancerous)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":507},"executionInfo":{"elapsed":804,"status":"ok","timestamp":1683436116264,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"6f4hBdFGRNpI","outputId":"85c27584-7d67-4149-8bca-c2e23c6f8f59"},"outputs":[],"source":["a2util.getClassificationROC(\"IsCancerous\", \"Validation\", y_val_cancerous, y_val_pred_cancerous, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1683436116265,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"zTfShcFRK_pp","outputId":"4d50d8b6-77eb-4970-f4bf-809108fac75a"},"outputs":[],"source":["for i in range(3):\n","    print(predictions[i])\n","\n","print(len(predictions))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1683436116265,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"rF-DYPECK_pp","outputId":"1a1d9b20-6d1d-4ac4-aac0-bc2ee4fc1c60"},"outputs":[],"source":["# y_val_pred_cancerous = [item for sublist in y_val_pred_cancerous for item in sublist]\n","print(\"Labels\")\n","for i in range(5):\n","    print(y_val_cancerous[i])\n","\n","print(\"Predictions\")\n","for i in range(5):\n","    print(y_val_pred_cancerous[i])"]},{"cell_type":"markdown","metadata":{"id":"DHozRzzdK_pq"},"source":["Now also train a model for CellType Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1683436116265,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"AVvBdxxjK_pq","outputId":"8c63d8f9-8fbe-40f3-c897-bc3b461766d7"},"outputs":[],"source":["celltype_training_data = None\n","\n","# Create a custom Dataset for the training and validation data\n","if useFullData:\n","    celltype_training_data = CancerCellTypeDataset(isGoogleColab, dfImagesTrain, baseDirectory, transform=transform_normalize)\n","else:\n","    # For testing in a small dataset\n","    dfImagesTrainTest = dfImagesTrain.iloc[range(500), :].reset_index()\n","    celltype_training_data = CancerCellTypeDataset(isGoogleColab, dfImagesTrainTest, baseDirectory, transform=transform_normalize, target_transform=None)\n","\n","\n","celltype_validation_data = CancerCellTypeDataset(isGoogleColab, dfImagesVal, baseDirectory, transform=transform_normalize)\n","\n","# Create data loaders\n","celltype_train_dataloader = DataLoader(celltype_training_data, batch_size=32, shuffle=True, num_workers=2)\n","celltype_val_dataloader = DataLoader(celltype_validation_data, batch_size=32, shuffle=True, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"fd9XOjhpK_pq"},"source":["Create a class for the Cell Type Neural Network model. The structure of the class will be fundamentally the same, only the model will need to output 4 classes"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683436116266,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"eKOxU2-4K_pq"},"outputs":[],"source":["# Create a class for the Neural Network\n","class PT_CNN_AlexNet_CellType(nn.Module):\n","\n","    # In the constructor, initialize the layers to use\n","    def __init__(self):\n","        super(PT_CNN_AlexNet_CellType, self).__init__()\n","\n","        # first, define the subsampling methods. Though they are used multiple times, these are the\n","        # operations, so only need to be defined once\n","        # Note: added a second max pooling operation with padding 1 to fix some output sizing issue\n","        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n","        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n","\n","        # define the ReLU Activation method to use\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        # define the convolution layers\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=11, stride=4, padding=2)\n","        self.conv2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=5, padding=2)\n","        self.conv3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, stride=1, padding=1)\n","        self.conv4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1)\n","        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n","        \n","        # define the fully connected neural layers\n","        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n","        self.fc2 = nn.Linear(4096, 4096)\n","        self.fc3 = nn.Linear(4096, 4)\n","\n","    # Create the forward function, which is used in training\n","    def forward(self, x):\n","\n","        # print(\"Init Shape: \" + str(x.shape))\n","\n","        # Process the first 2 convolution layers, applying maxpooling\n","        x = self.relu(self.conv1(x))\n","        x = self.maxpool(x)\n","        x = self.relu(self.conv2(x))\n","        x = self.maxpool(x)\n","        \n","        # Then process the remaining convolution layers without any pooling\n","        x = self.relu(self.conv3(x))\n","        x = self.relu(self.conv4(x))\n","        x = self.relu(self.conv5(x))\n","\n","        # Then apply a max pool and average pool on the result\n","        x = self.maxpool2(x)\n","        x = self.avgpool(x)\n","\n","        # This should convert to tensors that are acceptable for the input into the NN 3 layers\n","        x = x.view(x.size(0), 256 * 6 * 6)\n","\n","        # Now process the 3 layers of the Fully Connected NN\n","        x = self.relu(self.fc1(x))\n","        x = self.relu(self.fc2(x))\n","        x = self.relu(self.fc3(x))        \n","\n","        # return the result\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"KRU0R9EMK_pq"},"source":["Now train the Fully Connected Neural Network Model. Use the same configuration (objective function, optimizer etc) as the Binary Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":179478,"status":"ok","timestamp":1683436295735,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"d3FjNKScK_pq","outputId":"82fb3b2a-903b-44b8-9791-1f77baa0d3b0"},"outputs":[],"source":["\n","\n","# set the Learning Rate to use\n","learning_rate = 0.0001\n","epochsToUse = 10\n","\n","net = PT_CNN_AlexNet_CellType()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n","\n","for epoch in range(epochsToUse):\n","    print(\"Starting Epoch \" + str(epoch) + \"...\")\n","    for i, data in enumerate(celltype_train_dataloader, 0):\n","        # Get the inputs\n","        inputs, labels = data\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Perform Forward and Backward propagation then optimize the weights\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"2d8yW4-2RNpJ"},"source":["Predict on the Training Set to get the Training Accuracy and Error"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12959,"status":"ok","timestamp":1683436308682,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"YzW_UTsrRNpK","outputId":"7038131f-c3a9-417b-9883-29a804a9ebeb"},"outputs":[],"source":["\n","\n","correct, total = 0,  0\n","predictions = []\n","\n","# Set the Neural Network into evaluation (test) mode\n","net.eval()\n","\n","y_train_celltype = []\n","y_train_pred_celltype = []\n","y_train_pred_celltype_scores = []\n","\n","showBatch=True\n","\n","# Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n","for i, data in enumerate(celltype_train_dataloader, 0):\n","    inputs, labels = data\n","\n","    outputs = net(inputs)\n","\n","    # outputs.data contains a tensor of size 4 for each record, with a score for each class. Use max\n","    # to select the class of the highest score for the prediction.\n","    class_score, predicted = torch.max(outputs.data, 1)\n","    \n","    # Loop through the batch, build the lists of the raw label and prediction values\n","    for j in range(len(labels)):\n","        y_train_celltype.append(labels[j].item())\n","        y_train_pred_celltype.append(predicted[j].item())\n","        y_train_pred_celltype_scores.append(outputs.data[j].tolist())\n","\n","    showBatch = False\n","    \n","    predictions.append(predicted)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","\n","print(\"Evaluate the Training Predictions and Error: \\n\")\n","\n","print('Confusion matrix: \\n')\n","print(confusion_matrix(y_train_celltype, y_train_pred_celltype))\n","\n","print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_train_celltype, y_train_pred_celltype)))\n","print(\"- Precision Score: \" + str(precision_score(y_train_celltype, y_train_pred_celltype, average=\"micro\")))\n","print(\"- Recall Score: \" + str(recall_score(y_train_celltype, y_train_pred_celltype, average=\"micro\")))\n","print(\"- F1 Score: \" + str(f1_score(y_train_celltype, y_train_pred_celltype, average=\"micro\")))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1683436308682,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"bFRW0l6CRNpK","outputId":"10273db6-1314-4a43-cb76-36265e2a7ff9"},"outputs":[],"source":["for i in range(5):\n","    print(y_train_pred_celltype_scores[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1039,"status":"ok","timestamp":1683436309708,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"nV00GdMmRNpK","outputId":"cc73f51f-374c-4fe5-bb9e-c5e8bb8873b9"},"outputs":[],"source":["a2util.getClassificationROC(\"CellType\", \"Training\", y_train_celltype, y_train_pred_celltype, 4, y_train_pred_celltype_scores)"]},{"cell_type":"markdown","metadata":{"id":"NpLNiit9K_pr"},"source":["Predict on the Validation data and evaluate the results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1563,"status":"ok","timestamp":1683436311268,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"VfU8z2ljK_pr","outputId":"c82e156e-7f5c-4897-8222-633964f595fb"},"outputs":[],"source":["\n","\n","correct, total = 0,  0\n","predictions = []\n","\n","# Set the Neural Network into evaluation (test) mode\n","net.eval()\n","\n","step=0\n","\n","y_val_celltype = []\n","y_val_pred_celltype = []\n","y_val_pred_celltype_scores = []\n","\n","# Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n","for i, data in enumerate(celltype_val_dataloader, 0):\n","    inputs, labels = data\n","\n","    outputs = net(inputs)\n","    class_score, predicted = torch.max(outputs.data, 1)\n","    \n","    # Loop through the batch, build the lists of the raw label and prediction values\n","    for j in range(len(labels)):\n","        y_val_celltype.append(labels[j].item())\n","        y_val_pred_celltype.append(predicted[j].item())\n","        y_val_pred_celltype_scores.append(outputs.data[j].tolist())\n","\n","    predictions.append(predicted)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","\n","accuracy = (correct/total) * 100\n","print(\"Evaluate the Validation Predictions and Error: \\n\")\n","\n","print('Confusion matrix: \\n')\n","print(confusion_matrix(y_val_celltype, y_val_pred_celltype))\n","\n","print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_val_celltype, y_val_pred_celltype)))\n","print(\"- Precision Score: \" + str(precision_score(y_val_celltype, y_val_pred_celltype, average=\"micro\")))\n","print(\"- Recall Score: \" + str(recall_score(y_val_celltype, y_val_pred_celltype, average=\"micro\")))\n","print(\"- F1 Score: \" + str(f1_score(y_val_celltype, y_val_pred_celltype, average=\"micro\")))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1602,"status":"ok","timestamp":1683436312865,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"L3PMC4flRNpK","outputId":"1a8729ce-8b29-4d65-b72d-5f3f7fc3d89b"},"outputs":[],"source":["a2util.getClassificationROC(\"CellType\", \"Validation\", y_val_celltype, y_val_pred_celltype, 4, y_val_pred_celltype_scores)"]},{"cell_type":"markdown","metadata":{"id":"imXr4wLKRNpL"},"source":["<h1>Analysis of Performance and Accuracy</h1>\n","\n","<strong>Binary Classification - IsCancerous</strong>\n","<p>\n","In this model we see that the model has a very low training error and a high accuracy value, in this experiment achieving a 97.3% accuracy. This indicates that the model has <strong>low bias</strong>. However, when predicting on the validation dataset, the validation error rises. It can be seen that the Accuracy for the validation predictions is 84.4%, and the are under the ROC Curve is significantly less, with a value of 0.824. This is an indication of <strong>high variance</strong>, and in combination with low bias, indicates a possible problem of <strong>overfitting</strong>\n","</p>\n","\n","<strong>Multi-class Classification - Cell Type</strong>\n","<p>\n","Similarly, the Cell Type model has a very low training error and a high accuracy value of 99.5%, indicating that the model has <strong>low bias</strong>. However, the Cell Type model has a considerably worse accuracy of only 76.5% when predicting on the validation dataset. A ROC curve is generated for each class, and it can be seen that the model performs better at predicting some classes compared to others. The average the under the ROC Curve score, with a value of 0.838. This indication of <strong>high variance</strong>, and in combination with low bias, indicates a possible problem of <strong>overfitting</strong>.\n","</p>"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"dcbc78149e46ccbab92a3f68a48c52feb0796c7e10dad8e3f1a2a5a780973376"}}},"nbformat":4,"nbformat_minor":0}
