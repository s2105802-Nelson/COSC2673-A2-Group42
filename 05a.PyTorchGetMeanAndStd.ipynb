{"cells":[{"cell_type":"markdown","metadata":{"id":"QDRW51eYK_pg"},"source":["<hr style=\"color:green\" />\n","<h1 style=\"color:green\">COSC2673 Assignment 2: Image Classification for Cancerous Cells</h1>\n","<h2 style=\"color:green\">File 05a: PyTorch - Get Mean and Standard Deviation</h2>\n","<hr style=\"color:green\" />\n","\n","<p>\n","In a number of future experiments with PyTorch, The Tensor Data Loaders will make use of a Normalize operation on the image data, which requires the Means and Standard Deviations of the R, G and B numbers\n","</p>\n","<p>\n","Rather than loading all the training images then computing the mean and std in every experiment, run this file to get the mean and standard deviation values, which can then be hardcoded into other NN training experiments.\n","<p>\n","<p> \n","Note, any time the images are reloaded and re-split into training, validation and test splits, this will need to be rerun to recalculate the values for the new training split\n","</p>"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":539,"status":"ok","timestamp":1683003685462,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"LuaHh7dfK_pj"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\nelso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n","c:\\Users\\nelso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n","c:\\Users\\nelso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n","  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import cv2\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torch.utils.data\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision.io import read_image\n"]},{"cell_type":"markdown","metadata":{"id":"4ayZvnueK_pk"},"source":["Configure this script as to whether it runs on Google Colab, or locally"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1683003686280,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"vFtUm6uXK_pk"},"outputs":[],"source":["# When on Google Colab, running full training, change both to true. Locally, advised set both to false\n","isGoogleColab = False\n","useFullData = False"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2800,"status":"ok","timestamp":1683003689076,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"zzl3NpfVK_pk","outputId":"fb65a0b0-1895-411a-ff76-8cc7086526e5"},"outputs":[],"source":["# In local, the base directory is the current directory\n","baseDirectory = \"./\"\n","\n","if isGoogleColab:\n","    from google.colab import drive\n","    \n","    # If this is running on Google colab, assume the notebook runs in a \"COSC2673\" folder, which also contains the data files \n","    # in a subfolder called \"image_classification_data\"\n","    drive.mount(\"/content/drive\")\n","    !ls /content/drive/'My Drive'/COSC2673/\n","\n","    # Import the directory so that custom python libraries can be imported\n","    import sys\n","    sys.path.append(\"/content/drive/MyDrive/COSC2673/\")\n","\n","    # Set the base directory to the Google Drive specific folder\n","    baseDirectory = \"/content/drive/MyDrive/COSC2673/\""]},{"cell_type":"markdown","metadata":{"id":"vZCfUn3EK_pl"},"source":["Import the custom python files that contain reusable code"]},{"cell_type":"code","execution_count":83,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683003689077,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"beWSdbauK_pl","outputId":"855961c1-7da8-4450-c7b3-3f9e66292cc3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Random Seed: 266305\n"]}],"source":["import data_basic_utility as dbutil\n","import graphing_utility as graphutil\n","import statistics_utility as statsutil\n","\n","import a2_utility as a2util\n","import pytorch_utility as ptutil\n","from pytorch_utility import CancerBinaryDataset\n","from pytorch_utility import CancerCellTypeDataset\n","\n","\n","# randomSeed = dbutil.get_random_seed()\n","randomSeed = 266305\n","print(\"Random Seed: \" + str(randomSeed))"]},{"cell_type":"code","execution_count":84,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1683003689078,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"zgIXX9BXK_pl"},"outputs":[],"source":["# this file should have previously been created in the root directory\n","dfImages = pd.read_csv(baseDirectory + \"images_main.csv\")"]},{"cell_type":"code","execution_count":85,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1683003689078,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"6_WFGagHK_pl","outputId":"f1ca6c45-d622-4dd2-91cb-adbd202a8b26"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-656eccd9-c7ad-460c-a5ae-a3ac5873de6f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>ImageName</th>\n","      <th>isCancerous</th>\n","      <th>cellType</th>\n","      <th>trainValTest</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>./Image_classification_data/patch_images\\1.png</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>./Image_classification_data/patch_images\\10.png</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>./Image_classification_data/patch_images\\1000.png</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>./Image_classification_data/patch_images\\10000...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>./Image_classification_data/patch_images\\10001...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-656eccd9-c7ad-460c-a5ae-a3ac5873de6f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-656eccd9-c7ad-460c-a5ae-a3ac5873de6f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-656eccd9-c7ad-460c-a5ae-a3ac5873de6f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   index                                          ImageName  isCancerous  \\\n","0      0     ./Image_classification_data/patch_images\\1.png            0   \n","1      1    ./Image_classification_data/patch_images\\10.png            0   \n","2      3  ./Image_classification_data/patch_images\\1000.png            1   \n","3      4  ./Image_classification_data/patch_images\\10000...            0   \n","4      5  ./Image_classification_data/patch_images\\10001...            0   \n","\n","   cellType  trainValTest  \n","0         0             0  \n","1         0             0  \n","2         2             0  \n","3         1             0  \n","4         1             0  "]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["# Get The training Split and the Validation Split\n","dfImagesTrain = dfImages[dfImages[\"trainValTest\"] == 0].reset_index()\n","dfImagesVal = dfImages[dfImages[\"trainValTest\"] == 1].reset_index()\n","\n","dfImagesTrain.head()"]},{"cell_type":"code","execution_count":86,"metadata":{"executionInfo":{"elapsed":16204,"status":"ok","timestamp":1683003705276,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"QdDdY_sRK_pm"},"outputs":[],"source":["# Load the images into a numpy array. This is so we can find the mean and std of the RBG values in order to Normalize the images for the Neural Network\n","imgsTrain = []\n","for imageName in dfImagesTrain[\"ImageName\"]:\n","    # In colab, we want to remove the leading \"./\" from the image name\n","    if isGoogleColab:\n","        if imageName.startswith(\"./\"):\n","            imageName = imageName[2:]\n","\n","    imageName = imageName.replace(\"\\\\\", \"/\")\n","    img = cv2.imread(baseDirectory + imageName)\n","    imgsTrain.append(img)\n","\n","    "]},{"cell_type":"code","execution_count":87,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1683003705277,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"7SvfjberK_pm","outputId":"d8bb2246-e6c8-44cf-9de1-ee257dfae89e"},"outputs":[{"name":"stdout","output_type":"stream","text":["7837\n","[[[213 187 220]\n","  [214 189 220]\n","  [240 215 246]\n","  ...\n","  [226 197 231]\n","  [207 175 210]\n","  [211 183 215]]\n","\n"," [[209 182 216]\n","  [237 210 242]\n","  [254 228 255]\n","  ...\n","  [217 182 223]\n","  [216 178 220]\n","  [231 198 237]]\n","\n"," [[226 197 232]\n","  [252 224 255]\n","  [239 211 245]\n","  ...\n","  [223 190 228]\n","  [222 187 227]\n","  [221 188 226]]\n","\n"," ...\n","\n"," [[203 151 212]\n","  [193 142 204]\n","  [206 154 217]\n","  ...\n","  [224 187 242]\n","  [230 191 247]\n","  [232 190 247]]\n","\n"," [[217 162 222]\n","  [213 161 223]\n","  [205 153 216]\n","  ...\n","  [221 176 233]\n","  [219 174 233]\n","  [224 181 240]]\n","\n"," [[219 161 217]\n","  [210 153 214]\n","  [223 167 230]\n","  ...\n","  [215 169 228]\n","  [213 169 228]\n","  [220 177 235]]]\n"]}],"source":["print(len(imgsTrain))\n","print(imgsTrain[0])"]},{"cell_type":"code","execution_count":88,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":510,"status":"ok","timestamp":1683003705776,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"SakYttHlK_pn","outputId":"0bac7ae7-491c-4eb3-f8ee-8656394023d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean: tensor([0.8035, 0.5909, 0.7640])\n","Standard deviation: tensor([0.1246, 0.1947, 0.1714])\n"]}],"source":["# Define a transform to convert images to PyTorch tensors\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","# Convert images to tensors and concatenate them into a single tensor\n","tensor_images = torch.cat([transform(img).unsqueeze(0) for img in imgsTrain])\n","\n","# Calculate the mean and standard deviation of the tensor values\n","tensor_train_mean = tensor_images.mean(dim=(0, 2, 3))\n","tensor_train_std = tensor_images.std(dim=(0, 2, 3))\n","\n","print(\"Mean:\", tensor_train_mean)\n","print(\"Standard deviation:\", tensor_train_std)\n","\n","print(\"\\n\\n\")\n","print(\"Lines of code to copy over:\")\n","print(\"train_mean = torch.\" + tensor_train_mean)\n","print(\"train_std = torch.\" + tensor_train_std)"]},{"cell_type":"markdown","metadata":{"id":"TZ1R94YdK_pn"},"source":["Take these lines of code and update them in pytorch_utility.py, which other experiment files will use in the Normalize Transform for Data Loaders. That way, if they change, it just needs to be updated in one location."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.9.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"dcbc78149e46ccbab92a3f68a48c52feb0796c7e10dad8e3f1a2a5a780973376"}}},"nbformat":4,"nbformat_minor":0}
