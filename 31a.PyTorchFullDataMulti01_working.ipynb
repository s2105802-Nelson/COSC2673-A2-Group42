{"cells":[{"cell_type":"markdown","metadata":{"id":"QDRW51eYK_pg"},"source":["<hr style=\"color:green\" />\n","<h1 style=\"color:green\">COSC2673 Assignment 2: Image Classification for Cancerous Cells</h1>\n","<h2 style=\"color:green\">File 31: Modelling on the Full data For the Cell Type Multiclass Classification</h2>\n","<hr style=\"color:green\" />\n","\n","<p>\n","After Reviewing files 22, 24, 25 and 26, we have experimented with improving the performance of the CNN with more complexity in the Convolutions and the Classifier layers. Taking into account performance increases but also processing time and complexity, the following model will be used here:\n","</p>\n","<ul>\n","<li>CNN with more complex convolutions (from file 24)</li>\n","</ul>\n","<p>\n","Now, in an attempt to improve the performance of both the Binary and the Multiclass models, this process will make use of the full data, ie. both the Main Labels file and the Extras Label file. Because handling these two will be different, processing will be split into separate notebooks.\n","</p>\n","\n","<ul>\n","<li>Binary IsCancerous: All data is labeled, so just add the new data to the dataset</li>\n","<li>Multiclass Cell Type (here): The data in the Extras file are unlabeled, so therefore, we will need to apply a Semi Supervised Learning approach</li>\n","</ul>\n","<p>\n","In this file, also load the Extra Label data as well as the main data. Then we will perform a Semi Supervised learning process. The basic premise of Semi Supervised learning will be  as follows\n","</p>\n","\n","<ol>\n","<li>Train a baseline model using the labelled Main Data</li>\n","<li>Predict using this model on the unlabelled Extra Data, taking care to also retain the Softmax score for each value, which is a probability of being that label</li>\n","<li>Review these predictions, find any that have a high confidence (generally 80% probability is used)</li>\n","<li>Take this data, and apply that predicted label, called a \"pseudo-label\". Incorporate this pseudo-labeled data into the training data (also remove them from the extra data)</li>\n","<li>Train an updated model using the updated training data</li>\n","<li>Iterate the process from Step 2, until no high confidence data is generated, or capped at a number of rounds</li>\n","</ol>"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":8285,"status":"ok","timestamp":1683527199852,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"LuaHh7dfK_pj"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\nelso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n","c:\\Users\\nelso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n","c:\\Users\\nelso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n","  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import cv2\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torch.utils.data\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision.io import read_image\n"]},{"cell_type":"markdown","metadata":{"id":"4ayZvnueK_pk"},"source":["Configure this script as to whether it runs on Google Colab, or locally"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1683527199853,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"vFtUm6uXK_pk"},"outputs":[],"source":["# When on Google Colab, running full training, change both to true. Locally, advised set both to false\n","isGoogleColab = False\n","useFullData = False"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31235,"status":"ok","timestamp":1683527231083,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"zzl3NpfVK_pk","outputId":"164b6588-4bfc-4f20-ba1f-26e3563def4e"},"outputs":[],"source":["# In local, the base directory is the current directory\n","baseDirectory = \"./\"\n","\n","if isGoogleColab:\n","    from google.colab import drive\n","    \n","    # If this is running on Google colab, assume the notebook runs in a \"COSC2673\" folder, which also contains the data files \n","    # in a subfolder called \"image_classification_data\"\n","    drive.mount(\"/content/drive\")\n","    !ls /content/drive/'My Drive'/COSC2673/\n","\n","    # Import the directory so that custom python libraries can be imported\n","    import sys\n","    sys.path.append(\"/content/drive/MyDrive/COSC2673/\")\n","\n","    # Set the base directory to the Google Drive specific folder\n","    baseDirectory = \"/content/drive/MyDrive/COSC2673/\""]},{"cell_type":"markdown","metadata":{"id":"vZCfUn3EK_pl"},"source":["Import the custom python files that contain reusable code"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6079,"status":"ok","timestamp":1683527237159,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"beWSdbauK_pl","outputId":"6c7403c5-4961-4c43-eb4e-a3be29d7dc1d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Random Seed: 266305\n"]}],"source":["import data_basic_utility as dbutil\n","import graphing_utility as graphutil\n","import statistics_utility as statsutil\n","\n","import a2_utility as a2util\n","import pytorch_utility as ptutil\n","from pytorch_utility import CancerBinaryDataset\n","from pytorch_utility import CancerCellTypeDataset\n","\n","\n","# randomSeed = dbutil.get_random_seed()\n","randomSeed = 266305\n","print(\"Random Seed: \" + str(randomSeed))"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":767,"status":"ok","timestamp":1683527237916,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"zgIXX9BXK_pl"},"outputs":[],"source":["# this file should have previously been created in the root directory\n","dfImages = pd.read_csv(baseDirectory + \"images_main.csv\")\n","dfImagesExtra = pd.read_csv(baseDirectory + \"images_extra.csv\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1683527237917,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"6_WFGagHK_pl","outputId":"b3660e2b-a19a-49c7-b513-eee0feb2ac3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["(7837, 5)\n","(1031, 5)\n","(1028, 5)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>ImageName</th>\n","      <th>isCancerous</th>\n","      <th>cellType</th>\n","      <th>trainValTest</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>./Image_classification_data/patch_images\\1.png</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>./Image_classification_data/patch_images\\10.png</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>./Image_classification_data/patch_images\\1000.png</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>./Image_classification_data/patch_images\\10000...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>./Image_classification_data/patch_images\\10001...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   index                                          ImageName  isCancerous  \\\n","0      0     ./Image_classification_data/patch_images\\1.png            0   \n","1      1    ./Image_classification_data/patch_images\\10.png            0   \n","2      3  ./Image_classification_data/patch_images\\1000.png            1   \n","3      4  ./Image_classification_data/patch_images\\10000...            0   \n","4      5  ./Image_classification_data/patch_images\\10001...            0   \n","\n","   cellType  trainValTest  \n","0         0             0  \n","1         0             0  \n","2         2             0  \n","3         1             0  \n","4         1             0  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Get The training Split and the Validation Split\n","dfImagesTrain = dfImages[dfImages[\"trainValTest\"] == 0].reset_index()\n","dfImagesVal = dfImages[dfImages[\"trainValTest\"] == 1].reset_index()\n","dfImagesTest = dfImages[dfImages[\"trainValTest\"] == 2].reset_index()\n","\n","print(dfImagesTrain.shape)\n","print(dfImagesVal.shape)\n","print(dfImagesTest.shape)\n","\n","dfImagesTrain.head()"]},{"cell_type":"markdown","metadata":{},"source":["In the Semi Supervised Learning, there is no point in Splitting the Extra data into the flagged Train/Validation/Test Split, as it's unlabelled by Cell Type. Therefore, all extra data will be used as part of the SS Training process"]},{"cell_type":"markdown","metadata":{"id":"TZ1R94YdK_pn"},"source":["Note: The definition of the Custom Datasets for both the isCancerous data and the Cell Type data are defined in the pytorch_utility.py file.\n","\n","Also, rather than loading all the training images and calculating the mean and standard deviation values in here, that was run separately in file 05a.PyTorchGetMeanAndStd.ipynb\n","\n","Here we can just define the values to use, which shouldn't change unless the data is reloaded and a new train/validation/test split is generated"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1683527237917,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"qaY5AA-tsWWw","outputId":"53ce6f3a-aeec-4d4d-a697-e083f0b01746"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0.8035, 0.5909, 0.7640])\n","tensor([0.1246, 0.1947, 0.1714])\n"]}],"source":["train_mean, train_std = ptutil.getTrainMeanAndStdTensors()\n","print(train_mean)\n","print(train_std)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683527237918,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"I8z3yX8ZK_pn"},"outputs":[],"source":["# Create a tranform operation that also normalizes the images according to the mean and standard deviations of the images\n","transform_normalize = transforms.Compose(\n","    [transforms.ToPILImage(),\n","    transforms.ToTensor(), \n","    transforms.Normalize(train_mean, train_std)])\n"]},{"cell_type":"markdown","metadata":{"id":"F4jD7E2PK_po"},"source":["# Semi Supervised Learning\n","\n","Initially, only create data loaders for the validation data and the test data, as these sets will not change. "]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683530208456,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"AVvBdxxjK_pq","outputId":"f39b609a-d26c-41e4-e295-3ee0e50d2573"},"outputs":[],"source":["celltype_validation_data = CancerCellTypeDataset(isGoogleColab, dfImagesVal, baseDirectory, transform=transform_normalize)\n","celltype_test_data = CancerCellTypeDataset(isGoogleColab, dfImagesTest, baseDirectory, transform=transform_normalize)\n","\n","# Create data loaders\n","celltype_val_dataloader = DataLoader(celltype_validation_data, batch_size=32, shuffle=True, num_workers=2)\n","celltype_test_dataloader = DataLoader(celltype_test_data, batch_size=32, shuffle=True, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"fd9XOjhpK_pq"},"source":["Define the classe for the CNN. At the moment, we are using the the standard CNN structure with Early stopping (File 22)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1683530208456,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"eKOxU2-4K_pq"},"outputs":[],"source":["# Create a class for the Neural Network\n","class PT_CNN_CellType(nn.Module):\n","\n","    # In the constructor, initialize the layers to use\n","    def __init__(self):\n","        super(PT_CNN_CellType, self).__init__()\n","\n","\n","        # first, define the subsampling methods. Though they are used multiple times, these are the\n","        # operations, so only need to be defined once\n","        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n","\n","        # define the Activation methods to use\n","        self.relu = nn.ReLU(inplace=True)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","        # define the convolution layers\n","\n","        # input should be 27x27x3. Apply a 3x3 filter, therefore, output should be 25x25x32 (channels aka feature maps)\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1)\n","        # There will be a Relu\n","        # Then a MaxPool of 2x2, halving the dimensions per feature map\n","        # So input is 12x12x32. Apply a 3x3 filter, also include padding=1, as this is already quite small, and lets consider the edges\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)    \n","        # There will be a Relu        \n","        # So input is 12x12x64. Apply a 3x3 filter, also include padding=1, as this is already quite small, and lets consider the edges\n","        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n","        # Then a MaxPool of 2x2, halving the dimensions per feature map\n","        # Then an Average Pool of keeping the dimensions as 6x6\n","        \n","        # define the fully connected neural layers\n","        self.fc1 = nn.Linear(128 * 6 * 6, 4608)\n","        self.fc2 = nn.Linear(4608, 2306)\n","        self.fc3 = nn.Linear(2306, 4)\n","\n","    # Create the forward function, which is used in training\n","    def forward(self, x):\n","\n","        # print(\"Init Shape: \" + str(x.shape))\n","\n","        # Process the first 2 convolution layers, applying maxpooling\n","        x = self.relu(self.conv1(x))\n","        x = self.maxpool(x)\n","\n","        # Then process the remaining convolution layers without any pooling\n","        x = self.relu(self.conv2(x))        \n","        x = self.relu(self.conv3(x))\n","\n","        # Then apply a max pool and average pool on the result\n","        x = self.maxpool(x)\n","        #x = self.avgpool(x)\n","\n","        # Flatten: This should convert to tensors that are acceptable for the input into the NN 3 layers\n","        x = x.view(x.size(0), 128 * 6 * 6)\n","\n","        # Now process the 3 layers of the Fully Connected NN\n","        x = self.relu(self.fc1(x))  \n","        x = self.relu(self.fc2(x))              \n","        # x = self.fc3(x)\n","        # x = self.relu(self.fc3(x))\n","        x = F.softmax(self.fc3(x), dim=1)        \n","\n","        # return the result\n","        return x\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Create a function to Predict using a particular mode and return results"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1683530208457,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"9sPKkwUkCKbm"},"outputs":[],"source":["def predictCellTypeOnDataSetAccuracy(net, setName, dataloader, printResult=True, printFullResults=False):\n","    correct, total = 0,  0\n","    predictions = []\n","\n","    y_celltype = []\n","    y_pred_celltype = []\n","    y_pred_celltype_scores = []\n","\n","    # Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n","    for i, data in enumerate(dataloader, 0):\n","        inputs, labels = data\n","\n","        outputs = net(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        \n","        # Loop through the batch, build the lists of the raw label and prediction values\n","        for j in range(len(labels)):\n","            y_celltype.append(labels[j].item())\n","            y_pred_celltype.append(predicted[j].item())\n","            y_pred_celltype_scores.append(outputs.data[j].tolist())\n","\n","        predictions.append(predicted)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    accuracy = accuracy_score(y_celltype, y_pred_celltype)\n","    f1Score = f1_score(y_celltype, y_pred_celltype, average=\"micro\")\n","\n","    if printFullResults:\n","        print(setName + \":\")        \n","        print('Confusion matrix: \\n')\n","        print(confusion_matrix(y_celltype, y_pred_celltype))\n","        print(\"\\n- Accuracy Score: \" + str(accuracy))\n","        print(\"- Precision Score: \" + str(precision_score(y_celltype, y_pred_celltype, average=\"micro\")))\n","        print(\"- Recall Score: \" + str(recall_score(y_celltype, y_pred_celltype, average=\"micro\")))\n","        print(\"- F1 Score: \" + str(f1Score))\n","    elif printResult:\n","        print(\"- \" + setName + \" F1: \" + str(f1Score))\n","\n","    return accuracy, y_celltype, y_pred_celltype, y_pred_celltype_scores"]},{"cell_type":"markdown","metadata":{},"source":["Set some standard learning variables that will be used across all iterations"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# set the Learning Rate to use\n","learning_rate = 0.0001\n","maxEpochs = 15\n","patience = 2\n","disableEarlyStopping = False\n","\n","if useFullData == False:\n","    maxEpochs = 5\n","    patience = 1"]},{"cell_type":"markdown","metadata":{},"source":["Create a Function, that given a PyTorch dataload with a set of data to train in, train a CNN model"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":153417,"status":"ok","timestamp":1683530361867,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"d3FjNKScK_pq","outputId":"8843af12-24dd-4b58-a714-d447c1d66f9b"},"outputs":[],"source":["def train_celltype_model(iteration, celltype_train_dataloader):\n","    # Define CNN with the loss and the optimizer\n","    net = PT_CNN_CellType()\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n","\n","    bestErrorDiff = 99999\n","    concurrentNonImproves = 0\n","    currentEpoch = 0\n","\n","    bestValAcc = -1\n","    lstEpochs = []\n","    lstTrainAccs = []\n","    lstValAccs = []\n","    for epoch in range(maxEpochs):\n","        print(\"Starting Epoch \" + str(epoch) + \"...\")\n","        currentEpoch = epoch\n","\n","        # Set the Neural Network into training mode\n","        net.train()\n","\n","        # Train through this epoch\n","        for i, data in enumerate(celltype_train_dataloader, 0):\n","            # Get the inputs\n","            inputs, labels = data\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # Perform Forward and Backward propagation then optimize the weights\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","        \n","        # Set the Neural Network into evaluation (test) mode, so we can evaluate both training and validation error\n","        net.eval()        \n","        trainingAccuracy, y_train_celltype, y_train_pred_celltype, y_train_pred_celltype_scores = predictCellTypeOnDataSetAccuracy(\n","            net, \"Training\", celltype_train_dataloader, True, False)\n","        validationAccuracy, y_val_celltype, y_val_pred_celltype, y_val_pred_celltype_scores  = predictCellTypeOnDataSetAccuracy(\n","            net, \"Validation\", celltype_val_dataloader, True, False)\n","\n","        errorDiff = trainingAccuracy - validationAccuracy\n","        print(\"- Accuracy Difference: \" + str(errorDiff))\n","\n","        lstEpochs.append(epoch)\n","        lstTrainAccs.append(trainingAccuracy)\n","        lstValAccs.append(validationAccuracy)\n","\n","        if epoch > 0 and (validationAccuracy - bestValAcc > 0.01):        \n","            # There is at least percentage point improvement in the validation F1, count this as a \n","            # good iteration, regardless of the error difference\n","            print(\"- IsGoodStep\")\n","            concurrentNonImproves = 0\n","            if errorDiff > 0 and errorDiff < bestErrorDiff:  \n","                bestErrorDiff = errorDiff        \n","        elif errorDiff < bestErrorDiff:        \n","            # This epoch is an improvement on the last, so we will continue. the concurrent non improve counts reset for the patience\n","            print(\"- IsBetter: \" + str(errorDiff) + \" : \" + str(bestErrorDiff))        \n","            concurrentNonImproves = 0\n","            if errorDiff > 0:\n","                bestErrorDiff = errorDiff\n","        else:\n","            # This epoch has the same or worse performance than the last. Check if we have reached the patience, if so, then stop early\n","            print(\"- IsNotBetter: \" + str(errorDiff) + \" : \" + str(bestErrorDiff))        \n","            concurrentNonImproves += 1\n","            if disableEarlyStopping == False:\n","                if concurrentNonImproves >= patience:\n","                    print(\"Early Stopping occurred at Epoch \" + str(epoch))\n","                    break\n","\n","        # update the val F1 score from the previous epoch if it's the best\n","        if validationAccuracy > bestValAcc:\n","            bestValAcc = validationAccuracy\n","\n","    # Create a dataframe that can be used to plot the Training/Validation Loss plot, if we want to use it later\n","    dfLoss = pd.DataFrame({ 'epoch': lstEpochs, 'train': lstTrainAccs, 'validation': lstValAccs })\n","\n","    # Once this is done, return the new CNN model and the loss dataframe\n","    return net, dfLoss\n"]},{"cell_type":"markdown","metadata":{},"source":["Now, load the initial training data into the celltype_training_data then configure it as a PyTorch Dataloader. This will be for our baseline model\n","\n","Also, initialize a dataloader for the extra data"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["celltype_training_data = None\n","\n","# Create copies of the Train and Extra images dataset, these will be continually updated through SS\n","dfImagesTrainSS = dfImagesTrain\n","dfExtraSS = dfImagesExtra\n","\n","# Create a an initial dataloader for both train and extra images\n","if useFullData:\n","    celltype_training_data = CancerCellTypeDataset(isGoogleColab, dfImagesTrainSS, baseDirectory, transform=transform_normalize)\n","    celltype_extra_data = CancerCellTypeDataset(isGoogleColab, dfExtraSS, baseDirectory, transform=transform_normalize)\n","else:\n","    # For testing in a small dataset\n","    dfImagesTrainSS = dfImagesTrainSS.iloc[range(500), :].reset_index()\n","    dfExtraSS = dfExtraSS.iloc[range(1000), :].reset_index()\n","    celltype_training_data = CancerCellTypeDataset(isGoogleColab, dfImagesTrainSS, baseDirectory, transform=transform_normalize, target_transform=None)\n","    celltype_extra_data = CancerCellTypeDataset(isGoogleColab, dfExtraSS, baseDirectory, transform=transform_normalize, target_transform=None)\n","\n","    \n","celltype_train_dataloader = DataLoader(celltype_training_data, batch_size=32, shuffle=True, num_workers=2)\n","celltype_extra_dataloader = DataLoader(celltype_extra_data, batch_size=32, shuffle=True, num_workers=2)"]},{"cell_type":"markdown","metadata":{},"source":["### Semi-supervised Loop\n","\n","Here we will implement the Semi-supervised Learning Loop. This is the high level process:\n","\n","<ol>\n","<li>Train a baseline model using the labelled Main Data</li>\n","<li>Predict using this model on the unlabelled Extra Data, taking care to also retain the Softmax score for each value, which is a probability of being that label</li>\n","<li>Review these predictions, find any that have a high confidence (generally 80% probability is used)</li>\n","<li>Take this data, and apply that predicted label, called a \"pseudo-label\". Incorporate this pseudo-labeled data into the training data (also remove them from the extra data)</li>\n","<li>Train an updated model using the updated training data</li>\n","<li>Iterate the process from Step 2, until no high confidence data is generated, or capped at a number of rounds</li>\n","</ol>"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["maxIterations = 5\n","currentIteration = 0\n","highConfThreshold = 0.8\n","noMorePseudos = False\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test\n","Starting Epoch 0...\n","- Training F1: 0.574\n","- Validation F1: 0.6042677012609118\n","- Accuracy Difference: -0.030267701260911828\n","- IsBetter: -0.030267701260911828 : 99999\n","Starting Epoch 1...\n","- Training F1: 0.574\n","- Validation F1: 0.6042677012609118\n","- Accuracy Difference: -0.030267701260911828\n","- IsBetter: -0.030267701260911828 : 99999\n","Starting Epoch 2...\n","- Training F1: 0.574\n","- Validation F1: 0.6042677012609118\n","- Accuracy Difference: -0.030267701260911828\n","- IsBetter: -0.030267701260911828 : 99999\n","Starting Epoch 3...\n","- Training F1: 0.574\n","- Validation F1: 0.6042677012609118\n","- Accuracy Difference: -0.030267701260911828\n","- IsBetter: -0.030267701260911828 : 99999\n","Starting Epoch 4...\n","- Training F1: 0.574\n","- Validation F1: 0.6042677012609118\n","- Accuracy Difference: -0.030267701260911828\n","- IsBetter: -0.030267701260911828 : 99999\n","Starting Epoch 5...\n","- Training F1: 0.574\n","- Validation F1: 0.6042677012609118\n","- Accuracy Difference: -0.030267701260911828\n","- IsBetter: -0.030267701260911828 : 99999\n","Starting Epoch 6...\n","- Training F1: 0.574\n","- Validation F1: 0.6042677012609118\n","- Accuracy Difference: -0.030267701260911828\n","- IsBetter: -0.030267701260911828 : 99999\n","Starting Epoch 7...\n","- Training F1: 0.574\n","- Validation F1: 0.6042677012609118\n","- Accuracy Difference: -0.030267701260911828\n","- IsBetter: -0.030267701260911828 : 99999\n","Starting Epoch 8...\n","- Training F1: 0.574\n","- Validation F1: 0.6042677012609118\n","- Accuracy Difference: -0.030267701260911828\n","- IsBetter: -0.030267701260911828 : 99999\n","Starting Epoch 9...\n","- Training F1: 0.574\n","- Validation F1: 0.6042677012609118\n","- Accuracy Difference: -0.030267701260911828\n","- IsBetter: -0.030267701260911828 : 99999\n","Starting Epoch 10...\n","- Training F1: 0.574\n","- Validation F1: 0.6042677012609118\n","- Accuracy Difference: -0.030267701260911828\n","- IsBetter: -0.030267701260911828 : 99999\n","Starting Epoch 11...\n","- Training F1: 0.574\n","- Validation F1: 0.6042677012609118\n","- Accuracy Difference: -0.030267701260911828\n","- IsBetter: -0.030267701260911828 : 99999\n","Starting Epoch 12...\n","- Training F1: 0.574\n","- Validation F1: 0.6042677012609118\n","- Accuracy Difference: -0.030267701260911828\n","- IsBetter: -0.030267701260911828 : 99999\n","Starting Epoch 13...\n","- Training F1: 0.574\n","- Validation F1: 0.6042677012609118\n","- Accuracy Difference: -0.030267701260911828\n","- IsBetter: -0.030267701260911828 : 99999\n","Starting Epoch 14...\n","- Training F1: 0.574\n","- Validation F1: 0.6042677012609118\n","- Accuracy Difference: -0.030267701260911828\n","- IsBetter: -0.030267701260911828 : 99999\n","Extra:\n","Confusion matrix: \n","\n","[[   0 1000]\n"," [   0    0]]\n","\n","- Accuracy Score: 0.0\n","- Precision Score: 0.0\n","- Recall Score: 0.0\n","- F1 Score: 0.0\n"]},{"ename":"ValueError","evalue":"Length of values (1000) does not match length of index (10384)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[17], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m extraAcc, y_extra_celltype, y_extra_pred_celltype, y_extra_pred_celltype_scores \u001b[38;5;241m=\u001b[39m predictCellTypeOnDataSetAccuracy(\n\u001b[0;32m      8\u001b[0m     net, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra\u001b[39m\u001b[38;5;124m\"\u001b[39m, celltype_extra_dataloader, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m dfExtraPred \u001b[38;5;241m=\u001b[39m dfExtraSS\n\u001b[1;32m---> 11\u001b[0m \u001b[43mdfExtraPred\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPredCellType\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m y_extra_pred_celltype\n\u001b[0;32m     12\u001b[0m dfExtraPred[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredScore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m y_extra_pred_celltype_scores\n\u001b[0;32m     14\u001b[0m dfExtraPred\u001b[38;5;241m.\u001b[39mhead()\n","File \u001b[1;32mc:\\Users\\nelso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:3163\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array(key, value)\n\u001b[0;32m   3161\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3162\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[1;32m-> 3163\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n","File \u001b[1;32mc:\\Users\\nelso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:3242\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3232\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3233\u001b[0m \u001b[39mAdd series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   3234\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3239\u001b[0m \u001b[39mensure homogeneity.\u001b[39;00m\n\u001b[0;32m   3240\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3241\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_valid_index(value)\n\u001b[1;32m-> 3242\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(key, value)\n\u001b[0;32m   3243\u001b[0m NDFrame\u001b[39m.\u001b[39m_set_item(\u001b[39mself\u001b[39m, key, value)\n\u001b[0;32m   3245\u001b[0m \u001b[39m# check if we are modifying a copy\u001b[39;00m\n\u001b[0;32m   3246\u001b[0m \u001b[39m# try to set first as we want an invalid\u001b[39;00m\n\u001b[0;32m   3247\u001b[0m \u001b[39m# value exception to occur first\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\nelso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, key, value, broadcast)\u001b[0m\n\u001b[0;32m   3894\u001b[0m     value \u001b[39m=\u001b[39m sanitize_index(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[0;32m   3896\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Index) \u001b[39mor\u001b[39;00m is_sequence(value):\n\u001b[0;32m   3897\u001b[0m \n\u001b[0;32m   3898\u001b[0m     \u001b[39m# turn me into an ndarray\u001b[39;00m\n\u001b[1;32m-> 3899\u001b[0m     value \u001b[39m=\u001b[39m sanitize_index(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[0;32m   3900\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, (np\u001b[39m.\u001b[39mndarray, Index)):\n\u001b[0;32m   3901\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(value) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n","File \u001b[1;32mc:\\Users\\nelso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:751\u001b[0m, in \u001b[0;36msanitize_index\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[39mSanitize an index type to return an ndarray of the underlying, pass\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[39mthrough a non-Index.\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[1;32m--> 751\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    752\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    753\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    754\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    755\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    756\u001b[0m     )\n\u001b[0;32m    758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m    759\u001b[0m \n\u001b[0;32m    760\u001b[0m     \u001b[39m# coerce datetimelike types\u001b[39;00m\n\u001b[0;32m    761\u001b[0m     \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mm\u001b[39m\u001b[39m\"\u001b[39m]:\n","\u001b[1;31mValueError\u001b[0m: Length of values (1000) does not match length of index (10384)"]}],"source":["print(\"Test\")\n","\n","# Using the current training dataloader, train a model\n","net, dfLoss = train_celltype_model(0, celltype_train_dataloader)\n","\n","# Predict using the model on all the extra data. Ensure to get the predicted labels and the softmax probability score\n","extraAcc, y_extra_celltype, y_extra_pred_celltype, y_extra_pred_celltype_scores = predictCellTypeOnDataSetAccuracy(\n","    net, \"Extra\", celltype_extra_dataloader, True, True)\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(1000, 6)\n","(1000, 8)\n"]}],"source":["\n","print(dfImagesTrainSS.shape)\n","print(dfExtraSS.shape)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[1.1491333465297668e-14, 3.133239585210827e-14, 1.0, 1.0236431114644257e-13]\n","[3.2361017077847665e-12, 7.531211765332557e-12, 1.0, 2.0281824830714612e-11]\n","[9.122078634994646e-14, 2.2601531919890644e-13, 1.0, 6.840182717113286e-13]\n","[4.943386845701614e-12, 1.1122811872932292e-11, 1.0, 2.92586857519872e-11]\n","[4.363796218432724e-16, 1.35856228509004e-15, 1.0, 4.941116722864073e-15]\n","[2.143725362426397e-14, 5.879509796776275e-14, 1.0, 1.8644816051315016e-13]\n","[1.8943130729931908e-20, 7.505082722232072e-20, 1.0, 3.9656894552291563e-19]\n","[1.152315268147775e-15, 3.390747003241849e-15, 1.0, 1.1747698998212496e-14]\n","[5.873692070092067e-18, 2.045948030780291e-17, 1.0, 8.521272548619939e-17]\n","[5.324228877874246e-15, 1.494396511918239e-14, 1.0, 4.947985313033358e-14]\n","[1.703018079849013e-15, 4.8128297713541466e-15, 1.0, 1.6609116697003518e-14]\n","[4.279453640628816e-18, 1.5070344023099507e-17, 1.0, 6.507964568502623e-17]\n","[7.109092932384914e-18, 2.4237821315902203e-17, 1.0, 1.0186729905335026e-16]\n","[2.152701624678335e-15, 6.139100407637774e-15, 1.0, 2.1239413832839678e-14]\n","[1.7420176838652704e-19, 6.515335657659144e-19, 1.0, 3.191656334406003e-18]\n","[7.433253718259206e-20, 2.9100392310435095e-19, 1.0, 1.4407835011375933e-18]\n","[1.997034041670645e-20, 8.111061021821642e-20, 1.0, 4.2166741476536445e-19]\n","[6.436854765564173e-14, 1.6694435276461833e-13, 1.0, 5.195683293324205e-13]\n","[1.3706846798327311e-17, 4.535793272864304e-17, 1.0, 1.8614725597616545e-16]\n","[4.726614451680335e-19, 1.706852762610175e-18, 1.0, 7.906509932137059e-18]\n"]}],"source":["for i in range(20):\n","    print(y_extra_pred_celltype_scores[i])"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>ImageName</th>\n","      <th>isCancerous</th>\n","      <th>cellType</th>\n","      <th>trainValTest</th>\n","      <th>PredCellType</th>\n","      <th>PredScore</th>\n","      <th>PredScoreTensor</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>./Image_classification_data/patch_images\\10246...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[1.1491333465297668e-14, 3.133239585210827e-14...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>./Image_classification_data/patch_images\\10247...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[3.2361017077847665e-12, 7.531211765332557e-12...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>./Image_classification_data/patch_images\\10248...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[9.122078634994646e-14, 2.2601531919890644e-13...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>./Image_classification_data/patch_images\\10249...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[4.943386845701614e-12, 1.1122811872932292e-11...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>./Image_classification_data/patch_images\\10250...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[4.363796218432724e-16, 1.35856228509004e-15, ...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>./Image_classification_data/patch_images\\10251...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[2.143725362426397e-14, 5.879509796776275e-14,...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>./Image_classification_data/patch_images\\10252...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[1.8943130729931908e-20, 7.505082722232072e-20...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>./Image_classification_data/patch_images\\10253...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[1.152315268147775e-15, 3.390747003241849e-15,...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>./Image_classification_data/patch_images\\10254...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[5.873692070092067e-18, 2.045948030780291e-17,...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>./Image_classification_data/patch_images\\10255...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[5.324228877874246e-15, 1.494396511918239e-14,...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>./Image_classification_data/patch_images\\10256...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[1.703018079849013e-15, 4.8128297713541466e-15...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>./Image_classification_data/patch_images\\10257...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[4.279453640628816e-18, 1.5070344023099507e-17...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>./Image_classification_data/patch_images\\10258...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[7.109092932384914e-18, 2.4237821315902203e-17...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>./Image_classification_data/patch_images\\10259...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[2.152701624678335e-15, 6.139100407637774e-15,...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>./Image_classification_data/patch_images\\10260...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[1.7420176838652704e-19, 6.515335657659144e-19...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>./Image_classification_data/patch_images\\10261...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[7.433253718259206e-20, 2.9100392310435095e-19...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>./Image_classification_data/patch_images\\10264...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[1.997034041670645e-20, 8.111061021821642e-20,...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>./Image_classification_data/patch_images\\10265...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[6.436854765564173e-14, 1.6694435276461833e-13...</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>18</td>\n","      <td>./Image_classification_data/patch_images\\10266...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[1.3706846798327311e-17, 4.535793272864304e-17...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>19</td>\n","      <td>./Image_classification_data/patch_images\\10267...</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>[4.726614451680335e-19, 1.706852762610175e-18,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    index                                          ImageName  isCancerous  \\\n","0       0  ./Image_classification_data/patch_images\\10246...            0   \n","1       1  ./Image_classification_data/patch_images\\10247...            0   \n","2       2  ./Image_classification_data/patch_images\\10248...            0   \n","3       3  ./Image_classification_data/patch_images\\10249...            0   \n","4       4  ./Image_classification_data/patch_images\\10250...            0   \n","5       5  ./Image_classification_data/patch_images\\10251...            0   \n","6       6  ./Image_classification_data/patch_images\\10252...            0   \n","7       7  ./Image_classification_data/patch_images\\10253...            0   \n","8       8  ./Image_classification_data/patch_images\\10254...            0   \n","9       9  ./Image_classification_data/patch_images\\10255...            0   \n","10     10  ./Image_classification_data/patch_images\\10256...            0   \n","11     11  ./Image_classification_data/patch_images\\10257...            0   \n","12     12  ./Image_classification_data/patch_images\\10258...            0   \n","13     13  ./Image_classification_data/patch_images\\10259...            0   \n","14     14  ./Image_classification_data/patch_images\\10260...            0   \n","15     15  ./Image_classification_data/patch_images\\10261...            0   \n","16     16  ./Image_classification_data/patch_images\\10264...            0   \n","17     17  ./Image_classification_data/patch_images\\10265...            0   \n","18     18  ./Image_classification_data/patch_images\\10266...            0   \n","19     19  ./Image_classification_data/patch_images\\10267...            0   \n","\n","    cellType  trainValTest  PredCellType  PredScore  \\\n","0         -1             1             2        1.0   \n","1         -1             0             2        1.0   \n","2         -1             2             2        1.0   \n","3         -1             0             2        1.0   \n","4         -1             0             2        1.0   \n","5         -1             0             2        1.0   \n","6         -1             1             2        1.0   \n","7         -1             2             2        1.0   \n","8         -1             2             2        1.0   \n","9         -1             1             2        1.0   \n","10        -1             0             2        1.0   \n","11        -1             1             2        1.0   \n","12        -1             0             2        1.0   \n","13        -1             2             2        1.0   \n","14        -1             1             2        1.0   \n","15        -1             2             2        1.0   \n","16        -1             1             2        1.0   \n","17        -1             0             2        1.0   \n","18        -1             1             2        1.0   \n","19        -1             0             2        1.0   \n","\n","                                      PredScoreTensor  \n","0   [1.1491333465297668e-14, 3.133239585210827e-14...  \n","1   [3.2361017077847665e-12, 7.531211765332557e-12...  \n","2   [9.122078634994646e-14, 2.2601531919890644e-13...  \n","3   [4.943386845701614e-12, 1.1122811872932292e-11...  \n","4   [4.363796218432724e-16, 1.35856228509004e-15, ...  \n","5   [2.143725362426397e-14, 5.879509796776275e-14,...  \n","6   [1.8943130729931908e-20, 7.505082722232072e-20...  \n","7   [1.152315268147775e-15, 3.390747003241849e-15,...  \n","8   [5.873692070092067e-18, 2.045948030780291e-17,...  \n","9   [5.324228877874246e-15, 1.494396511918239e-14,...  \n","10  [1.703018079849013e-15, 4.8128297713541466e-15...  \n","11  [4.279453640628816e-18, 1.5070344023099507e-17...  \n","12  [7.109092932384914e-18, 2.4237821315902203e-17...  \n","13  [2.152701624678335e-15, 6.139100407637774e-15,...  \n","14  [1.7420176838652704e-19, 6.515335657659144e-19...  \n","15  [7.433253718259206e-20, 2.9100392310435095e-19...  \n","16  [1.997034041670645e-20, 8.111061021821642e-20,...  \n","17  [6.436854765564173e-14, 1.6694435276461833e-13...  \n","18  [1.3706846798327311e-17, 4.535793272864304e-17...  \n","19  [4.726614451680335e-19, 1.706852762610175e-18,...  "]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["dfExtraPred = dfExtraSS\n","dfExtraPred[\"PredCellType\"] = y_extra_pred_celltype\n","dfExtraPred[\"PredScoreTensor\"] = y_extra_pred_celltype_scores\n","\n","pred_scores = []\n","for i in range(len(y_extra_pred_celltype_scores)):\n","    # The predicted label will give the index of the related score\n","    ind = y_extra_pred_celltype[i]\n","    pred_scores.append(y_extra_pred_celltype_scores[i][ind])\n","\n","    \n","dfExtraPred[\"PredScore\"] = pred_scores\n","\n","dfExtraPred.head(20)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(2000, 7)\n","(0, 7)\n"]}],"source":["# Get the high Confidence predicted records and add them to the training data\n","dfHighConf = dfExtraPred[dfExtraPred[\"PredScore\"] >= highConfThreshold]\n","dfHighConf = dfHighConf.drop([\"PredCellType\", \"PredScore\"], axis=1)\n","\n","dfImagesTrainSS = dfImagesTrainSS.append(dfHighConf).reset_index(drop=True)\n","\n","# Filter out the high conf records from the extra dataset\n","dfExtraSS = dfExtraPred[dfExtraPred[\"PredScore\"] < highConfThreshold].reset_index()\n","dfExtraSS = dfExtraSS.drop([\"PredCellType\", \"PredScore\"], axis=1)\n","\n","# Recreate the data loaders\n","celltype_train_dataloader = DataLoader(celltype_training_data, batch_size=32, shuffle=True, num_workers=2)\n","celltype_extra_dataloader = DataLoader(celltype_extra_data, batch_size=32, shuffle=True, num_workers=2)\n","\n","print(dfImagesTrainSS.shape)\n","print(dfExtraSS.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","while (currentIteration < maxIterations) and noMorePseudos == False:\n","    print(\"Semi-Supervised Iteration \" + str(currentIteration))\n","    \n","    # Using the current training dataloader, train a model\n","    print(\"Semi-Supervised Iteration \" + str(currentIteration))\n","    net, dfLoss = train_celltype_model(currentIteration, celltype_train_dataloader)\n","\n","    # Predict using the model on all the extra data. Ensure to get the predicted labels and the softmax probability score\n","    extraAcc, y_extra_celltype, y_extra_pred_celltype, y_extra_pred_celltype_scores = predictCellTypeOnDataSetAccuracy(\n","        net, \"Extra\", celltype_extra_dataloader, True, True)\n","\n","    dfExtraPred = dfExtraSS\n","    dfExtraPred[\"PredCellType\"] = y_extra_pred_celltype\n","    dfExtraPred[\"PredScore\"] = y_extra_pred_celltype_scores\n","\n","\n","    # Get all the predictions that exceed the high confidence threshold\n","    # If there are none, set noMorePseudos = True then break the loop\n","\n","    # Add the high conf records to the training data then update the dataloader\n","\n","    # Filter out the high conf records from the extra dataset\n","\n","    currentIteration += 1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# dfLoss = pd.DataFrame({ 'epoch': lstEpochs, 'train': lstTrainAccs, 'validation': lstValAccs })\n","# graphutil.graphBasicTwoSeries(dfLoss, \"epoch\", \"train\", \"validation\", \"CellType Training and Validation Accuracy\", \n","#         \"Epoch\", \"Training Accuracy\", \"Validation Accuracy\")"]},{"cell_type":"markdown","metadata":{"id":"XPplgEbCsWW0"},"source":["Predict on the Training Set to get the Training Accuracy and Error"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15121,"status":"ok","timestamp":1683530376984,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"dWdn3fsTsWW0","outputId":"8a7d2c09-a37c-429b-9b0f-1cdaf439c9d0"},"outputs":[],"source":["trainingAcc, y_train_celltype, y_train_pred_celltype, y_train_pred_celltype_scores = predictCellTypeOnDataSetAccuracy(\n","    net, \"Training\", celltype_train_dataloader, True, True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1683530376984,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"u27Mb-rTsWW0","outputId":"1c26c722-78c9-49a3-8416-ab1a6ca56423"},"outputs":[],"source":["for i in range(5):\n","    print(y_train_pred_celltype_scores[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1600,"status":"ok","timestamp":1683530378571,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"eiHy8WeqsWW0","outputId":"6f4318a2-121f-41bb-c9cc-a904c85aa1e6"},"outputs":[],"source":["a2util.getClassificationROC(\"CellType\", \"Training\", y_train_celltype, y_train_pred_celltype, 4, y_train_pred_celltype_scores)"]},{"cell_type":"markdown","metadata":{"id":"NpLNiit9K_pr"},"source":["Predict on the Validation data and evaluate the results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":735,"status":"ok","timestamp":1683530379303,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"VfU8z2ljK_pr","outputId":"319eec34-c67e-40e7-f277-97b11f044944"},"outputs":[],"source":["testAccuracy, y_test_celltype, y_test_pred_celltype, y_test_pred_celltype_scores  = predictCellTypeOnDataSetAccuracy(\n","        net, \"Test\", celltype_test_dataloader, True, True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2437,"status":"ok","timestamp":1683530381739,"user":{"displayName":"Nelson Cheng","userId":"13903294221993632929"},"user_tz":-600},"id":"PxUAI5fksWW1","outputId":"4e2b2521-578d-4a34-bf51-ec368d57a6c4"},"outputs":[],"source":["a2util.getClassificationROC(\"CellType\", \"Test\", y_test_celltype, y_test_pred_celltype, 4, y_test_pred_celltype_scores)"]},{"cell_type":"markdown","metadata":{},"source":["# Results\n","\n","Append Results for both Binary IsCancerous and Cell Type here"]},{"cell_type":"markdown","metadata":{},"source":["### IsCancerous Results\n","\n","Basic 01, 3 Layer NN with Dropout - On Colab, Full data - Previous best performing\n","- Training Accuracy: 0.942\n","- Training F1: 0.923\n","- Validation Accuracy: 0.869\n","- Validation F1: 0.897\n","\n","CNN01, 3 Layer Classifier Binary with Sigmoid as Final Activation Function\n","- **Training**\n","- Accuracy Score: 0.9518948577261708\n","- Precision Score: 0.9426820475847152\n","- Recall Score: 0.9230497705612425\n","- F1 Score: 0.9327626181558766\n","- **Validation**\n","- Accuracy Score: 0.8894277400581959\n","- Precision Score: 0.9178981937602627\n","- Recall Score: 0.8972712680577849\n","- F1 Score: 0.9074675324675324\n","\n","CNN02, 3 Layer Classifier with Early Stopping\n","- **Training**\n","- Accuracy Score: 0.9475564629322445\n","- Precision Score: 0.9328091493924232\n","- Recall Score: 0.9212848570420049\n","- F1 Score: 0.927011188066063\n","- **Test**\n","- Accuracy Score: 0.8764591439688716\n","- Precision Score: 0.8936507936507937\n","- Recall Score: 0.9036918138041734\n","- F1 Score: 0.8986432561851556\n","\n","\n","CNN02, 2 Layer Classifier with Early Stopping\n","- **Training**\n","- Accuracy Score: 0.9281612862064565\n","- Precision Score: 0.9235074626865671\n","- Recall Score: 0.8736321920225909\n","- F1 Score: 0.8978777435153275\n","- **Test**\n","- Accuracy Score: 0.8764591439688716\n","- Precision Score: 0.9039087947882736\n","- Recall Score: 0.8908507223113965\n","- F1 Score: 0.8973322554567501\n"]},{"cell_type":"markdown","metadata":{"id":"a1xI7RrpsWW1"},"source":["### Cell type Results\n","\n","Basic 01, 3 Layer NN with Dropout - On Colab, Full data - Previous best performing\n","- Training Accuracy: 0.844\n","- Training F1: 0.844\n","- Validation Accuracy: 0.778\n","- Validation F1: 0.778\n","\n","CNN01, 3 Layer Classifier with Softmax as Final Activation Function, no Early Stopping\n","- **Training**\n","- Accuracy Score: 0.842414189102973\n","- Precision Score: 0.842414189102973\n","- Recall Score: 0.842414189102973\n","- F1 Score: 0.842414189102973\n","- **Validation**\n","- Accuracy Score: 0.7827352085354026\n","- Precision Score: 0.7827352085354026\n","- Recall Score: 0.7827352085354026\n","- F1 Score: 0.7827352085354026\n","\n","CNN02, 3 Layer Classifier with Early Stopping\n","- **Training**\n","- Accuracy Score: 0.8100038279954064\n","- Precision Score: 0.8100038279954064\n","- Recall Score: 0.8100038279954064\n","- F1 Score: 0.8100038279954064\n","- **Validation**\n","- Accuracy Score: 0.7957198443579766\n","- Precision Score: 0.7957198443579766\n","- Recall Score: 0.7957198443579766\n","- F1 Score: 0.7957198443579766\n","\n","CNN02, 2 Layer Classifier with Early Stopping\n","- **Training**\n","- Accuracy Score: 0.7962230445323466\n","- Precision Score: 0.7962230445323466\n","- Recall Score: 0.7962230445323466\n","- F1 Score: 0.7962230445323466\n","- **Test**\n","- Accuracy Score: 0.7928015564202334\n","- Precision Score: 0.7928015564202334\n","- Recall Score: 0.7928015564202334\n","- F1 Score: 0.7928015564202334"]},{"cell_type":"markdown","metadata":{"id":"Ys43DeSksWW1"},"source":["<h1>Analysis of Performance and Accuracy</h1>\n","\n","<h3>Is Cancerous</h3>\n","<p>\n","The best performing model after initial CNN experiments, according to F1 Score, is the CNN with 3 Layer Classifier, with no Early stopping. the F1 Score is <strong>0.907</strong>, which is only a 0.01 improvement on the Basic NN 2 Layer model.\n","</p>\n","<p>\n","There isn't much benefit in the Early Stopping process here, looking at the 3 layer experiment with and without early stopping\n","</p>\n","<p>\n","However, we can see that the difference between the Training F1 and the Test F1 in this CNN is a lot smaller, with the Training F1 being 0.93. This is a situation where we have higher bias but less variance, with higher bias compared to other experiments. Therefore, it may be possible to model with more accuracy to reduce the bias, however, we must be careful not to overfit.\n","</p>\n","\n","<h3>Cell Type</h3>\n","<p>\n","The best performing model after initial CNN experiments, according to Accuracy, is the CNN with 3 Layer Classifier, with Early stopping. the F1 Score is <strong>0.796</strong>, which is a 0.018 improvement on the Basic NN 2 Layer model.\n","</p>\n","<p>\n","However, we can see that the difference between the Training Accuracy and the Test Accuracy in this CNN very small, with the Testing accuracy being <strong>0.81</strong>. This indicates that the model is generalizing very well. However, it appears that this model has a higher bias than other experiments.\n","</p>\n","\n","<h3>Conclusion</h3>\n","<p>\n","Therefore, for both models, it is worth experimenting on how accuracy can be improved. There are 3 methods that initially should be tried\n","</p>\n","<ol>\n","<li>First, experiment with a more complex CNN model, with additional complexity in the Convolution and the Classifier parts</li>\n","<li>Addition of more data to train with, specifically the Extra data provided</li>\n","<li>Data Augmentation</li>\n","</ol>\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"dcbc78149e46ccbab92a3f68a48c52feb0796c7e10dad8e3f1a2a5a780973376"}}},"nbformat":4,"nbformat_minor":0}
