{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDRW51eYK_pg"
      },
      "source": [
        "<hr style=\"color:green\" />\n",
        "<h1 style=\"color:green\">COSC2673 Assignment 2: Image Classification for Cancerous Cells</h1>\n",
        "<h2 style=\"color:green\">File 24: CNN with more complex convolutions</h2>\n",
        "<hr style=\"color:green\" />\n",
        "\n",
        "<p>\n",
        "After Analysis in File 22, CNNs with 3 layer and 2 layer classifiers were experimented with, with good results. But overfitting in these is not the primary concern, first look to improve accuracy\n",
        "</p>\n",
        "<p>\n",
        "Create a custom, simple CNN structure appropriate to the 27x27 pixel image size, this time with a more complex CNN architecture. This may include more complexity in the Convolution Layers or more complexity in the NN layers\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "This notebook initially only considers added complexity to the convolution layers. Previous experiments had a slight improvement with no early stopping, however, for the improvement in training time, use early stopping. Also increase Ealy Stopping patience to 3, as we seem to have instances of validation error being less than training error for a number of epochs in some CNNS\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 8285,
          "status": "ok",
          "timestamp": 1683527199852,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "LuaHh7dfK_pj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.io import read_image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ayZvnueK_pk"
      },
      "source": [
        "Configure this script as to whether it runs on Google Colab, or locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "ok",
          "timestamp": 1683527199853,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "vFtUm6uXK_pk"
      },
      "outputs": [],
      "source": [
        "# When on Google Colab, running full training, change both to true. Locally, advised set both to false\n",
        "isGoogleColab = False\n",
        "useFullData = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 31235,
          "status": "ok",
          "timestamp": 1683527231083,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "zzl3NpfVK_pk",
        "outputId": "164b6588-4bfc-4f20-ba1f-26e3563def4e"
      },
      "outputs": [],
      "source": [
        "# In local, the base directory is the current directory\n",
        "baseDirectory = \"./\"\n",
        "\n",
        "if isGoogleColab:\n",
        "    from google.colab import drive\n",
        "    \n",
        "    # If this is running on Google colab, assume the notebook runs in a \"COSC2673\" folder, which also contains the data files \n",
        "    # in a subfolder called \"image_classification_data\"\n",
        "    drive.mount(\"/content/drive\")\n",
        "    !ls /content/drive/'My Drive'/COSC2673/\n",
        "\n",
        "    # Import the directory so that custom python libraries can be imported\n",
        "    import sys\n",
        "    sys.path.append(\"/content/drive/MyDrive/COSC2673/\")\n",
        "\n",
        "    # Set the base directory to the Google Drive specific folder\n",
        "    baseDirectory = \"/content/drive/MyDrive/COSC2673/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZCfUn3EK_pl"
      },
      "source": [
        "Import the custom python files that contain reusable code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 6079,
          "status": "ok",
          "timestamp": 1683527237159,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "beWSdbauK_pl",
        "outputId": "6c7403c5-4961-4c43-eb4e-a3be29d7dc1d"
      },
      "outputs": [],
      "source": [
        "import data_basic_utility as dbutil\n",
        "import graphing_utility as graphutil\n",
        "import statistics_utility as statsutil\n",
        "\n",
        "import a2_utility as a2util\n",
        "import pytorch_utility as ptutil\n",
        "from pytorch_utility import CancerBinaryDataset\n",
        "from pytorch_utility import CancerCellTypeDataset\n",
        "\n",
        "\n",
        "# randomSeed = dbutil.get_random_seed()\n",
        "randomSeed = 266305\n",
        "print(\"Random Seed: \" + str(randomSeed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 767,
          "status": "ok",
          "timestamp": 1683527237916,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "zgIXX9BXK_pl"
      },
      "outputs": [],
      "source": [
        "# this file should have previously been created in the root directory\n",
        "dfImages = pd.read_csv(baseDirectory + \"images_main.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "executionInfo": {
          "elapsed": 12,
          "status": "ok",
          "timestamp": 1683527237917,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "6_WFGagHK_pl",
        "outputId": "b3660e2b-a19a-49c7-b513-eee0feb2ac3b"
      },
      "outputs": [],
      "source": [
        "# Get The training Split and the Validation Split\n",
        "dfImagesTrain = dfImages[dfImages[\"trainValTest\"] == 0].reset_index()\n",
        "dfImagesVal = dfImages[dfImages[\"trainValTest\"] == 1].reset_index()\n",
        "dfImagesTest = dfImages[dfImages[\"trainValTest\"] == 2].reset_index()\n",
        "\n",
        "print(dfImagesTrain.shape)\n",
        "print(dfImagesVal.shape)\n",
        "print(dfImagesTest.shape)\n",
        "\n",
        "dfImagesTrain.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ1R94YdK_pn"
      },
      "source": [
        "Note: The definition of the Custom Datasets for both the isCancerous data and the Cell Type data are defined in the pytorch_utility.py file.\n",
        "\n",
        "Also, rather than loading all the training images and calculating the mean and standard deviation values in here, that was run separately in file 05a.PyTorchGetMeanAndStd.ipynb\n",
        "\n",
        "Here we can just define the values to use, which shouldn't change unless the data is reloaded and a new train/validation/test split is generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 10,
          "status": "ok",
          "timestamp": 1683527237917,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "qaY5AA-tsWWw",
        "outputId": "53ce6f3a-aeec-4d4d-a697-e083f0b01746"
      },
      "outputs": [],
      "source": [
        "train_mean, train_std = ptutil.getTrainMeanAndStdTensors()\n",
        "print(train_mean)\n",
        "print(train_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 9,
          "status": "ok",
          "timestamp": 1683527237918,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "I8z3yX8ZK_pn"
      },
      "outputs": [],
      "source": [
        "# Create a tranform operation that also normalizes the images according to the mean and standard deviations of the images\n",
        "transform_normalize = transforms.Compose(\n",
        "    [transforms.ToPILImage(),\n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize(train_mean, train_std)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 9,
          "status": "ok",
          "timestamp": 1683527237918,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "qb_33dFmK_pn"
      },
      "outputs": [],
      "source": [
        "cancerous_training_data = None\n",
        "\n",
        "# Create a custom Dataset for the training and validation data\n",
        "if useFullData:\n",
        "    cancerous_training_data = CancerBinaryDataset(isGoogleColab, dfImagesTrain, baseDirectory, transform=transform_normalize)\n",
        "else:\n",
        "    # For testing in a small dataset\n",
        "    dfImagesTrainTest = dfImagesTrain.iloc[range(1000), :].reset_index()\n",
        "    cancerous_training_data = CancerBinaryDataset(isGoogleColab, dfImagesTrainTest, baseDirectory, transform=transform_normalize, target_transform=None)\n",
        "\n",
        "cancerous_validation_data = CancerBinaryDataset(isGoogleColab, dfImagesVal, baseDirectory, transform=transform_normalize, target_transform=None)\n",
        "cancerous_test_data = CancerBinaryDataset(isGoogleColab, dfImagesTest, baseDirectory, transform=transform_normalize, target_transform=None)\n",
        "\n",
        "# Create data loaders\n",
        "cancerous_train_dataloader = DataLoader(cancerous_training_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "cancerous_val_dataloader = DataLoader(cancerous_validation_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "cancerous_test_dataloader = DataLoader(cancerous_test_data, batch_size=32, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4jD7E2PK_po"
      },
      "source": [
        "# Early Stopping\n",
        "\n",
        "Using the basic model (class), refactor the code such that the test and validation predictions are made at the end of each epoch and the loss difference between training error and validation error are calculated. Use the F1 Score as the metric\n",
        "\n",
        "Also implement a \"Patience\" level, where the patience is the number of consecutive epochs that can occur with no improvement in loss before the process is stopped early."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 10,
          "status": "ok",
          "timestamp": 1683527237919,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "0two1S9gK_po"
      },
      "outputs": [],
      "source": [
        "# Create a class for the Neural Network\n",
        "class PT_CNN_IsCancerous(nn.Module):\n",
        "\n",
        "    # In the constructor, initialize the layers to use\n",
        "    def __init__(self):\n",
        "        super(PT_CNN_IsCancerous, self).__init__()\n",
        "\n",
        "        # first, define the subsampling methods. Though they are used multiple times, these are the\n",
        "        # operations, so only need to be defined once\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # define the Activation methods to use\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # define the convolution layers\n",
        "\n",
        "        # input should be 27x27x3. Apply a 5x5 filter but add a zero-padding layer, \n",
        "        # therefore, output should be 24x24x48 (channels aka feature maps)\n",
        "        # Also, increase the number of features \n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=48, kernel_size=5, stride=1, padding=1)\n",
        "        # There will be a Relu\n",
        "        # Then a MaxPool of 2x2, halving the dimensions per feature map\n",
        "        # So input is 12x12x48. Apply a 3x3 filter, also include padding=1, as this is already quite small, and lets consider the edges\n",
        "        self.conv2 = nn.Conv2d(in_channels=48, out_channels=96, kernel_size=3, stride=1, padding=1)    \n",
        "        # There will be a Relu        \n",
        "        # So input is 12x12x96. Apply a 3x3 filter, also include padding=1, as this is already quite small, and lets consider the edges\n",
        "        self.conv3 = nn.Conv2d(in_channels=96, out_channels=192, kernel_size=3, stride=1, padding=1)\n",
        "        # Then a MaxPool of 2x2, halving the dimensions per feature map\n",
        "        \n",
        "        # define the fully connected neural layers\n",
        "        self.fc1 = nn.Linear(192 * 6 * 6, 6912)\n",
        "        self.fc2 = nn.Linear(6912, 3456)\n",
        "        self.fc3 = nn.Linear(3456, 2)\n",
        "\n",
        "    # Create the forward function, which is used in training\n",
        "    def forward(self, x):\n",
        "\n",
        "        # print(\"Init Shape: \" + str(x.shape))\n",
        "\n",
        "        # Process the first 2 convolution layers, applying maxpooling\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Then process the remaining convolution layers without any pooling\n",
        "        x = self.relu(self.conv2(x))        \n",
        "        x = self.relu(self.conv3(x))\n",
        "\n",
        "        # Then apply a max pool and average pool on the result\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Flatten: This should convert to tensors that are acceptable for the input into the NN 3 layers\n",
        "        x = x.view(x.size(0), 192 * 6 * 6)\n",
        "\n",
        "        # Now process the 3 layers of the Fully Connected NN\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))        \n",
        "        # x = self.relu(self.fc3(x))        \n",
        "        # x = self.fc3(x)        \n",
        "\n",
        "        # return the result\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO5dPhy6K_po"
      },
      "source": [
        "Now train the Fully Connected Neural Network Model.\n",
        "\n",
        "During training, we will use the following:\n",
        "- Softmax Cross Entropy Loss as our Loss function. This is a good Loss function that basically converts scores for each class into probabilities\n",
        "- The Adam Optimizer, which is a version of Gradient Descent\n",
        "- Initially, just 10 epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc7rjcsiCKbe"
      },
      "source": [
        "Create a function to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 9,
          "status": "ok",
          "timestamp": 1683527237919,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "7SAdIfM-CKbe"
      },
      "outputs": [],
      "source": [
        "def predictCancerousOnDataSetF1(net, setName, dataloader, printResult=True, printFullResults=False):\n",
        "    correct, total = 0,  0\n",
        "    predictions = []\n",
        "\n",
        "    y_cancerous = []\n",
        "    y_pred_cancerous = []\n",
        "\n",
        "    # Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        # Loop through the batch, build the lists of the raw label and prediction values\n",
        "        for j in range(len(labels)):\n",
        "            y_cancerous.append(labels[j].item())\n",
        "            y_pred_cancerous.append(predicted[j].item())\n",
        "\n",
        "        predictions.append(predicted)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    f1Score = f1_score(y_cancerous, y_pred_cancerous)\n",
        "\n",
        "    if printFullResults:\n",
        "        print(setName + \":\")        \n",
        "        print('Confusion matrix: \\n')\n",
        "        print(confusion_matrix(y_cancerous, y_pred_cancerous))\n",
        "        print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_cancerous, y_pred_cancerous)))\n",
        "        print(\"- Precision Score: \" + str(precision_score(y_cancerous, y_pred_cancerous)))\n",
        "        print(\"- Recall Score: \" + str(recall_score(y_cancerous, y_pred_cancerous)))\n",
        "        print(\"- F1 Score: \" + str(f1Score))\n",
        "    elif printResult:\n",
        "        print(\"- \" + setName + \" F1: \" + str(f1Score))        \n",
        "\n",
        "    return f1Score, y_cancerous, y_pred_cancerous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2953420,
          "status": "ok",
          "timestamp": 1683530191330,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "vQUAMyGuK_po",
        "outputId": "1249f0c1-16d1-4685-e541-dd82aca7d58e"
      },
      "outputs": [],
      "source": [
        "# set the Learning Rate to use\n",
        "learning_rate = 0.0001\n",
        "maxEpochs = 15\n",
        "patience = 3\n",
        "disableEarlyStopping = False\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use this for L2 Regularization\n",
        "# net = PT_NN_IsCancerous()\n",
        "# optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "\n",
        "# Use this for dropouts\n",
        "net = PT_CNN_IsCancerous()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "bestErrorDiff = 99999\n",
        "concurrentNonImproves = 0\n",
        "currentEpoch = 0\n",
        "\n",
        "bestValF1 = -1\n",
        "lstEpochs = []\n",
        "lstTrainF1s = []\n",
        "lstValF1s = []\n",
        "for epoch in range(maxEpochs):\n",
        "    print(\"Starting Epoch \" + str(epoch) + \"...\")\n",
        "    currentEpoch = epoch\n",
        "\n",
        "    # Set the Neural Network into training mode\n",
        "    net.train()\n",
        "\n",
        "    # Train through this epoch\n",
        "    for i, data in enumerate(cancerous_train_dataloader, 0):\n",
        "        # Get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Perform Forward and Backward propagation then optimize the weights\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Set the Neural Network into evaluation (test) mode, so we can evaluate both training and validation error\n",
        "    net.eval()        \n",
        "    trainingF1, y_train_cancerous, y_train_pred_cancerous = predictCancerousOnDataSetF1(net, \"Training\", cancerous_train_dataloader, True, False)\n",
        "    validationF1, y_val_cancerous, y_val_pred_cancerous = predictCancerousOnDataSetF1(net, \"Validation\", cancerous_val_dataloader, True, False)\n",
        "\n",
        "    errorDiff = trainingF1 - validationF1\n",
        "    print(\"- F1 Difference: \" + str(errorDiff))\n",
        "\n",
        "    lstEpochs.append(epoch)\n",
        "    lstTrainF1s.append(trainingF1)\n",
        "    lstValF1s.append(validationF1)\n",
        "\n",
        "    if epoch > 0 and (validationF1 - bestValF1 > 0.01):        \n",
        "        # There is at least percentage point improvement in the validation F1, count this as a \n",
        "        # good iteration, regardless of the error difference\n",
        "        print(\"- IsGoodStep\")\n",
        "        concurrentNonImproves = 0\n",
        "        if errorDiff > 0 and errorDiff < bestErrorDiff:  \n",
        "            bestErrorDiff = errorDiff        \n",
        "    elif errorDiff < bestErrorDiff:        \n",
        "        # This epoch is an improvement on the last, so we will continue. the concurrent non improve counts reset for the patience\n",
        "        print(\"- IsBetter: \" + str(errorDiff) + \" : \" + str(bestErrorDiff))        \n",
        "        concurrentNonImproves = 0\n",
        "        if errorDiff > 0:\n",
        "            bestErrorDiff = errorDiff\n",
        "    else:\n",
        "        # This epoch has the same or worse performance than the last. Check if we have reached the patience, if so, then stop early\n",
        "        print(\"- IsNotBetter: \" + str(errorDiff) + \" : \" + str(bestErrorDiff))        \n",
        "        concurrentNonImproves += 1\n",
        "        if disableEarlyStopping == False:\n",
        "            if concurrentNonImproves >= patience:\n",
        "                print(\"Early Stopping occurred at Epoch \" + str(epoch))\n",
        "                break\n",
        "\n",
        "    # update the val F1 score from the previous epoch if it's the best\n",
        "    if validationF1 > bestValF1:\n",
        "        bestValF1 = validationF1\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LezmNEykCKbg"
      },
      "source": [
        "Look at the training and validation loss plot to track how the model improved during epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "executionInfo": {
          "elapsed": 11,
          "status": "ok",
          "timestamp": 1683530191331,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "OznU1dWACKbg",
        "outputId": "3430c067-47f7-4cd6-c8b6-109adb1208c7"
      },
      "outputs": [],
      "source": [
        "dfLoss = pd.DataFrame({ 'epoch': lstEpochs, 'train': lstTrainF1s, 'validation': lstValF1s })\n",
        "graphutil.graphBasicTwoSeries(dfLoss, \"epoch\", \"train\", \"validation\", \"IsCancerous Training and Validation F1\", \n",
        "        \"Epoch\", \"Training F1\", \"Validation F1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEB7hSkAK_po"
      },
      "source": [
        "Training Time in Nelson's Local Environment on the full data takes a very long time, stopped after 100 minutes. This will need to be done in Colab.\n",
        "\n",
        "First, Predict on the training data so that we can find the training error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 14613,
          "status": "ok",
          "timestamp": 1683530205937,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "f1E180BesWWy",
        "outputId": "f6e5c3e9-f40a-4fa8-c4d9-29fcbdeb1169"
      },
      "outputs": [],
      "source": [
        "# Use the final NN model to predict on the training data\n",
        "trainingF1, y_train_cancerous, y_train_pred_cancerous = predictCancerousOnDataSetF1(net, \"Training\", cancerous_train_dataloader, True, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "executionInfo": {
          "elapsed": 7,
          "status": "ok",
          "timestamp": 1683530205938,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "4M0Jb0fosWWy",
        "outputId": "957ef8c3-e39e-4688-9b56-65bf55aa13ba"
      },
      "outputs": [],
      "source": [
        "a2util.getClassificationROC(\"IsCancerous\", \"Training\", y_train_cancerous, y_train_pred_cancerous)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbvP6a19sWWy"
      },
      "source": [
        "Now Predict according to the Validation data and evaluate. While looping through here, we will need to get out the Labels from the data loader, because the order of predictions in the batches do not match the order of the original Target values in the dataset (because we turned Shuffle on)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1567,
          "status": "ok",
          "timestamp": 1683530207501,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "ithF1bLMK_pp",
        "outputId": "e8404f51-0136-4da5-8919-385846d654ed"
      },
      "outputs": [],
      "source": [
        "testF1, y_test_cancerous, y_test_pred_cancerous = predictCancerousOnDataSetF1(net, \"Test\", cancerous_test_dataloader, True, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "executionInfo": {
          "elapsed": 957,
          "status": "ok",
          "timestamp": 1683530208455,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "ivOulSepsWWz",
        "outputId": "af3f23f2-9aab-48d3-b01b-e98f5b613625"
      },
      "outputs": [],
      "source": [
        "a2util.getClassificationROC(\"IsCancerous\", \"Test\", y_test_cancerous, y_test_pred_cancerous, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHozRzzdK_pq"
      },
      "source": [
        "Now also train a model for CellType Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 9,
          "status": "ok",
          "timestamp": 1683530208456,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "AVvBdxxjK_pq",
        "outputId": "f39b609a-d26c-41e4-e295-3ee0e50d2573"
      },
      "outputs": [],
      "source": [
        "celltype_training_data = None\n",
        "\n",
        "# Create a custom Dataset for the training and validation data\n",
        "if useFullData:\n",
        "    celltype_training_data = CancerCellTypeDataset(isGoogleColab, dfImagesTrain, baseDirectory, transform=transform_normalize)\n",
        "else:\n",
        "    # For testing in a small dataset\n",
        "    dfImagesTrainTest = dfImagesTrain.iloc[range(1000), :].reset_index()\n",
        "    celltype_training_data = CancerCellTypeDataset(isGoogleColab, dfImagesTrainTest, baseDirectory, transform=transform_normalize, target_transform=None)\n",
        "\n",
        "celltype_validation_data = CancerCellTypeDataset(isGoogleColab, dfImagesVal, baseDirectory, transform=transform_normalize)\n",
        "celltype_test_data = CancerCellTypeDataset(isGoogleColab, dfImagesTest, baseDirectory, transform=transform_normalize)\n",
        "\n",
        "# Create data loaders\n",
        "celltype_train_dataloader = DataLoader(celltype_training_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "celltype_val_dataloader = DataLoader(celltype_validation_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "celltype_test_dataloader = DataLoader(celltype_test_data, batch_size=32, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd9XOjhpK_pq"
      },
      "source": [
        "Create a class for the Cell Type Neural Network model. The structure of the class will be fundamentally the same, only the model will need to output 4 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 7,
          "status": "ok",
          "timestamp": 1683530208456,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "eKOxU2-4K_pq"
      },
      "outputs": [],
      "source": [
        "# Create a class for the Neural Network\n",
        "class PT_CNN_CellType(nn.Module):\n",
        "\n",
        "    # In the constructor, initialize the layers to use\n",
        "    def __init__(self):\n",
        "        super(PT_CNN_CellType, self).__init__()\n",
        "\n",
        "        # first, define the subsampling methods. Though they are used multiple times, these are the\n",
        "        # operations, so only need to be defined once\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # define the Activation methods to use\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # define the convolution layers\n",
        "\n",
        "        # input should be 27x27x3. Apply a 5x5 filter but add a zero-padding layer, \n",
        "        # therefore, output should be 24x24x48 (channels aka feature maps)\n",
        "        # Also, increase the number of features \n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=48, kernel_size=5, stride=1, padding=1)\n",
        "        # There will be a Relu\n",
        "        # Then a MaxPool of 2x2, halving the dimensions per feature map\n",
        "        # So input is 12x12x48. Apply a 3x3 filter, also include padding=1, as this is already quite small, and lets consider the edges\n",
        "        self.conv2 = nn.Conv2d(in_channels=48, out_channels=96, kernel_size=3, stride=1, padding=1)    \n",
        "        # There will be a Relu        \n",
        "        # So input is 12x12x96. Apply a 3x3 filter, also include padding=1, as this is already quite small, and lets consider the edges\n",
        "        self.conv3 = nn.Conv2d(in_channels=96, out_channels=192, kernel_size=3, stride=1, padding=1)\n",
        "        # Then a MaxPool of 2x2, halving the dimensions per feature map\n",
        "        \n",
        "        # define the fully connected neural layers\n",
        "        self.fc1 = nn.Linear(192 * 6 * 6, 6912)\n",
        "        self.fc2 = nn.Linear(6912, 3456)\n",
        "        self.fc3 = nn.Linear(3456, 4)\n",
        "\n",
        "    # Create the forward function, which is used in training\n",
        "    def forward(self, x):\n",
        "\n",
        "        # print(\"Init Shape: \" + str(x.shape))\n",
        "\n",
        "        # Process the first 2 convolution layers, applying maxpooling\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Then process the remaining convolution layers without any pooling\n",
        "        x = self.relu(self.conv2(x))        \n",
        "        x = self.relu(self.conv3(x))\n",
        "\n",
        "        # Then apply a max pool and average pool on the result\n",
        "        x = self.maxpool(x)\n",
        "        #x = self.avgpool(x)\n",
        "\n",
        "        # Flatten: This should convert to tensors that are acceptable for the input into the NN 3 layers\n",
        "        x = x.view(x.size(0), 192 * 6 * 6)\n",
        "\n",
        "        # Now process the 3 layers of the Fully Connected NN\n",
        "        x = self.relu(self.fc1(x))  \n",
        "        x = self.relu(self.fc2(x))              \n",
        "        # x = self.fc3(x)\n",
        "        # x = self.relu(self.fc3(x))\n",
        "        x = F.softmax(self.fc3(x), dim=1)        \n",
        "\n",
        "        # return the result\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRU0R9EMK_pq"
      },
      "source": [
        "Now train the Fully Connected Neural Network Model. Use the same configuration (objective function, optimizer etc) as the Binary Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 8,
          "status": "ok",
          "timestamp": 1683530208457,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "9sPKkwUkCKbm"
      },
      "outputs": [],
      "source": [
        "def predictCellTypeOnDataSetAccuracy(net, setName, dataloader, printResult=True, printFullResults=False):\n",
        "    correct, total = 0,  0\n",
        "    predictions = []\n",
        "\n",
        "    y_celltype = []\n",
        "    y_pred_celltype = []\n",
        "    y_pred_celltype_scores = []\n",
        "\n",
        "    # Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        # Loop through the batch, build the lists of the raw label and prediction values\n",
        "        for j in range(len(labels)):\n",
        "            y_celltype.append(labels[j].item())\n",
        "            y_pred_celltype.append(predicted[j].item())\n",
        "            y_pred_celltype_scores.append(outputs.data[j].tolist())\n",
        "\n",
        "        predictions.append(predicted)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = accuracy_score(y_celltype, y_pred_celltype)\n",
        "    f1Score = f1_score(y_celltype, y_pred_celltype, average=\"micro\")\n",
        "\n",
        "    if printFullResults:\n",
        "        print(setName + \":\")        \n",
        "        print('Confusion matrix: \\n')\n",
        "        print(confusion_matrix(y_celltype, y_pred_celltype))\n",
        "        print(\"\\n- Accuracy Score: \" + str(accuracy))\n",
        "        print(\"- Precision Score: \" + str(precision_score(y_celltype, y_pred_celltype, average=\"micro\")))\n",
        "        print(\"- Recall Score: \" + str(recall_score(y_celltype, y_pred_celltype, average=\"micro\")))\n",
        "        print(\"- F1 Score: \" + str(f1Score))\n",
        "    elif printResult:\n",
        "        print(\"- \" + setName + \" Accuracy: \" + str(accuracy))\n",
        "\n",
        "    return accuracy, y_celltype, y_pred_celltype, y_pred_celltype_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 153417,
          "status": "ok",
          "timestamp": 1683530361867,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "d3FjNKScK_pq",
        "outputId": "8843af12-24dd-4b58-a714-d447c1d66f9b"
      },
      "outputs": [],
      "source": [
        "# set the Learning Rate to use\n",
        "learning_rate = 0.0001\n",
        "maxEpochs = 15\n",
        "patience = 3\n",
        "disableEarlyStopping = False\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use this for L2 Regularization\n",
        "# net = PT_NN_IsCancerous()\n",
        "# optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "\n",
        "# Use this for dropouts\n",
        "net = PT_CNN_CellType()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "bestErrorDiff = 99999\n",
        "concurrentNonImproves = 0\n",
        "currentEpoch = 0\n",
        "\n",
        "bestValAcc = -1\n",
        "lstEpochs = []\n",
        "lstTrainAccs = []\n",
        "lstValAccs = []\n",
        "for epoch in range(maxEpochs):\n",
        "    print(\"Starting Epoch \" + str(epoch) + \"...\")\n",
        "    currentEpoch = epoch\n",
        "\n",
        "    # Set the Neural Network into training mode\n",
        "    net.train()\n",
        "\n",
        "    # Train through this epoch\n",
        "    for i, data in enumerate(celltype_train_dataloader, 0):\n",
        "        # Get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Perform Forward and Backward propagation then optimize the weights\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Set the Neural Network into evaluation (test) mode, so we can evaluate both training and validation error\n",
        "    net.eval()        \n",
        "    trainingAccuracy, y_train_celltype, y_train_pred_celltype, y_train_pred_celltype_scores = predictCellTypeOnDataSetAccuracy(\n",
        "        net, \"Training\", celltype_train_dataloader, True, False)\n",
        "    validationAccuracy, y_val_celltype, y_val_pred_celltype, y_val_pred_celltype_scores  = predictCellTypeOnDataSetAccuracy(\n",
        "        net, \"Validation\", celltype_val_dataloader, True, False)\n",
        "\n",
        "    errorDiff = trainingAccuracy - validationAccuracy\n",
        "    print(\"- Accuracy Difference: \" + str(errorDiff))\n",
        "\n",
        "    lstEpochs.append(epoch)\n",
        "    lstTrainAccs.append(trainingAccuracy)\n",
        "    lstValAccs.append(validationAccuracy)\n",
        "\n",
        "    if epoch > 0 and (validationAccuracy - bestValAcc > 0.01):        \n",
        "        # There is at least percentage point improvement in the validation F1, count this as a \n",
        "        # good iteration, regardless of the error difference\n",
        "        print(\"- IsGoodStep\")\n",
        "        concurrentNonImproves = 0\n",
        "        if errorDiff > 0 and errorDiff < bestErrorDiff:  \n",
        "            bestErrorDiff = errorDiff        \n",
        "    elif errorDiff < bestErrorDiff:        \n",
        "        # This epoch is an improvement on the last, so we will continue. the concurrent non improve counts reset for the patience\n",
        "        print(\"- IsBetter: \" + str(errorDiff) + \" : \" + str(bestErrorDiff))        \n",
        "        concurrentNonImproves = 0\n",
        "        if errorDiff > 0:\n",
        "            bestErrorDiff = errorDiff\n",
        "    else:\n",
        "        # This epoch has the same or worse performance than the last. Check if we have reached the patience, if so, then stop early\n",
        "        print(\"- IsNotBetter: \" + str(errorDiff) + \" : \" + str(bestErrorDiff))        \n",
        "        concurrentNonImproves += 1\n",
        "        if disableEarlyStopping == False:\n",
        "            if concurrentNonImproves >= patience:\n",
        "                print(\"Early Stopping occurred at Epoch \" + str(epoch))\n",
        "                break\n",
        "\n",
        "    # update the val F1 score from the previous epoch if it's the best\n",
        "    if validationAccuracy > bestValAcc:\n",
        "        bestValAcc = validationAccuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfLoss = pd.DataFrame({ 'epoch': lstEpochs, 'train': lstTrainAccs, 'validation': lstValAccs })\n",
        "graphutil.graphBasicTwoSeries(dfLoss, \"epoch\", \"train\", \"validation\", \"CellType Training and Validation Accuracy\", \n",
        "        \"Epoch\", \"Training Accuracy\", \"Validation Accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPplgEbCsWW0"
      },
      "source": [
        "Predict on the Training Set to get the Training Accuracy and Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 15121,
          "status": "ok",
          "timestamp": 1683530376984,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "dWdn3fsTsWW0",
        "outputId": "8a7d2c09-a37c-429b-9b0f-1cdaf439c9d0"
      },
      "outputs": [],
      "source": [
        "trainingAcc, y_train_celltype, y_train_pred_celltype, y_train_pred_celltype_scores = predictCellTypeOnDataSetAccuracy(\n",
        "    net, \"Training\", celltype_train_dataloader, True, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 15,
          "status": "ok",
          "timestamp": 1683530376984,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "u27Mb-rTsWW0",
        "outputId": "1c26c722-78c9-49a3-8416-ab1a6ca56423"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    print(y_train_pred_celltype_scores[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 1600,
          "status": "ok",
          "timestamp": 1683530378571,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "eiHy8WeqsWW0",
        "outputId": "6f4318a2-121f-41bb-c9cc-a904c85aa1e6"
      },
      "outputs": [],
      "source": [
        "a2util.getClassificationROC(\"CellType\", \"Training\", y_train_celltype, y_train_pred_celltype, 4, y_train_pred_celltype_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpLNiit9K_pr"
      },
      "source": [
        "Predict on the Validation data and evaluate the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 735,
          "status": "ok",
          "timestamp": 1683530379303,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "VfU8z2ljK_pr",
        "outputId": "319eec34-c67e-40e7-f277-97b11f044944"
      },
      "outputs": [],
      "source": [
        "testAccuracy, y_test_celltype, y_test_pred_celltype, y_test_pred_celltype_scores  = predictCellTypeOnDataSetAccuracy(\n",
        "        net, \"Test\", celltype_test_dataloader, True, True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 2437,
          "status": "ok",
          "timestamp": 1683530381739,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "PxUAI5fksWW1",
        "outputId": "4e2b2521-578d-4a34-bf51-ec368d57a6c4"
      },
      "outputs": [],
      "source": [
        "a2util.getClassificationROC(\"CellType\", \"Test\", y_test_celltype, y_test_pred_celltype, 4, y_test_pred_celltype_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results\n",
        "\n",
        "Append Results for both Binary IsCancerous and Cell Type here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### IsCancerous Results\n",
        "\n",
        "Basic 01, 3 Layer NN with Dropout - On Colab, Full data - Previous best performing\n",
        "- Training Accuracy: 0.942\n",
        "- Training F1: 0.923\n",
        "- Validation Accuracy: 0.869\n",
        "- Validation F1: 0.897\n",
        "\n",
        "CNN01, 3 Layer Classifier Binary with Sigmoid as Final Activation Function\n",
        "- **Training**\n",
        "- Accuracy Score: 0.9518948577261708\n",
        "- Precision Score: 0.9426820475847152\n",
        "- Recall Score: 0.9230497705612425\n",
        "- F1 Score: 0.9327626181558766\n",
        "- **Validation**\n",
        "- Accuracy Score: 0.8894277400581959\n",
        "- Precision Score: 0.9178981937602627\n",
        "- Recall Score: 0.8972712680577849\n",
        "- F1 Score: 0.9074675324675324\n",
        "\n",
        "CNN02, 3 Layer Classifier with Early Stopping\n",
        "- **Training**\n",
        "- **Validation**\n",
        "\n",
        "\n",
        "CNN02, 2 Layer Classifier with Early Stopping\n",
        "- **Training**\n",
        "- Accuracy Score: 0.9281612862064565\n",
        "- Precision Score: 0.9235074626865671\n",
        "- Recall Score: 0.8736321920225909\n",
        "- F1 Score: 0.8978777435153275\n",
        "- **Test**\n",
        "- Accuracy Score: 0.8764591439688716\n",
        "- Precision Score: 0.9039087947882736\n",
        "- Recall Score: 0.8908507223113965\n",
        "- F1 Score: 0.8973322554567501\n",
        "\n",
        "CNN03, 3 Layer Classifier, more complex Convolutions\n",
        "- **Training**\n",
        "- Accuracy Score: 0.9532984560418527\n",
        "- Precision Score: 0.949690120306234\n",
        "- Recall Score: 0.9195199435227673\n",
        "- F1 Score: 0.9343615494978479\n",
        "- **Validation**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1xI7RrpsWW1"
      },
      "source": [
        "### Cell type Results\n",
        "\n",
        "Basic 01, 3 Layer NN with Dropout - On Colab, Full data - Previous best performing\n",
        "- Training Accuracy: 0.844\n",
        "- Training F1: 0.844\n",
        "- Validation Accuracy: 0.778\n",
        "- Validation F1: 0.778\n",
        "\n",
        "CNN01, 3 Layer Classifier with Softmax as Final Activation Function, no Early Stopping\n",
        "- **Training**\n",
        "- Accuracy Score: 0.842414189102973\n",
        "- Precision Score: 0.842414189102973\n",
        "- Recall Score: 0.842414189102973\n",
        "- F1 Score: 0.842414189102973\n",
        "- **Validation**\n",
        "- Accuracy Score: 0.7827352085354026\n",
        "- Precision Score: 0.7827352085354026\n",
        "- Recall Score: 0.7827352085354026\n",
        "- F1 Score: 0.7827352085354026\n",
        "\n",
        "CNN02, 3 Layer Classifier with Early Stopping\n",
        "- **Training**\n",
        "- Accuracy Score: 0.8100038279954064\n",
        "- Precision Score: 0.8100038279954064\n",
        "- Recall Score: 0.8100038279954064\n",
        "- F1 Score: 0.8100038279954064\n",
        "- **Validation**\n",
        "- Accuracy Score: 0.7957198443579766\n",
        "- Precision Score: 0.7957198443579766\n",
        "- Recall Score: 0.7957198443579766\n",
        "- F1 Score: 0.7957198443579766\n",
        "\n",
        "CNN02, 2 Layer Classifier with Early Stopping\n",
        "- **Training**\n",
        "- Accuracy Score: 0.7962230445323466\n",
        "- Precision Score: 0.7962230445323466\n",
        "- Recall Score: 0.7962230445323466\n",
        "- F1 Score: 0.7962230445323466\n",
        "- **Test**\n",
        "- Accuracy Score: 0.7928015564202334\n",
        "- Precision Score: 0.7928015564202334\n",
        "- Recall Score: 0.7928015564202334\n",
        "- F1 Score: 0.7928015564202334"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys43DeSksWW1"
      },
      "source": [
        "<h1>Analysis of Performance and Accuracy</h1>\n",
        "\n",
        "<h3>Is Cancerous</h3>\n",
        "<p>\n",
        "The best performing model after initial CNN experiments, according to F1 Score, is the CNN with 3 Layer Classifier, with no Early stopping. the F1 Score is <strong>0.907</strong>, which is only a 0.01 improvement on the Basic NN 2 Layer model.\n",
        "</p>\n",
        "<p>\n",
        "However, we can see that the difference between the Training F1 and the Test F1 in this CNN is a lot smaller, with the Training F1 being 0.93. This is a situation where we have higher bias but less variance, with higher bias compared to other experiments. Therefore, it may be possible to model with more accuracy to reduce the bias, however, we must be careful not to overfit.\n",
        "</p>\n",
        "\n",
        "<h3>Is Cancerous</h3>\n",
        "<p>\n",
        "The best performing model after initial CNN experiments, according to Accuracy, is the CNN with 2 Layer Classifier, with Early stopping. the F1 Score is <strong>0.792</strong>, which is a 0.014 improvement on the Basic NN 2 Layer model.\n",
        "</p>\n",
        "<p>\n",
        "However, we can see that the difference between the Training Accuracy and the Test Accuracy in this CNN very small, with the Testing accuracy being <strong>0.796</strong>. This indicates that the model is generalizing very well. However, it appears that this model has a higher bias than other experiments.\n",
        "</p>\n",
        "\n",
        "<h3>Conclusion</h3>\n",
        "<p>\n",
        "Therefore, for both models, it is worth experimenting on how accuracy can be improved. There are 3 methods that initially should be tried\n",
        "</p>\n",
        "<ol>\n",
        "<li>First, experiment with a more complex CNN model, with additional complexity in the Convolution and the Classifier parts</li>\n",
        "<li>Addition of more data to train with, specifically the Extra data provided</li>\n",
        "<li>Data Augmentation</li>\n",
        "</ol>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDRW51eYK_pg"
      },
      "source": [
        "<hr style=\"color:green\" />\n",
        "<h1 style=\"color:green\">COSC2673 Assignment 2: Image Classification for Cancerous Cells</h1>\n",
        "<h2 style=\"color:green\">File 25: CNN with more complex NN layers</h2>\n",
        "<hr style=\"color:green\" />\n",
        "\n",
        "<p>\n",
        "After Analysis in File 22, CNNs with 3 layer and 2 layer classifiers were experimented with, with good results. But overfitting in these is not the primary concern, first look to improve accuracy\n",
        "</p>\n",
        "<p>\n",
        "Create a custom, simple CNN structure appropriate to the 27x27 pixel image size, this time with a more complex CNN architecture. This may include more complexity in the Convolution Layers or more complexity in the NN layers\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "This notebook initially only considers added complexity to the convolution layers. Previous experiments had a slight improvement with no early stopping, however, for the improvement in training time, use early stopping. Also increase Ealy Stopping patience to 3, as we seem to have instances of validation error being less than training error for a number of epochs in some CNNS\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 8285,
          "status": "ok",
          "timestamp": 1683527199852,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "LuaHh7dfK_pj"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import cv2\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# from sklearn.metrics import classification_report\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from sklearn.metrics import precision_score\n",
        "# from sklearn.metrics import recall_score\n",
        "# from sklearn.metrics import f1_score\n",
        "# from sklearn.metrics import roc_curve\n",
        "# from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "# import torchvision\n",
        "# import torch.utils.data\n",
        "# import torchvision.transforms as transforms\n",
        "# from torch.utils.data import Dataset\n",
        "# from torch.utils.data import DataLoader\n",
        "# from torchvision.io import read_image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ayZvnueK_pk"
      },
      "source": [
        "Configure this script as to whether it runs on Google Colab, or locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "ok",
          "timestamp": 1683527199853,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "vFtUm6uXK_pk"
      },
      "outputs": [],
      "source": [
        "# # When on Google Colab, running full training, change both to true. Locally, advised set both to false\n",
        "# isGoogleColab = False\n",
        "# useFullData = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 31235,
          "status": "ok",
          "timestamp": 1683527231083,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "zzl3NpfVK_pk",
        "outputId": "164b6588-4bfc-4f20-ba1f-26e3563def4e"
      },
      "outputs": [],
      "source": [
        "# # In local, the base directory is the current directory\n",
        "# baseDirectory = \"./\"\n",
        "\n",
        "# if isGoogleColab:\n",
        "#     from google.colab import drive\n",
        "    \n",
        "#     # If this is running on Google colab, assume the notebook runs in a \"COSC2673\" folder, which also contains the data files \n",
        "#     # in a subfolder called \"image_classification_data\"\n",
        "#     drive.mount(\"/content/drive\")\n",
        "#     !ls /content/drive/'My Drive'/COSC2673/\n",
        "\n",
        "#     # Import the directory so that custom python libraries can be imported\n",
        "#     import sys\n",
        "#     sys.path.append(\"/content/drive/MyDrive/COSC2673/\")\n",
        "\n",
        "#     # Set the base directory to the Google Drive specific folder\n",
        "#     baseDirectory = \"/content/drive/MyDrive/COSC2673/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZCfUn3EK_pl"
      },
      "source": [
        "Import the custom python files that contain reusable code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 6079,
          "status": "ok",
          "timestamp": 1683527237159,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "beWSdbauK_pl",
        "outputId": "6c7403c5-4961-4c43-eb4e-a3be29d7dc1d"
      },
      "outputs": [],
      "source": [
        "# import data_basic_utility as dbutil\n",
        "# import graphing_utility as graphutil\n",
        "# import statistics_utility as statsutil\n",
        "\n",
        "# import a2_utility as a2util\n",
        "# import pytorch_utility as ptutil\n",
        "# from pytorch_utility import CancerBinaryDataset\n",
        "# from pytorch_utility import CancerCellTypeDataset\n",
        "\n",
        "\n",
        "# # randomSeed = dbutil.get_random_seed()\n",
        "# randomSeed = 266305\n",
        "# print(\"Random Seed: \" + str(randomSeed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 767,
          "status": "ok",
          "timestamp": 1683527237916,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "zgIXX9BXK_pl"
      },
      "outputs": [],
      "source": [
        "# # this file should have previously been created in the root directory\n",
        "# dfImages = pd.read_csv(baseDirectory + \"images_main.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "executionInfo": {
          "elapsed": 12,
          "status": "ok",
          "timestamp": 1683527237917,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "6_WFGagHK_pl",
        "outputId": "b3660e2b-a19a-49c7-b513-eee0feb2ac3b"
      },
      "outputs": [],
      "source": [
        "# # Get The training Split and the Validation Split\n",
        "# dfImagesTrain = dfImages[dfImages[\"trainValTest\"] == 0].reset_index()\n",
        "# dfImagesVal = dfImages[dfImages[\"trainValTest\"] == 1].reset_index()\n",
        "# dfImagesTest = dfImages[dfImages[\"trainValTest\"] == 2].reset_index()\n",
        "\n",
        "# print(dfImagesTrain.shape)\n",
        "# print(dfImagesVal.shape)\n",
        "# print(dfImagesTest.shape)\n",
        "\n",
        "# dfImagesTrain.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ1R94YdK_pn"
      },
      "source": [
        "Note: The definition of the Custom Datasets for both the isCancerous data and the Cell Type data are defined in the pytorch_utility.py file.\n",
        "\n",
        "Also, rather than loading all the training images and calculating the mean and standard deviation values in here, that was run separately in file 05a.PyTorchGetMeanAndStd.ipynb\n",
        "\n",
        "Here we can just define the values to use, which shouldn't change unless the data is reloaded and a new train/validation/test split is generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 10,
          "status": "ok",
          "timestamp": 1683527237917,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "qaY5AA-tsWWw",
        "outputId": "53ce6f3a-aeec-4d4d-a697-e083f0b01746"
      },
      "outputs": [],
      "source": [
        "# train_mean, train_std = ptutil.getTrainMeanAndStdTensors()\n",
        "# print(train_mean)\n",
        "# print(train_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 9,
          "status": "ok",
          "timestamp": 1683527237918,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "I8z3yX8ZK_pn"
      },
      "outputs": [],
      "source": [
        "# # Create a tranform operation that also normalizes the images according to the mean and standard deviations of the images\n",
        "# transform_normalize = transforms.Compose(\n",
        "#     [transforms.ToPILImage(),\n",
        "#     transforms.ToTensor(), \n",
        "#     transforms.Normalize(train_mean, train_std)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 9,
          "status": "ok",
          "timestamp": 1683527237918,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "qb_33dFmK_pn"
      },
      "outputs": [],
      "source": [
        "# cancerous_training_data = None\n",
        "\n",
        "# # Create a custom Dataset for the training and validation data\n",
        "# if useFullData:\n",
        "#     cancerous_training_data = CancerBinaryDataset(isGoogleColab, dfImagesTrain, baseDirectory, transform=transform_normalize)\n",
        "# else:\n",
        "#     # For testing in a small dataset\n",
        "#     dfImagesTrainTest = dfImagesTrain.iloc[range(1000), :].reset_index()\n",
        "#     cancerous_training_data = CancerBinaryDataset(isGoogleColab, dfImagesTrainTest, baseDirectory, transform=transform_normalize, target_transform=None)\n",
        "\n",
        "# cancerous_validation_data = CancerBinaryDataset(isGoogleColab, dfImagesVal, baseDirectory, transform=transform_normalize, target_transform=None)\n",
        "# cancerous_test_data = CancerBinaryDataset(isGoogleColab, dfImagesTest, baseDirectory, transform=transform_normalize, target_transform=None)\n",
        "\n",
        "# # Create data loaders\n",
        "# cancerous_train_dataloader = DataLoader(cancerous_training_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "# cancerous_val_dataloader = DataLoader(cancerous_validation_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "# cancerous_test_dataloader = DataLoader(cancerous_test_data, batch_size=32, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4jD7E2PK_po"
      },
      "source": [
        "# Early Stopping\n",
        "\n",
        "Using the basic model (class), refactor the code such that the test and validation predictions are made at the end of each epoch and the loss difference between training error and validation error are calculated. Use the F1 Score as the metric\n",
        "\n",
        "Also implement a \"Patience\" level, where the patience is the number of consecutive epochs that can occur with no improvement in loss before the process is stopped early."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 10,
          "status": "ok",
          "timestamp": 1683527237919,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "0two1S9gK_po"
      },
      "outputs": [],
      "source": [
        "# Create a class for the Neural Network\n",
        "class PT_CNN_IsCancerous(nn.Module):\n",
        "\n",
        "    # In the constructor, initialize the layers to use\n",
        "    def __init__(self):\n",
        "        super(PT_CNN_IsCancerous, self).__init__()\n",
        "\n",
        "        # first, define the subsampling methods. Though they are used multiple times, these are the\n",
        "        # operations, so only need to be defined once\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # define the Activation methods to use\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # define the convolution layers\n",
        "\n",
        "        # input should be 27x27x3. Apply a 3x3 filter, therefore, output should be 25x25x32 (channels aka feature maps)\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1)\n",
        "        # There will be a Relu\n",
        "        # Then a MaxPool of 2x2, halving the dimensions per feature map\n",
        "        # So input is 12x12x32. Apply a 3x3 filter, also include padding=1, as this is already quite small, and lets consider the edges\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)    \n",
        "        # There will be a Relu        \n",
        "        # So input is 12x12x64. Apply a 3x3 filter, also include padding=1, as this is already quite small, and lets consider the edges\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        # Then a MaxPool of 2x2, halving the dimensions per feature map\n",
        "        # Then an Average Pool of keeping the dimensions as 6x6\n",
        "        \n",
        "        # define the fully connected neural layers\n",
        "        self.fc1 = nn.Linear(128 * 6 * 6, 4608)\n",
        "        self.fc2 = nn.Linear(4608, 4608)\n",
        "        self.fc2 = nn.Linear(4608, 4608)\n",
        "        self.fc4 = nn.Linear(4608, 2)\n",
        "\n",
        "    # Create the forward function, which is used in training\n",
        "    def forward(self, x):\n",
        "\n",
        "        # print(\"Init Shape: \" + str(x.shape))\n",
        "\n",
        "        # Process the first 2 convolution layers, applying maxpooling\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Then process the remaining convolution layers without any pooling\n",
        "        x = self.relu(self.conv2(x))        \n",
        "        x = self.relu(self.conv3(x))\n",
        "\n",
        "        # Then apply a max pool and average pool on the result\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Flatten: This should convert to tensors that are acceptable for the input into the NN 3 layers\n",
        "        x = x.view(x.size(0), 192 * 6 * 6)\n",
        "\n",
        "        # Now process the 3 layers of the Fully Connected NN\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.sigmoid(self.fc4(x))        \n",
        "        # x = self.relu(self.fc3(x))        \n",
        "        # x = self.fc3(x)        \n",
        "\n",
        "        # return the result\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO5dPhy6K_po"
      },
      "source": [
        "Now train the Fully Connected Neural Network Model.\n",
        "\n",
        "During training, we will use the following:\n",
        "- Softmax Cross Entropy Loss as our Loss function. This is a good Loss function that basically converts scores for each class into probabilities\n",
        "- The Adam Optimizer, which is a version of Gradient Descent\n",
        "- Initially, just 10 epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc7rjcsiCKbe"
      },
      "source": [
        "Create a function to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 9,
          "status": "ok",
          "timestamp": 1683527237919,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "7SAdIfM-CKbe"
      },
      "outputs": [],
      "source": [
        "def predictCancerousOnDataSetF1(net, setName, dataloader, printResult=True, printFullResults=False):\n",
        "    correct, total = 0,  0\n",
        "    predictions = []\n",
        "\n",
        "    y_cancerous = []\n",
        "    y_pred_cancerous = []\n",
        "\n",
        "    # Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        # Loop through the batch, build the lists of the raw label and prediction values\n",
        "        for j in range(len(labels)):\n",
        "            y_cancerous.append(labels[j].item())\n",
        "            y_pred_cancerous.append(predicted[j].item())\n",
        "\n",
        "        predictions.append(predicted)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    f1Score = f1_score(y_cancerous, y_pred_cancerous)\n",
        "\n",
        "    if printFullResults:\n",
        "        print(setName + \":\")        \n",
        "        print('Confusion matrix: \\n')\n",
        "        print(confusion_matrix(y_cancerous, y_pred_cancerous))\n",
        "        print(\"\\n- Accuracy Score: \" + str(accuracy_score(y_cancerous, y_pred_cancerous)))\n",
        "        print(\"- Precision Score: \" + str(precision_score(y_cancerous, y_pred_cancerous)))\n",
        "        print(\"- Recall Score: \" + str(recall_score(y_cancerous, y_pred_cancerous)))\n",
        "        print(\"- F1 Score: \" + str(f1Score))\n",
        "    elif printResult:\n",
        "        print(\"- \" + setName + \" F1: \" + str(f1Score))        \n",
        "\n",
        "    return f1Score, y_cancerous, y_pred_cancerous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2953420,
          "status": "ok",
          "timestamp": 1683530191330,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "vQUAMyGuK_po",
        "outputId": "1249f0c1-16d1-4685-e541-dd82aca7d58e"
      },
      "outputs": [],
      "source": [
        "# set the Learning Rate to use\n",
        "learning_rate = 0.0001\n",
        "maxEpochs = 15\n",
        "patience = 3\n",
        "disableEarlyStopping = False\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use this for L2 Regularization\n",
        "# net = PT_NN_IsCancerous()\n",
        "# optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "\n",
        "# Use this for dropouts\n",
        "net = PT_CNN_IsCancerous()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "bestErrorDiff = 99999\n",
        "concurrentNonImproves = 0\n",
        "currentEpoch = 0\n",
        "\n",
        "bestValF1 = -1\n",
        "lstEpochs = []\n",
        "lstTrainF1s = []\n",
        "lstValF1s = []\n",
        "for epoch in range(maxEpochs):\n",
        "    print(\"Starting Epoch \" + str(epoch) + \"...\")\n",
        "    currentEpoch = epoch\n",
        "\n",
        "    # Set the Neural Network into training mode\n",
        "    net.train()\n",
        "\n",
        "    # Train through this epoch\n",
        "    for i, data in enumerate(cancerous_train_dataloader, 0):\n",
        "        # Get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Perform Forward and Backward propagation then optimize the weights\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Set the Neural Network into evaluation (test) mode, so we can evaluate both training and validation error\n",
        "    net.eval()        \n",
        "    trainingF1, y_train_cancerous, y_train_pred_cancerous = predictCancerousOnDataSetF1(net, \"Training\", cancerous_train_dataloader, True, False)\n",
        "    validationF1, y_val_cancerous, y_val_pred_cancerous = predictCancerousOnDataSetF1(net, \"Validation\", cancerous_val_dataloader, True, False)\n",
        "\n",
        "    errorDiff = trainingF1 - validationF1\n",
        "    print(\"- F1 Difference: \" + str(errorDiff))\n",
        "\n",
        "    lstEpochs.append(epoch)\n",
        "    lstTrainF1s.append(trainingF1)\n",
        "    lstValF1s.append(validationF1)\n",
        "\n",
        "    if epoch > 0 and (validationF1 - bestValF1 > 0.01):        \n",
        "        # There is at least percentage point improvement in the validation F1, count this as a \n",
        "        # good iteration, regardless of the error difference\n",
        "        print(\"- IsGoodStep\")\n",
        "        concurrentNonImproves = 0\n",
        "        if errorDiff > 0 and errorDiff < bestErrorDiff:  \n",
        "            bestErrorDiff = errorDiff        \n",
        "    elif errorDiff < bestErrorDiff:        \n",
        "        # This epoch is an improvement on the last, so we will continue. the concurrent non improve counts reset for the patience\n",
        "        print(\"- IsBetter: \" + str(errorDiff) + \" : \" + str(bestErrorDiff))        \n",
        "        concurrentNonImproves = 0\n",
        "        if errorDiff > 0:\n",
        "            bestErrorDiff = errorDiff\n",
        "    else:\n",
        "        # This epoch has the same or worse performance than the last. Check if we have reached the patience, if so, then stop early\n",
        "        print(\"- IsNotBetter: \" + str(errorDiff) + \" : \" + str(bestErrorDiff))        \n",
        "        concurrentNonImproves += 1\n",
        "        if disableEarlyStopping == False:\n",
        "            if concurrentNonImproves >= patience:\n",
        "                print(\"Early Stopping occurred at Epoch \" + str(epoch))\n",
        "                break\n",
        "\n",
        "    # update the val F1 score from the previous epoch if it's the best\n",
        "    if validationF1 > bestValF1:\n",
        "        bestValF1 = validationF1\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LezmNEykCKbg"
      },
      "source": [
        "Look at the training and validation loss plot to track how the model improved during epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "executionInfo": {
          "elapsed": 11,
          "status": "ok",
          "timestamp": 1683530191331,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "OznU1dWACKbg",
        "outputId": "3430c067-47f7-4cd6-c8b6-109adb1208c7"
      },
      "outputs": [],
      "source": [
        "dfLoss = pd.DataFrame({ 'epoch': lstEpochs, 'train': lstTrainF1s, 'validation': lstValF1s })\n",
        "graphutil.graphBasicTwoSeries(dfLoss, \"epoch\", \"train\", \"validation\", \"IsCancerous Training and Validation F1\", \n",
        "        \"Epoch\", \"Training F1\", \"Validation F1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEB7hSkAK_po"
      },
      "source": [
        "Training Time in Nelson's Local Environment on the full data takes a very long time, stopped after 100 minutes. This will need to be done in Colab.\n",
        "\n",
        "First, Predict on the training data so that we can find the training error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 14613,
          "status": "ok",
          "timestamp": 1683530205937,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "f1E180BesWWy",
        "outputId": "f6e5c3e9-f40a-4fa8-c4d9-29fcbdeb1169"
      },
      "outputs": [],
      "source": [
        "# Use the final NN model to predict on the training data\n",
        "trainingF1, y_train_cancerous, y_train_pred_cancerous = predictCancerousOnDataSetF1(net, \"Training\", cancerous_train_dataloader, True, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "executionInfo": {
          "elapsed": 7,
          "status": "ok",
          "timestamp": 1683530205938,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "4M0Jb0fosWWy",
        "outputId": "957ef8c3-e39e-4688-9b56-65bf55aa13ba"
      },
      "outputs": [],
      "source": [
        "a2util.getClassificationROC(\"IsCancerous\", \"Training\", y_train_cancerous, y_train_pred_cancerous)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbvP6a19sWWy"
      },
      "source": [
        "Now Predict according to the Validation data and evaluate. While looping through here, we will need to get out the Labels from the data loader, because the order of predictions in the batches do not match the order of the original Target values in the dataset (because we turned Shuffle on)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1567,
          "status": "ok",
          "timestamp": 1683530207501,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "ithF1bLMK_pp",
        "outputId": "e8404f51-0136-4da5-8919-385846d654ed"
      },
      "outputs": [],
      "source": [
        "testF1, y_test_cancerous, y_test_pred_cancerous = predictCancerousOnDataSetF1(net, \"Test\", cancerous_test_dataloader, True, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "executionInfo": {
          "elapsed": 957,
          "status": "ok",
          "timestamp": 1683530208455,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "ivOulSepsWWz",
        "outputId": "af3f23f2-9aab-48d3-b01b-e98f5b613625"
      },
      "outputs": [],
      "source": [
        "a2util.getClassificationROC(\"IsCancerous\", \"Test\", y_test_cancerous, y_test_pred_cancerous, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHozRzzdK_pq"
      },
      "source": [
        "Now also train a model for CellType Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 9,
          "status": "ok",
          "timestamp": 1683530208456,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "AVvBdxxjK_pq",
        "outputId": "f39b609a-d26c-41e4-e295-3ee0e50d2573"
      },
      "outputs": [],
      "source": [
        "celltype_training_data = None\n",
        "\n",
        "# Create a custom Dataset for the training and validation data\n",
        "if useFullData:\n",
        "    celltype_training_data = CancerCellTypeDataset(isGoogleColab, dfImagesTrain, baseDirectory, transform=transform_normalize)\n",
        "else:\n",
        "    # For testing in a small dataset\n",
        "    dfImagesTrainTest = dfImagesTrain.iloc[range(1000), :].reset_index()\n",
        "    celltype_training_data = CancerCellTypeDataset(isGoogleColab, dfImagesTrainTest, baseDirectory, transform=transform_normalize, target_transform=None)\n",
        "\n",
        "celltype_validation_data = CancerCellTypeDataset(isGoogleColab, dfImagesVal, baseDirectory, transform=transform_normalize)\n",
        "celltype_test_data = CancerCellTypeDataset(isGoogleColab, dfImagesTest, baseDirectory, transform=transform_normalize)\n",
        "\n",
        "# Create data loaders\n",
        "celltype_train_dataloader = DataLoader(celltype_training_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "celltype_val_dataloader = DataLoader(celltype_validation_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "celltype_test_dataloader = DataLoader(celltype_test_data, batch_size=32, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd9XOjhpK_pq"
      },
      "source": [
        "Create a class for the Cell Type Neural Network model. The structure of the class will be fundamentally the same, only the model will need to output 4 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 7,
          "status": "ok",
          "timestamp": 1683530208456,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "eKOxU2-4K_pq"
      },
      "outputs": [],
      "source": [
        "# Create a class for the Neural Network\n",
        "class PT_CNN_CellType(nn.Module):\n",
        "\n",
        "    # In the constructor, initialize the layers to use\n",
        "    def __init__(self):\n",
        "        super(PT_CNN_CellType, self).__init__()\n",
        "\n",
        "        # first, define the subsampling methods. Though they are used multiple times, these are the\n",
        "        # operations, so only need to be defined once\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # define the Activation methods to use\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # define the convolution layers\n",
        "\n",
        "        # input should be 27x27x3. Apply a 3x3 filter, therefore, output should be 25x25x32 (channels aka feature maps)\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1)\n",
        "        # There will be a Relu\n",
        "        # Then a MaxPool of 2x2, halving the dimensions per feature map\n",
        "        # So input is 12x12x32. Apply a 3x3 filter, also include padding=1, as this is already quite small, and lets consider the edges\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)    \n",
        "        # There will be a Relu        \n",
        "        # So input is 12x12x64. Apply a 3x3 filter, also include padding=1, as this is already quite small, and lets consider the edges\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        # Then a MaxPool of 2x2, halving the dimensions per feature map\n",
        "        # Then an Average Pool of keeping the dimensions as 6x6\n",
        "        \n",
        "        # define the fully connected neural layers\n",
        "        self.fc1 = nn.Linear(128 * 6 * 6, 4608)\n",
        "        self.fc2 = nn.Linear(4608, 4608)\n",
        "        self.fc2 = nn.Linear(4608, 4608)\n",
        "        self.fc3 = nn.Linear(4608, 4)\n",
        "\n",
        "    # Create the forward function, which is used in training\n",
        "    def forward(self, x):\n",
        "\n",
        "        # print(\"Init Shape: \" + str(x.shape))\n",
        "\n",
        "        # Process the first 2 convolution layers, applying maxpooling\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Then process the remaining convolution layers without any pooling\n",
        "        x = self.relu(self.conv2(x))        \n",
        "        x = self.relu(self.conv3(x))\n",
        "\n",
        "        # Then apply a max pool and average pool on the result\n",
        "        x = self.maxpool(x)\n",
        "        #x = self.avgpool(x)\n",
        "\n",
        "        # Flatten: This should convert to tensors that are acceptable for the input into the NN 3 layers\n",
        "        x = x.view(x.size(0), 192 * 6 * 6)\n",
        "\n",
        "        # Now process the 3 layers of the Fully Connected NN\n",
        "        x = self.relu(self.fc1(x))  \n",
        "        x = self.relu(self.fc2(x))  \n",
        "        x = self.relu(self.fc3(x))              \n",
        "        # x = self.fc3(x)\n",
        "        # x = self.relu(self.fc3(x))\n",
        "        x = F.softmax(self.fc4(x), dim=1)        \n",
        "\n",
        "        # return the result\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRU0R9EMK_pq"
      },
      "source": [
        "Now train the Fully Connected Neural Network Model. Use the same configuration (objective function, optimizer etc) as the Binary Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 8,
          "status": "ok",
          "timestamp": 1683530208457,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "9sPKkwUkCKbm"
      },
      "outputs": [],
      "source": [
        "def predictCellTypeOnDataSetAccuracy(net, setName, dataloader, printResult=True, printFullResults=False):\n",
        "    correct, total = 0,  0\n",
        "    predictions = []\n",
        "\n",
        "    y_celltype = []\n",
        "    y_pred_celltype = []\n",
        "    y_pred_celltype_scores = []\n",
        "\n",
        "    # Looping through this dataloader essentially processes them in batches of 32 (or whatever the batchsize is configured in the data loader\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        # Loop through the batch, build the lists of the raw label and prediction values\n",
        "        for j in range(len(labels)):\n",
        "            y_celltype.append(labels[j].item())\n",
        "            y_pred_celltype.append(predicted[j].item())\n",
        "            y_pred_celltype_scores.append(outputs.data[j].tolist())\n",
        "\n",
        "        predictions.append(predicted)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = accuracy_score(y_celltype, y_pred_celltype)\n",
        "    f1Score = f1_score(y_celltype, y_pred_celltype, average=\"micro\")\n",
        "\n",
        "    if printFullResults:\n",
        "        print(setName + \":\")        \n",
        "        print('Confusion matrix: \\n')\n",
        "        print(confusion_matrix(y_celltype, y_pred_celltype))\n",
        "        print(\"\\n- Accuracy Score: \" + str(accuracy))\n",
        "        print(\"- Precision Score: \" + str(precision_score(y_celltype, y_pred_celltype, average=\"micro\")))\n",
        "        print(\"- Recall Score: \" + str(recall_score(y_celltype, y_pred_celltype, average=\"micro\")))\n",
        "        print(\"- F1 Score: \" + str(f1Score))\n",
        "    elif printResult:\n",
        "        print(\"- \" + setName + \" Accuracy: \" + str(accuracy))\n",
        "\n",
        "    return accuracy, y_celltype, y_pred_celltype, y_pred_celltype_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 153417,
          "status": "ok",
          "timestamp": 1683530361867,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "d3FjNKScK_pq",
        "outputId": "8843af12-24dd-4b58-a714-d447c1d66f9b"
      },
      "outputs": [],
      "source": [
        "# set the Learning Rate to use\n",
        "learning_rate = 0.0001\n",
        "maxEpochs = 15\n",
        "patience = 3\n",
        "disableEarlyStopping = False\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use this for L2 Regularization\n",
        "# net = PT_NN_IsCancerous()\n",
        "# optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "\n",
        "# Use this for dropouts\n",
        "net = PT_CNN_CellType()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "bestErrorDiff = 99999\n",
        "concurrentNonImproves = 0\n",
        "currentEpoch = 0\n",
        "\n",
        "bestValAcc = -1\n",
        "lstEpochs = []\n",
        "lstTrainAccs = []\n",
        "lstValAccs = []\n",
        "for epoch in range(maxEpochs):\n",
        "    print(\"Starting Epoch \" + str(epoch) + \"...\")\n",
        "    currentEpoch = epoch\n",
        "\n",
        "    # Set the Neural Network into training mode\n",
        "    net.train()\n",
        "\n",
        "    # Train through this epoch\n",
        "    for i, data in enumerate(celltype_train_dataloader, 0):\n",
        "        # Get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Perform Forward and Backward propagation then optimize the weights\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Set the Neural Network into evaluation (test) mode, so we can evaluate both training and validation error\n",
        "    net.eval()        \n",
        "    trainingAccuracy, y_train_celltype, y_train_pred_celltype, y_train_pred_celltype_scores = predictCellTypeOnDataSetAccuracy(\n",
        "        net, \"Training\", celltype_train_dataloader, True, False)\n",
        "    validationAccuracy, y_val_celltype, y_val_pred_celltype, y_val_pred_celltype_scores  = predictCellTypeOnDataSetAccuracy(\n",
        "        net, \"Validation\", celltype_val_dataloader, True, False)\n",
        "\n",
        "    errorDiff = trainingAccuracy - validationAccuracy\n",
        "    print(\"- Accuracy Difference: \" + str(errorDiff))\n",
        "\n",
        "    lstEpochs.append(epoch)\n",
        "    lstTrainAccs.append(trainingAccuracy)\n",
        "    lstValAccs.append(validationAccuracy)\n",
        "\n",
        "    if epoch > 0 and (validationAccuracy - bestValAcc > 0.01):        \n",
        "        # There is at least percentage point improvement in the validation F1, count this as a \n",
        "        # good iteration, regardless of the error difference\n",
        "        print(\"- IsGoodStep\")\n",
        "        concurrentNonImproves = 0\n",
        "        if errorDiff > 0 and errorDiff < bestErrorDiff:  \n",
        "            bestErrorDiff = errorDiff        \n",
        "    elif errorDiff < bestErrorDiff:        \n",
        "        # This epoch is an improvement on the last, so we will continue. the concurrent non improve counts reset for the patience\n",
        "        print(\"- IsBetter: \" + str(errorDiff) + \" : \" + str(bestErrorDiff))        \n",
        "        concurrentNonImproves = 0\n",
        "        if errorDiff > 0:\n",
        "            bestErrorDiff = errorDiff\n",
        "    else:\n",
        "        # This epoch has the same or worse performance than the last. Check if we have reached the patience, if so, then stop early\n",
        "        print(\"- IsNotBetter: \" + str(errorDiff) + \" : \" + str(bestErrorDiff))        \n",
        "        concurrentNonImproves += 1\n",
        "        if disableEarlyStopping == False:\n",
        "            if concurrentNonImproves >= patience:\n",
        "                print(\"Early Stopping occurred at Epoch \" + str(epoch))\n",
        "                break\n",
        "\n",
        "    # update the val F1 score from the previous epoch if it's the best\n",
        "    if validationAccuracy > bestValAcc:\n",
        "        bestValAcc = validationAccuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfLoss = pd.DataFrame({ 'epoch': lstEpochs, 'train': lstTrainAccs, 'validation': lstValAccs })\n",
        "graphutil.graphBasicTwoSeries(dfLoss, \"epoch\", \"train\", \"validation\", \"CellType Training and Validation Accuracy\", \n",
        "        \"Epoch\", \"Training Accuracy\", \"Validation Accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPplgEbCsWW0"
      },
      "source": [
        "Predict on the Training Set to get the Training Accuracy and Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 15121,
          "status": "ok",
          "timestamp": 1683530376984,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "dWdn3fsTsWW0",
        "outputId": "8a7d2c09-a37c-429b-9b0f-1cdaf439c9d0"
      },
      "outputs": [],
      "source": [
        "trainingAcc, y_train_celltype, y_train_pred_celltype, y_train_pred_celltype_scores = predictCellTypeOnDataSetAccuracy(\n",
        "    net, \"Training\", celltype_train_dataloader, True, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 15,
          "status": "ok",
          "timestamp": 1683530376984,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "u27Mb-rTsWW0",
        "outputId": "1c26c722-78c9-49a3-8416-ab1a6ca56423"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    print(y_train_pred_celltype_scores[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 1600,
          "status": "ok",
          "timestamp": 1683530378571,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "eiHy8WeqsWW0",
        "outputId": "6f4318a2-121f-41bb-c9cc-a904c85aa1e6"
      },
      "outputs": [],
      "source": [
        "a2util.getClassificationROC(\"CellType\", \"Training\", y_train_celltype, y_train_pred_celltype, 4, y_train_pred_celltype_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpLNiit9K_pr"
      },
      "source": [
        "Predict on the Validation data and evaluate the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 735,
          "status": "ok",
          "timestamp": 1683530379303,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "VfU8z2ljK_pr",
        "outputId": "319eec34-c67e-40e7-f277-97b11f044944"
      },
      "outputs": [],
      "source": [
        "testAccuracy, y_test_celltype, y_test_pred_celltype, y_test_pred_celltype_scores  = predictCellTypeOnDataSetAccuracy(\n",
        "        net, \"Test\", celltype_test_dataloader, True, True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 2437,
          "status": "ok",
          "timestamp": 1683530381739,
          "user": {
            "displayName": "Nelson Cheng",
            "userId": "13903294221993632929"
          },
          "user_tz": -600
        },
        "id": "PxUAI5fksWW1",
        "outputId": "4e2b2521-578d-4a34-bf51-ec368d57a6c4"
      },
      "outputs": [],
      "source": [
        "a2util.getClassificationROC(\"CellType\", \"Test\", y_test_celltype, y_test_pred_celltype, 4, y_test_pred_celltype_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results\n",
        "\n",
        "Append Results for both Binary IsCancerous and Cell Type here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### IsCancerous Results\n",
        "\n",
        "Basic 01, 3 Layer NN with Dropout - On Colab, Full data - Previous best performing\n",
        "- Training Accuracy: 0.942\n",
        "- Training F1: 0.923\n",
        "- Validation Accuracy: 0.869\n",
        "- Validation F1: 0.897\n",
        "\n",
        "CNN01, 3 Layer Classifier Binary with Sigmoid as Final Activation Function\n",
        "- **Training**\n",
        "- Accuracy Score: 0.9518948577261708\n",
        "- Precision Score: 0.9426820475847152\n",
        "- Recall Score: 0.9230497705612425\n",
        "- F1 Score: 0.9327626181558766\n",
        "- **Validation**\n",
        "- Accuracy Score: 0.8894277400581959\n",
        "- Precision Score: 0.9178981937602627\n",
        "- Recall Score: 0.8972712680577849\n",
        "- F1 Score: 0.9074675324675324\n",
        "\n",
        "CNN02, 3 Layer Classifier with Early Stopping\n",
        "- **Training**\n",
        "- **Validation**\n",
        "\n",
        "\n",
        "CNN02, 2 Layer Classifier with Early Stopping\n",
        "- **Training**\n",
        "- Accuracy Score: 0.9281612862064565\n",
        "- Precision Score: 0.9235074626865671\n",
        "- Recall Score: 0.8736321920225909\n",
        "- F1 Score: 0.8978777435153275\n",
        "- **Test**\n",
        "- Accuracy Score: 0.8764591439688716\n",
        "- Precision Score: 0.9039087947882736\n",
        "- Recall Score: 0.8908507223113965\n",
        "- F1 Score: 0.8973322554567501\n",
        "\n",
        "CNN03, 3 Layer Classifier, more complex Convolutions\n",
        "- **Training**\n",
        "- Accuracy Score: 0.9532984560418527\n",
        "- Precision Score: 0.949690120306234\n",
        "- Recall Score: 0.9195199435227673\n",
        "- F1 Score: 0.9343615494978479\n",
        "- **Validation**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1xI7RrpsWW1"
      },
      "source": [
        "### Cell type Results\n",
        "\n",
        "Basic 01, 3 Layer NN with Dropout - On Colab, Full data - Previous best performing\n",
        "- Training Accuracy: 0.844\n",
        "- Training F1: 0.844\n",
        "- Validation Accuracy: 0.778\n",
        "- Validation F1: 0.778\n",
        "\n",
        "CNN01, 3 Layer Classifier with Softmax as Final Activation Function, no Early Stopping\n",
        "- **Training**\n",
        "- Accuracy Score: 0.842414189102973\n",
        "- Precision Score: 0.842414189102973\n",
        "- Recall Score: 0.842414189102973\n",
        "- F1 Score: 0.842414189102973\n",
        "- **Validation**\n",
        "- Accuracy Score: 0.7827352085354026\n",
        "- Precision Score: 0.7827352085354026\n",
        "- Recall Score: 0.7827352085354026\n",
        "- F1 Score: 0.7827352085354026\n",
        "\n",
        "CNN02, 3 Layer Classifier with Early Stopping\n",
        "- **Training**\n",
        "- Accuracy Score: 0.8100038279954064\n",
        "- Precision Score: 0.8100038279954064\n",
        "- Recall Score: 0.8100038279954064\n",
        "- F1 Score: 0.8100038279954064\n",
        "- **Validation**\n",
        "- Accuracy Score: 0.7957198443579766\n",
        "- Precision Score: 0.7957198443579766\n",
        "- Recall Score: 0.7957198443579766\n",
        "- F1 Score: 0.7957198443579766\n",
        "\n",
        "CNN02, 2 Layer Classifier with Early Stopping\n",
        "- **Training**\n",
        "- Accuracy Score: 0.7962230445323466\n",
        "- Precision Score: 0.7962230445323466\n",
        "- Recall Score: 0.7962230445323466\n",
        "- F1 Score: 0.7962230445323466\n",
        "- **Test**\n",
        "- Accuracy Score: 0.7928015564202334\n",
        "- Precision Score: 0.7928015564202334\n",
        "- Recall Score: 0.7928015564202334\n",
        "- F1 Score: 0.7928015564202334"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys43DeSksWW1"
      },
      "source": [
        "<h1>Analysis of Performance and Accuracy</h1>\n",
        "\n",
        "<h3>Is Cancerous</h3>\n",
        "<p>\n",
        "The best performing model after initial CNN experiments, according to F1 Score, is the CNN with 3 Layer Classifier, with no Early stopping. the F1 Score is <strong>0.907</strong>, which is only a 0.01 improvement on the Basic NN 2 Layer model.\n",
        "</p>\n",
        "<p>\n",
        "However, we can see that the difference between the Training F1 and the Test F1 in this CNN is a lot smaller, with the Training F1 being 0.93. This is a situation where we have higher bias but less variance, with higher bias compared to other experiments. Therefore, it may be possible to model with more accuracy to reduce the bias, however, we must be careful not to overfit.\n",
        "</p>\n",
        "\n",
        "<h3>Is Cancerous</h3>\n",
        "<p>\n",
        "The best performing model after initial CNN experiments, according to Accuracy, is the CNN with 2 Layer Classifier, with Early stopping. the F1 Score is <strong>0.792</strong>, which is a 0.014 improvement on the Basic NN 2 Layer model.\n",
        "</p>\n",
        "<p>\n",
        "However, we can see that the difference between the Training Accuracy and the Test Accuracy in this CNN very small, with the Testing accuracy being <strong>0.796</strong>. This indicates that the model is generalizing very well. However, it appears that this model has a higher bias than other experiments.\n",
        "</p>\n",
        "\n",
        "<h3>Conclusion</h3>\n",
        "<p>\n",
        "Therefore, for both models, it is worth experimenting on how accuracy can be improved. There are 3 methods that initially should be tried\n",
        "</p>\n",
        "<ol>\n",
        "<li>First, experiment with a more complex CNN model, with additional complexity in the Convolution and the Classifier parts</li>\n",
        "<li>Addition of more data to train with, specifically the Extra data provided</li>\n",
        "<li>Data Augmentation</li>\n",
        "</ol>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "dcbc78149e46ccbab92a3f68a48c52feb0796c7e10dad8e3f1a2a5a780973376"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
